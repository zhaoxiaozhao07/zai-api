"""
OpenAI API endpoints - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé›†æˆ Toolify å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼‰
"""

import time
import json
import random
import asyncio
from fastapi import APIRouter, Header, HTTPException
from fastapi.responses import StreamingResponse
import httpx

from .config import settings
from .schemas import OpenAIRequest, ModelsResponse, Model
from .helpers import debug_log, get_logger, perf_timer
from .zai_transformer import ZAITransformer
from .toolify_handler import should_enable_toolify, prepare_toolify_request, parse_toolify_response
from .toolify.detector import StreamingFunctionCallDetector
from .toolify_config import get_toolify

# å°è¯•å¯¼å…¥orjsonï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼Œå¦‚æœä¸å¯ç”¨åˆ™fallbackåˆ°æ ‡å‡†json
try:
    import orjson
    
    # åˆ›å»ºorjsonå…¼å®¹å±‚
    class JSONEncoder:
        @staticmethod
        def dumps(obj, **kwargs):
            # orjson.dumpsè¿”å›bytesï¼Œéœ€è¦decode
            return orjson.dumps(obj).decode('utf-8')
        
        @staticmethod
        def loads(s, **kwargs):
            # orjson.loadså¯ä»¥æ¥å—stræˆ–bytes
            return orjson.loads(s)
    
    json_lib = JSONEncoder()
    debug_log("âœ… ä½¿ç”¨ orjson è¿›è¡Œ JSON åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰")
except ImportError:
    json_lib = json
    debug_log("âš ï¸ orjson æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡† json åº“")

router = APIRouter()

# è·å–ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨
logger = get_logger("openai_api")

# å…¨å±€è½¬æ¢å™¨å®ä¾‹
transformer = ZAITransformer()

# å…¨å±€httpxå®¢æˆ·ç«¯é…ç½®ï¼ˆè¿æ¥æ± å¤ç”¨ï¼‰
_http_client = None
_client_lock = asyncio.Lock()


async def get_http_client() -> httpx.AsyncClient:
    """
    è·å–æˆ–åˆ›å»ºå…¨å±€httpxå®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œæ”¯æŒè¿æ¥æ± å¤ç”¨ï¼‰
    ä½¿ç”¨asyncioç‰¹æ€§è¿›è¡Œå¹¶å‘æ§åˆ¶
    """
    global _http_client
    
    async with _client_lock:
        if _http_client is None or _http_client.is_closed:
            proxy = settings.HTTPS_PROXY or settings.HTTP_PROXY
            
            # é…ç½®è¿æ¥æ± å’Œè¶…æ—¶
            limits = httpx.Limits(
                max_connections=100,  # æœ€å¤§è¿æ¥æ•°
                max_keepalive_connections=20,  # ä¿æŒæ´»åŠ¨çš„è¿æ¥æ•°
                keepalive_expiry=30.0  # ä¿æŒæ´»åŠ¨è¶…æ—¶æ—¶é—´
            )
            
            timeout = httpx.Timeout(
                connect=10.0,  # è¿æ¥è¶…æ—¶
                read=60.0,     # è¯»å–è¶…æ—¶
                write=10.0,    # å†™å…¥è¶…æ—¶
                pool=5.0       # è¿æ¥æ± è¶…æ—¶
            )
            if proxy:
                debug_log("ä½¿ç”¨ä»£ç†åˆ›å»ºHTTPå®¢æˆ·ç«¯", proxy=proxy)
                _http_client = httpx.AsyncClient(
                    timeout=timeout,
                    limits=limits,
                    proxy=proxy,
                    http2=True,  # å¯ç”¨HTTP/2æ”¯æŒ
                    follow_redirects=True
                )
            else:
                debug_log("æœªä½¿ç”¨ä»£ç†åˆ›å»ºHTTPå®¢æˆ·ç«¯")
                _http_client = httpx.AsyncClient(
                    timeout=timeout,
                    limits=limits,
                    http2=True,  # å¯ç”¨HTTP/2æ”¯æŒ
                    follow_redirects=True
                )

        
        return _http_client


async def close_http_client():
    """å…³é—­å…¨å±€httpxå®¢æˆ·ç«¯"""
    global _http_client
    
    async with _client_lock:
        if _http_client is not None and not _http_client.is_closed:
            await _http_client.aclose()
            _http_client = None
            debug_log("HTTPå®¢æˆ·ç«¯å·²å…³é—­")


def calculate_backoff_delay(retry_count: int, status_code: int = None, base_delay: float = 3.0, max_delay: float = 30.0) -> float:
    """
    è®¡ç®—é€€é¿å»¶è¿Ÿæ—¶é—´ï¼ˆå¸¦éšæœºæŠ–åŠ¨å’Œé’ˆå¯¹ä¸åŒé”™è¯¯çš„ç­–ç•¥ï¼‰
    
    Args:
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°ï¼ˆä»1å¼€å§‹ï¼‰
        status_code: HTTPçŠ¶æ€ç ï¼Œç”¨äºåŒºåˆ†ä¸åŒé”™è¯¯ç±»å‹
        base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        è®¡ç®—åçš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    # åŸºç¡€æŒ‡æ•°é€€é¿ï¼šbase_delay * 2^(retry_count-1)
    exponential_delay = base_delay * (2 ** (retry_count - 1))
    
    # é’ˆå¯¹ä¸åŒé”™è¯¯ç±»å‹è°ƒæ•´å»¶è¿Ÿ
    if status_code == 429:  # Rate limit - æ›´é•¿çš„é€€é¿æ—¶é—´
        exponential_delay *= 2.0  # åŒå€å»¶è¿Ÿ
        debug_log("[BACKOFF] æ£€æµ‹åˆ°429é™æµé”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿é€€é¿æ—¶é—´")
    elif status_code in [502, 503, 504]:  # æœåŠ¡ç«¯é”™è¯¯ - é€‚ä¸­å»¶è¿Ÿ
        exponential_delay *= 1.5
        debug_log("[BACKOFF] æ£€æµ‹åˆ°æœåŠ¡ç«¯é”™è¯¯ï¼Œä½¿ç”¨é€‚ä¸­é€€é¿æ—¶é—´")
    elif status_code in [400, 401]:  # è®¤è¯/è¯·æ±‚é”™è¯¯ - æ ‡å‡†å»¶è¿Ÿ
        debug_log("[BACKOFF] æ£€æµ‹åˆ°è®¤è¯/è¯·æ±‚é”™è¯¯ï¼Œä½¿ç”¨æ ‡å‡†é€€é¿æ—¶é—´")
    
    # é™åˆ¶æœ€å¤§å»¶è¿Ÿ
    exponential_delay = min(exponential_delay, max_delay)
    
    # æ·»åŠ éšæœºæŠ–åŠ¨å› å­ï¼ˆÂ±25%ï¼‰ï¼Œé¿å…å¤šä¸ªè¯·æ±‚åŒæ—¶é‡è¯•é€ æˆé›ªå´©
    jitter = exponential_delay * 0.25  # 25%çš„æŠ–åŠ¨èŒƒå›´
    jittered_delay = exponential_delay + random.uniform(-jitter, jitter)
    
    # ç¡®ä¿å»¶è¿Ÿä¸ä¼šå°äºåŸºç¡€å»¶è¿Ÿçš„ä¸€åŠ
    final_delay = max(jittered_delay, base_delay * 0.5)
    
    debug_log(
        "[BACKOFF] è®¡ç®—é€€é¿å»¶è¿Ÿ",
        retry_count=retry_count,
        status_code=status_code,
        exponential=f"{exponential_delay:.2f}s",
        final=f"{final_delay:.2f}s"
    )
    
    return final_delay


@router.get("/v1/models")
async def list_models():
    """List available models"""
    current_time = int(time.time())
    response = ModelsResponse(
        data=[
            Model(id=settings.PRIMARY_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.THINKING_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.SEARCH_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.AIR_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_THINKING_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_SEARCH_MODEL, created=current_time, owned_by="z.ai"),
        ]
    )
    return response


@router.post("/v1/chat/completions")
async def chat_completions(request: OpenAIRequest, authorization: str = Header(...)):
    """Handle chat completion requests with ZAI transformer - æ”¯æŒæµå¼å’Œéæµå¼ä»¥åŠå·¥å…·è°ƒç”¨"""
    role = request.messages[0].role if request.messages else "unknown"
    debug_log(
        "æ”¶åˆ°å®¢æˆ·ç«¯è¯·æ±‚",
        model=request.model,
        stream=request.stream,
        message_count=len(request.messages),
        tools_count=len(request.tools) if request.tools else 0
    )
    
    try:
        # Validate API key
        if not settings.SKIP_AUTH_TOKEN:
            if not authorization.startswith("Bearer "):
                raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")
            
            api_key = authorization[7:]
            if api_key != settings.AUTH_TOKEN:
                raise HTTPException(status_code=401, detail="Invalid API key")
            
        # è½¬æ¢è¯·æ±‚
        request_dict = request.model_dump()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨å·¥å…·è°ƒç”¨
        enable_toolify = should_enable_toolify(request_dict)
        
        # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
        messages = [msg.model_dump() if hasattr(msg, 'model_dump') else msg for msg in request.messages]
        
        # å¦‚æœå¯ç”¨å·¥å…·è°ƒç”¨ï¼Œé¢„å¤„ç†æ¶ˆæ¯å¹¶æ³¨å…¥æç¤ºè¯
        if enable_toolify:
            debug_log("[TOOLIFY] å·¥å…·è°ƒç”¨åŠŸèƒ½å·²å¯ç”¨")
            messages, _ = prepare_toolify_request(request_dict, messages)
            # ä»è¯·æ±‚ä¸­ç§»é™¤ tools å’Œ tool_choiceï¼ˆå·²è½¬æ¢ä¸ºæç¤ºè¯ï¼‰
            request_dict_for_transform = request_dict.copy()
            request_dict_for_transform.pop("tools", None)
            request_dict_for_transform.pop("tool_choice", None)
            request_dict_for_transform["messages"] = messages
        else:
            request_dict_for_transform = request_dict
        
        debug_log("å¼€å§‹è½¬æ¢è¯·æ±‚æ ¼å¼: OpenAI -> Z.AI")
        
        # è·å–å…¨å±€HTTPå®¢æˆ·ç«¯ï¼ˆç”¨äºå›¾åƒä¸Šä¼ ï¼‰
        client = await get_http_client()
        
        # åˆå§‹è½¬æ¢ï¼ˆä¼ å…¥clientç”¨äºå›¾åƒä¸Šä¼ ï¼‰
        transformed = await transformer.transform_request_in(request_dict_for_transform, client=client)
        
        # æ ¹æ®streamå‚æ•°å†³å®šè¿”å›æµå¼æˆ–éæµå¼å“åº”
        if not request.stream:
            debug_log("ä½¿ç”¨éæµå¼æ¨¡å¼")
            return await handle_non_stream_request(request, transformed, enable_toolify)

        # è°ƒç”¨ä¸Šæ¸¸APIï¼ˆæµå¼æ¨¡å¼ï¼‰
        async def stream_response():
            """æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆåŒ…å«é‡è¯•æœºåˆ¶ã€æ™ºèƒ½é€€é¿ç­–ç•¥å’Œå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰"""
            nonlocal transformed  # å£°æ˜ä½¿ç”¨å¤–éƒ¨ä½œç”¨åŸŸçš„transformedå˜é‡
            retry_count = 0
            last_error = None
            last_status_code = None  # è®°å½•ä¸Šæ¬¡å¤±è´¥çš„çŠ¶æ€ç 
            
            # å¦‚æœå¯ç”¨å·¥å…·è°ƒç”¨ï¼Œåˆ›å»ºæ£€æµ‹å™¨
            toolify_detector = None
            if enable_toolify:
                toolify_instance = get_toolify()
                if toolify_instance:
                    toolify_detector = StreamingFunctionCallDetector(toolify_instance.trigger_signal)
                    debug_log("[TOOLIFY] æµå¼å·¥å…·è°ƒç”¨æ£€æµ‹å™¨å·²åˆå§‹åŒ–")

            while retry_count <= settings.MAX_RETRIES:
                try:
                    # æ™ºèƒ½é€€é¿é‡è¯•ç­–ç•¥ï¼ˆå¸¦éšæœºæŠ–åŠ¨å’Œé”™è¯¯ç±»å‹åŒºåˆ†ï¼‰
                    if retry_count > 0:
                        # ä½¿ç”¨æ™ºèƒ½é€€é¿ç­–ç•¥è®¡ç®—å»¶è¿Ÿ
                        delay = calculate_backoff_delay(
                            retry_count=retry_count,
                            status_code=last_status_code,
                            base_delay=3.0,
                            max_delay=40.0
                        )
                        debug_log(
                            f"[RETRY] é‡è¯•è¯·æ±‚ ({retry_count}/{settings.MAX_RETRIES})",
                            retry_count=retry_count,
                            max_retries=settings.MAX_RETRIES,
                            delay=f"{delay:.2f}s",
                            last_status=last_status_code
                        )
                        await asyncio.sleep(delay)

                    # è·å–å…¨å±€HTTPå®¢æˆ·ç«¯ï¼ˆå¤ç”¨è¿æ¥æ± ï¼‰
                    client = await get_http_client()
                    
                    # å‘èµ·æµå¼è¯·æ±‚ï¼ˆå¸¦æ€§èƒ½è¿½è¸ªï¼‰
                    # ä½¿ç”¨è½¬æ¢åçš„headersï¼ˆåŒ…å«Accept-Encodingï¼‰
                    headers = transformed["config"]["headers"].copy()
                    
                    request_start_time = time.perf_counter()
                    async with client.stream(
                        "POST",
                        transformed["config"]["url"],
                        json=transformed["body"],
                        headers=headers,
                    ) as response:
                        # è®°å½•é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
                        ttfb = (time.perf_counter() - request_start_time) * 1000
                        debug_log(f"â±ï¸ ä¸Šæ¸¸TTFB (é¦–å­—èŠ‚æ—¶é—´)", ttfb_ms=f"{ttfb:.2f}ms")
                        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                        if response.status_code != 200:
                            error_text = await response.aread()
                            error_msg = error_text.decode('utf-8', errors='ignore')
                            debug_log(
                                "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                                status_code=response.status_code,
                                error_detail=error_msg[:200]
                            )
                            
                            # å¯é‡è¯•çš„é”™è¯¯
                            retryable_codes = [400, 401, 429, 502, 503, 504]
                            if response.status_code in retryable_codes and retry_count < settings.MAX_RETRIES:
                                # è®°å½•çŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯ï¼Œç”¨äºæ™ºèƒ½é€€é¿ç­–ç•¥
                                last_status_code = response.status_code
                                last_error = f"{response.status_code}: {error_msg}"
                                retry_count += 1
                                
                                # å¦‚æœæ˜¯401è®¤è¯å¤±è´¥æˆ–400é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢token
                                if response.status_code in [400, 401]:
                                    debug_log("[SWITCH] æ£€æµ‹åˆ°è®¤è¯/è¯·æ±‚é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢token")
                                    new_token = transformer.switch_token()
                                    # ä½¿ç”¨æ–°tokené‡æ–°ç”Ÿæˆè¯·æ±‚
                                    transformed = await transformer.transform_request_in(request_dict, client=client)
                                    debug_log(f"[OK] å·²åˆ‡æ¢tokenå¹¶é‡æ–°ç”Ÿæˆè¯·æ±‚")
                                
                                continue
                            
                            error_response = {
                                "error": {
                                    "message": f"Upstream error: {response.status_code}",
                                    "type": "upstream_error",
                                    "code": response.status_code,
                                    "details": error_msg[:500]
                                }
                            }
                            yield f"data: {json_lib.dumps(error_response)}\n\n"
                            yield "data: [DONE]\n\n"
                            return

                        # 200 æˆåŠŸï¼Œå¤„ç†å“åº”
                        debug_log("Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç† SSE æµ", status="success")

                        # å¤„ç†çŠ¶æ€
                        has_thinking = False
                        first_thinking_chunk = True
                        accumulated_content = ""  # ç´¯ç§¯å†…å®¹ç”¨äºå·¥å…·è°ƒç”¨æ£€æµ‹
                        yielded_chunks_count = 0  # ç»Ÿè®¡yieldçš„chunkæ•°é‡
                        usage_info = None  # æš‚å­˜usageä¿¡æ¯

                        # å¤„ç†SSEæµ - ä½¿ç”¨ aiter_lines() è‡ªåŠ¨å¤„ç†è¡Œåˆ†å‰²
                        buffer = ""
                        line_count = 0
                        debug_log("å¼€å§‹è¿­ä»£SSEæµ...")

                        async for line in response.aiter_lines():
                            line_count += 1
                            if not line:
                                continue
                            
                            debug_log(f"[RAW-LINE-{line_count}] é•¿åº¦: {len(line)}, å¼€å¤´: {repr(line[:50]) if len(line) > 0 else 'EMPTY'}")

                            # ç´¯ç§¯åˆ°bufferå¤„ç†å®Œæ•´çš„æ•°æ®è¡Œ
                            buffer += line + "\n"

                            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„dataè¡Œ
                            while "\n" in buffer:
                                current_line, buffer = buffer.split("\n", 1)
                                if not current_line.strip():
                                    continue

                                if current_line.startswith("data:"):
                                    chunk_str = current_line[5:].strip()
                                    debug_log(f"[SSE-RAW] æ”¶åˆ°æ•°æ®è¡Œï¼Œé•¿åº¦: {len(chunk_str)}, é¢„è§ˆ: {chunk_str[:100] if chunk_str else 'empty'}")
                                    if not chunk_str or chunk_str == "[DONE]":
                                            if chunk_str == "[DONE]":
                                                debug_log("[SSE-RAW] æ”¶åˆ° [DONE] ä¿¡å·")
                                                # æµç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                                                if toolify_detector:
                                                    debug_log(f"[TOOLIFY] æµç»“æŸï¼Œæ£€æµ‹å™¨çŠ¶æ€: {toolify_detector.state}, ç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                    debug_log(f"[TOOLIFY] ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:500])}")
                                                    parsed_tools, remaining_content = toolify_detector.finalize()
                                                    debug_log(f"[TOOLIFY] finalize()ç»“æœ: tools={parsed_tools}, remaining={repr(remaining_content[:100]) if remaining_content else 'empty'}")
                                                    
                                                    # å…ˆè¾“å‡ºå‰©ä½™çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                                                    if remaining_content:
                                                        debug_log(f"[TOOLIFY] è¾“å‡ºç¼“å†²åŒºå‰©ä½™å†…å®¹: {len(remaining_content)}å­—ç¬¦")
                                                        if not has_thinking:
                                                            has_thinking = True
                                                            role_chunk = {
                                                                "choices": [{
                                                                    "delta": {"role": "assistant"},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                        
                                                        content_chunk = {
                                                            "choices": [{
                                                                "delta": {"content": remaining_content},
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                    
                                                    # ç„¶åå¤„ç†å·¥å…·è°ƒç”¨
                                                    if parsed_tools:
                                                        debug_log("[TOOLIFY] æµç»“æŸæ—¶æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                                                        from .toolify_handler import format_toolify_response_for_stream
                                                        tool_chunks = format_toolify_response_for_stream(
                                                            parsed_tools, 
                                                            request.model, 
                                                            transformed["body"]["chat_id"]
                                                        )
                                                        for chunk in tool_chunks:
                                                            yield chunk
                                                        return
                                                    else:
                                                        debug_log("[TOOLIFY] finalize()æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                                                yield "data: [DONE]\n\n"
                                            continue

                                    try:
                                        chunk_data = json_lib.loads(chunk_str)
                                        debug_log(f"[SSE-JSON] è§£ææˆåŠŸï¼Œtype: {chunk_data.get('type')}")

                                        if chunk_data.get("type") == "chat:completion":
                                            data = chunk_data.get("data", {})
                                            phase = data.get("phase")
                                            debug_log(f"[SSE] å¤„ç†å—: type={chunk_data.get('type')}, phase={phase}, has_delta={bool(data.get('delta_content'))}, has_usage={bool(data.get('usage'))}")

                                            # å¤„ç†tool_callé˜¶æ®µï¼ˆæå–æœç´¢ä¿¡æ¯ï¼‰
                                            if phase == "tool_call":
                                                edit_content = data.get("edit_content", "")
                                                
                                                # æå–æœç´¢æŸ¥è¯¢ä¿¡æ¯
                                                if edit_content and "<glm_block" in edit_content and "search" in edit_content:
                                                    # å°è¯•ä»edit_contentä¸­æå–æœç´¢æŸ¥è¯¢
                                                    try:
                                                        import re
                                                        # å…ˆå°è¯•ç›´æ¥è§£ç Unicode
                                                        decoded = edit_content
                                                        try:
                                                            # è§£ç \uXXXXæ ¼å¼çš„Unicodeå­—ç¬¦
                                                            decoded = edit_content.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')
                                                        except:
                                                            # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                                                            try:
                                                                import codecs
                                                                decoded = codecs.decode(edit_content, 'unicode_escape')
                                                            except:
                                                                pass
                                                        
                                                        # æå–queriesæ•°ç»„
                                                        queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                                                        if queries_match:
                                                            queries_str = queries_match.group(1)
                                                            # æå–æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
                                                            queries = re.findall(r'"([^"]+)"', queries_str)
                                                            if queries:
                                                                search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ªæŸ¥è¯¢
                                                                
                                                                if not has_thinking:
                                                                    has_thinking = True
                                                                    role_chunk = {
                                                                        "choices": [{
                                                                            "delta": {"role": "assistant"},
                                                                            "finish_reason": None,
                                                                            "index": 0,
                                                                            "logprobs": None,
                                                                        }],
                                                                        "created": int(time.time()),
                                                                        "id": transformed["body"]["chat_id"],
                                                                        "model": request.model,
                                                                        "object": "chat.completion.chunk",
                                                                        "system_fingerprint": "fp_zai_001",
                                                                    }
                                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                                
                                                                search_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"content": f"\n\n{search_info}\n\n"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yielded_chunks_count += 1
                                                                debug_log(f"[YIELD] search info chunk #{yielded_chunks_count}, queries: {queries}")
                                                                yield f"data: {json_lib.dumps(search_chunk)}\n\n"
                                                    except Exception as e:
                                                        debug_log(f"[SSE-TOOL_CALL] æå–æœç´¢ä¿¡æ¯å¤±è´¥: {e}")
                                                
                                                # å¦‚æœä¸æ˜¯usageä¿¡æ¯ï¼Œè·³è¿‡å…¶ä»–tool_callå†…å®¹
                                                if not data.get("usage"):
                                                    debug_log(f"[SSE-TOOL_CALL] è·³è¿‡tool_callé˜¶æ®µçš„å…¶ä»–å†…å®¹")
                                                    continue
                                            
                                            # å¤„ç†æ€è€ƒå†…å®¹
                                            if phase == "thinking":
                                                delta_content = data.get("delta_content", "")
                                                debug_log(f"[SSE-THINKING] delta_contenté•¿åº¦: {len(delta_content) if delta_content else 0}, å†…å®¹: {repr(delta_content[:50]) if delta_content else 'EMPTY'}")
                                                
                                                # å·¥å…·è°ƒç”¨æ£€æµ‹
                                                if toolify_detector and delta_content:
                                                    debug_log(f"[SSE-THINKING] è°ƒç”¨å·¥å…·æ£€æµ‹å™¨")
                                                    is_tool_detected, content_to_yield = toolify_detector.process_chunk(delta_content)
                                                    
                                                    if is_tool_detected:
                                                        debug_log(f"[TOOLIFY] åœ¨thinkingé˜¶æ®µæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è§¦å‘ä¿¡å·ï¼Œç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹æ—¶ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:200])}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹å™¨å½“å‰çŠ¶æ€: {toolify_detector.state}")
                                                        # å¦‚æœä¹‹å‰æœ‰è¾“å‡ºå†…å®¹ï¼Œå…ˆå‘é€
                                                        if content_to_yield:
                                                            debug_log(f"[TOOLIFY] è¾“å‡ºè§¦å‘ä¿¡å·å‰çš„å†…å®¹: {repr(content_to_yield[:100])}")
                                                            if not has_thinking:
                                                                has_thinking = True
                                                                role_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"role": "assistant"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                            
                                                            content_chunk = {
                                                                "choices": [{
                                                                    "delta": {"content": content_to_yield},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                        debug_log(f"[TOOLIFY] è·³è¿‡æœ¬æ¬¡deltaå¤„ç†ï¼Œç­‰å¾…æ›´å¤šå†…å®¹")
                                                        continue
                                                    
                                                    # ä½¿ç”¨æ£€æµ‹å™¨å¤„ç†åçš„å†…å®¹
                                                    delta_content = content_to_yield
                                                
                                                if not has_thinking:
                                                    has_thinking = True
                                                    role_chunk = {
                                                        "choices": [{
                                                            "delta": {"role": "assistant"},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                if delta_content:
                                                    if delta_content.startswith("<details"):
                                                        content = (
                                                            delta_content.split("</summary>\n>")[-1].strip()
                                                            if "</summary>\n>" in delta_content
                                                            else delta_content
                                                        )
                                                    else:
                                                        content = delta_content
                                                    
                                                    if first_thinking_chunk:
                                                        formatted_content = f"<think>{content}"
                                                        first_thinking_chunk = False
                                                    else:
                                                        formatted_content = content

                                                    thinking_chunk = {
                                                        "choices": [{
                                                            "delta": {
                                                                "role": "assistant",
                                                                "content": formatted_content,
                                                            },
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yielded_chunks_count += 1
                                                    debug_log(f"[YIELD] thinking chunk #{yielded_chunks_count}, contenté•¿åº¦: {len(formatted_content)}")
                                                    yield f"data: {json_lib.dumps(thinking_chunk)}\n\n"

                                            # å¤„ç†ç­”æ¡ˆå†…å®¹
                                            elif phase == "answer":
                                                edit_content = data.get("edit_content", "")
                                                delta_content = data.get("delta_content", "")
                                                debug_log(f"[SSE-ANSWER] delta_contenté•¿åº¦: {len(delta_content) if delta_content else 0}, edit_contenté•¿åº¦: {len(edit_content) if edit_content else 0}")
                                                
                                                # å·¥å…·è°ƒç”¨æ£€æµ‹
                                                if toolify_detector and delta_content:
                                                    debug_log(f"[SSE-ANSWER] è°ƒç”¨å·¥å…·æ£€æµ‹å™¨")
                                                    is_tool_detected, content_to_yield = toolify_detector.process_chunk(delta_content)
                                                    
                                                    if is_tool_detected:
                                                        debug_log(f"[TOOLIFY] åœ¨answeré˜¶æ®µæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è§¦å‘ä¿¡å·ï¼Œç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹æ—¶ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:200])}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹å™¨å½“å‰çŠ¶æ€: {toolify_detector.state}")
                                                        # å¦‚æœä¹‹å‰æœ‰è¾“å‡ºå†…å®¹ï¼Œå…ˆå‘é€
                                                        if content_to_yield:
                                                            debug_log(f"[TOOLIFY] è¾“å‡ºè§¦å‘ä¿¡å·å‰çš„å†…å®¹: {repr(content_to_yield[:100])}")
                                                            if not has_thinking:
                                                                has_thinking = True
                                                                role_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"role": "assistant"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                            
                                                            content_chunk = {
                                                                "choices": [{
                                                                    "delta": {"content": content_to_yield},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                        debug_log(f"[TOOLIFY] è·³è¿‡æœ¬æ¬¡deltaå¤„ç†ï¼Œç­‰å¾…æ›´å¤šå†…å®¹")
                                                        continue
                                                    
                                                    # ä½¿ç”¨æ£€æµ‹å™¨å¤„ç†åçš„å†…å®¹
                                                    delta_content = content_to_yield

                                                if not has_thinking:
                                                    has_thinking = True
                                                    role_chunk = {
                                                        "choices": [{
                                                            "delta": {"role": "assistant"},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                # å¤„ç†æ€è€ƒç»“æŸå’Œç­”æ¡ˆå¼€å§‹
                                                if edit_content and "</details>\n" in edit_content:
                                                    if has_thinking and not first_thinking_chunk:
                                                        sig_chunk = {
                                                            "choices": [{
                                                                "delta": {
                                                                    "role": "assistant",
                                                                    "content": "</think>",
                                                                },
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(sig_chunk)}\n\n"

                                                    content_after = edit_content.split("</details>\n")[-1]
                                                    if content_after:
                                                        content_chunk = {
                                                            "choices": [{
                                                                "delta": {
                                                                    "role": "assistant",
                                                                    "content": content_after,
                                                                },
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(content_chunk)}\n\n"

                                                elif delta_content:
                                                    if not has_thinking:
                                                        has_thinking = True
                                                        role_chunk = {
                                                            "choices": [{
                                                                "delta": {"role": "assistant"},
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                    content_chunk = {
                                                        "choices": [{
                                                            "delta": {"content": delta_content},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yielded_chunks_count += 1
                                                    debug_log(f"[YIELD] answer chunk #{yielded_chunks_count}, contenté•¿åº¦: {len(delta_content)}")
                                                    yield f"data: {json_lib.dumps(content_chunk)}\n\n"

                                            # æš‚å­˜usageä¿¡æ¯ï¼Œä½†ä¸ç«‹å³ç»“æŸï¼ˆå¯èƒ½åé¢è¿˜æœ‰answerå†…å®¹ï¼‰
                                            if data.get("usage"):
                                                debug_log("[SSE] æ”¶åˆ°usageä¿¡æ¯ï¼Œæš‚å­˜ä½†ç»§ç»­å¤„ç†")
                                                # æš‚å­˜usageä¿¡æ¯
                                                usage_info = data["usage"]
                                                # ä¸è¦åœ¨è¿™é‡Œreturnï¼Œç»§ç»­å¤„ç†åç»­chunks

                                    except json.JSONDecodeError as e:
                                        debug_log(
                                            "JSONè§£æé”™è¯¯",
                                            error=str(e),
                                            json_length=len(chunk_str),
                                            json_preview=chunk_str[:100] if len(chunk_str) > 100 else chunk_str
                                        )
                                    except Exception as e:
                                        debug_log("å¤„ç†chunké”™è¯¯", error=str(e))

                        debug_log("SSE æµå¤„ç†å®Œæˆ", line_count=line_count, yielded_chunks=yielded_chunks_count)
                        
                        # æµè‡ªç„¶ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                        if toolify_detector:
                            debug_log(f"[TOOLIFY] æµè‡ªç„¶ç»“æŸ - æ£€æµ‹å™¨çŠ¶æ€: {toolify_detector.state}, ç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                            parsed_tools, remaining_content = toolify_detector.finalize()
                            debug_log(f"[TOOLIFY] æµè‡ªç„¶ç»“æŸ - finalize()ç»“æœ: tools={parsed_tools}, remaining={repr(remaining_content[:100]) if remaining_content else 'empty'}")
                            
                            # å…ˆè¾“å‡ºå‰©ä½™çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                            if remaining_content:
                                debug_log(f"[TOOLIFY] è¾“å‡ºç¼“å†²åŒºå‰©ä½™å†…å®¹: {len(remaining_content)}å­—ç¬¦")
                                if not has_thinking:
                                    has_thinking = True
                                    role_chunk = {
                                        "choices": [{
                                            "delta": {"role": "assistant"},
                                            "finish_reason": None,
                                            "index": 0,
                                            "logprobs": None,
                                        }],
                                        "created": int(time.time()),
                                        "id": transformed["body"]["chat_id"],
                                        "model": request.model,
                                        "object": "chat.completion.chunk",
                                        "system_fingerprint": "fp_zai_001",
                                    }
                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                
                                content_chunk = {
                                    "choices": [{
                                        "delta": {"content": remaining_content},
                                        "finish_reason": None,
                                        "index": 0,
                                        "logprobs": None,
                                    }],
                                    "created": int(time.time()),
                                    "id": transformed["body"]["chat_id"],
                                    "model": request.model,
                                    "object": "chat.completion.chunk",
                                    "system_fingerprint": "fp_zai_001",
                                }
                                yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                            
                            # ç„¶åå¤„ç†å·¥å…·è°ƒç”¨
                            if parsed_tools:
                                debug_log("[TOOLIFY] æµè‡ªç„¶ç»“æŸ - æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºå·¥å…·è°ƒç”¨ç»“æœ")
                                from .toolify_handler import format_toolify_response_for_stream
                                tool_chunks = format_toolify_response_for_stream(
                                    parsed_tools, 
                                    request.model, 
                                    transformed["body"]["chat_id"]
                                )
                                for chunk in tool_chunks:
                                    yield chunk
                                return
                        
                        debug_log("[SSE] æµè‡ªç„¶ç»“æŸ - æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºfinish chunk")
                        # è¾“å‡ºæœ€åçš„finish chunkï¼ˆåŒ…å«usageä¿¡æ¯ï¼‰
                        finish_chunk = {
                            "choices": [{
                                "delta": {},
                                "finish_reason": "stop",
                                "index": 0,
                                "logprobs": None,
                            }],
                            "usage": usage_info if 'usage_info' in locals() else {
                                "prompt_tokens": 0,
                                "completion_tokens": 0,
                                "total_tokens": 0
                            },
                            "created": int(time.time()),
                            "id": transformed["body"]["chat_id"],
                            "model": request.model,
                            "object": "chat.completion.chunk",
                            "system_fingerprint": "fp_zai_001",
                        }
                        yield f"data: {json_lib.dumps(finish_chunk)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

                except Exception as e:
                    debug_log("æµå¤„ç†é”™è¯¯", error=str(e))
                    retry_count += 1
                    last_error = str(e)

                    if retry_count > settings.MAX_RETRIES:
                        error_response = {
                            "error": {
                                "message": f"Stream processing failed: {last_error}",
                                "type": "stream_error"
                            }
                        }
                        yield f"data: {json_lib.dumps(error_response)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

        return StreamingResponse(
            stream_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        debug_log("å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯", error=str(e))
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


def create_openai_response(chat_id: str, model: str, content: str, reasoning_content: str = "", usage: dict = None) -> dict:
    """åˆ›å»ºOpenAIæ ¼å¼çš„å“åº”å¯¹è±¡"""
    response = {
        "id": chat_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": "stop"
            }
        ],
        "usage": usage or {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }
    
    # å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œæ·»åŠ åˆ°messageä¸­
    if reasoning_content:
        response["choices"][0]["message"]["reasoning_content"] = reasoning_content
    
    return response


async def handle_non_stream_request(request: OpenAIRequest, transformed: dict, enable_toolify: bool = False) -> dict:
    """
    å¤„ç†éæµå¼è¯·æ±‚
    
    è¯´æ˜ï¼šä¸Šæ¸¸å§‹ç»ˆä»¥ SSE å½¢å¼è¿”å›ï¼ˆtransform_request_in å›ºå®š stream=Trueï¼‰ï¼Œ
    å› æ­¤è¿™é‡Œéœ€è¦èšåˆ aiter_lines() çš„ data: å—ï¼Œæå– usageã€æ€è€ƒå†…å®¹ä¸ç­”æ¡ˆå†…å®¹ï¼Œ
    å¹¶æœ€ç»ˆäº§å‡ºä¸€æ¬¡æ€§ OpenAI æ ¼å¼å“åº”ã€‚
    """
    final_content = ""
    reasoning_content = ""
    usage_info = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
    }
    
    retry_count = 0
    last_error = None
    last_status_code = None
    
    while retry_count <= settings.MAX_RETRIES:
        try:
            # æ™ºèƒ½é€€é¿é‡è¯•ç­–ç•¥
            if retry_count > 0:
                delay = calculate_backoff_delay(
                    retry_count=retry_count,
                    status_code=last_status_code,
                    base_delay=3.0,
                    max_delay=40.0
                )
                debug_log(
                    f"[RETRY] éæµå¼è¯·æ±‚é‡è¯• ({retry_count}/{settings.MAX_RETRIES})",
                    retry_count=retry_count,
                    delay=f"{delay:.2f}s"
                )
                await asyncio.sleep(delay)
            
            # è·å–å…¨å±€HTTPå®¢æˆ·ç«¯
            client = await get_http_client()
            
            # å‘èµ·æµå¼è¯·æ±‚ï¼ˆä¸Šæ¸¸å§‹ç»ˆè¿”å›SSEæµï¼Œå¸¦æ€§èƒ½è¿½è¸ªï¼‰
            # ä½¿ç”¨è½¬æ¢åçš„headersï¼ˆåŒ…å«Accept-Encodingï¼‰
            headers = transformed["config"]["headers"].copy()
            
            request_start_time = time.perf_counter()
            async with client.stream(
                "POST",
                transformed["config"]["url"],
                json=transformed["body"],
                headers=headers,
            ) as response:
                # è®°å½•éæµå¼è¯·æ±‚çš„TTFB
                ttfb = (time.perf_counter() - request_start_time) * 1000
                debug_log(f"â±ï¸ éæµå¼ä¸Šæ¸¸TTFB", ttfb_ms=f"{ttfb:.2f}ms")
                # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_msg = error_text.decode('utf-8', errors='ignore')
                    debug_log(
                        "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                        status_code=response.status_code,
                        error_detail=error_msg[:200]
                    )
                    
                    # å¯é‡è¯•çš„é”™è¯¯
                    retryable_codes = [400, 401, 429, 502, 503, 504]
                    if response.status_code in retryable_codes and retry_count < settings.MAX_RETRIES:
                        last_status_code = response.status_code
                        last_error = f"{response.status_code}: {error_msg}"
                        retry_count += 1
                        
                        # å¦‚æœæ˜¯401è®¤è¯å¤±è´¥æˆ–400é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢token
                        if response.status_code in [400, 401]:
                            debug_log("[SWITCH] æ£€æµ‹åˆ°è®¤è¯/è¯·æ±‚é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢token")
                            new_token = transformer.switch_token()
                            # ä½¿ç”¨æ–°tokené‡æ–°ç”Ÿæˆè¯·æ±‚
                            request_dict = request.model_dump()
                            transformed = await transformer.transform_request_in(request_dict, client=client)
                            debug_log(f"[OK] å·²åˆ‡æ¢tokenå¹¶é‡æ–°ç”Ÿæˆè¯·æ±‚")
                        
                        continue
                    
                    # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f"Upstream error: {error_msg[:500]}"
                    )
                
                # 200 æˆåŠŸï¼ŒèšåˆSSEæµæ•°æ®
                debug_log("Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹èšåˆéæµå¼æ•°æ®", status="success")
                
                # é‡ç½®èšåˆå˜é‡
                final_content = ""
                reasoning_content = ""
                
                async for line in response.aiter_lines():
                    if not line:
                        continue
                    
                    line = line.strip()
                    
                    # ä»…å¤„ç†ä»¥ data: å¼€å¤´çš„ SSE è¡Œ
                    if not line.startswith("data:"):
                        # å°è¯•è§£æä¸ºé”™è¯¯ JSON
                        try:
                            maybe_err = json_lib.loads(line)
                            if isinstance(maybe_err, dict) and (
                                "error" in maybe_err or "code" in maybe_err or "message" in maybe_err
                            ):
                                msg = (
                                    (maybe_err.get("error") or {}).get("message")
                                    if isinstance(maybe_err.get("error"), dict)
                                    else maybe_err.get("message")
                                ) or "ä¸Šæ¸¸è¿”å›é”™è¯¯"
                                raise HTTPException(status_code=500, detail=msg)
                        except (json.JSONDecodeError, HTTPException):
                            pass
                        continue
                    
                    data_str = line[5:].strip()
                    if not data_str or data_str in ("[DONE]", "DONE", "done"):
                        continue
                    
                    # è§£æ SSE æ•°æ®å—
                    try:
                        chunk = json_lib.loads(data_str)
                    except json.JSONDecodeError:
                        continue
                    
                    if chunk.get("type") != "chat:completion":
                        continue
                    
                    data = chunk.get("data", {})
                    phase = data.get("phase")
                    delta_content = data.get("delta_content", "")
                    edit_content = data.get("edit_content", "")
                    
                    # è®°å½•ç”¨é‡ï¼ˆé€šå¸¸åœ¨æœ€åå—ä¸­å‡ºç°ï¼‰
                    if data.get("usage"):
                        try:
                            usage_info = data["usage"]
                        except Exception:
                            pass
                    
                    # å¤„ç†tool_callé˜¶æ®µï¼ˆæå–æœç´¢ä¿¡æ¯ï¼‰
                    if phase == "tool_call":
                        edit_content = data.get("edit_content", "")
                        
                        # æå–æœç´¢æŸ¥è¯¢ä¿¡æ¯å¹¶æ·»åŠ åˆ°æœ€ç»ˆå†…å®¹
                        if edit_content and "<glm_block" in edit_content and "search" in edit_content:
                            try:
                                import re
                                # å…ˆå°è¯•ç›´æ¥è§£ç Unicode
                                decoded = edit_content
                                try:
                                    decoded = edit_content.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')
                                except:
                                    try:
                                        import codecs
                                        decoded = codecs.decode(edit_content, 'unicode_escape')
                                    except:
                                        pass
                                
                                # æå–queriesæ•°ç»„
                                queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                                if queries_match:
                                    queries_str = queries_match.group(1)
                                    queries = re.findall(r'"([^"]+)"', queries_str)
                                    if queries:
                                        search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])
                                        final_content += f"\n\n{search_info}\n\n"
                                        debug_log(f"[éæµå¼] æå–åˆ°æœç´¢ä¿¡æ¯: {queries}")
                            except Exception as e:
                                debug_log(f"[éæµå¼] æå–æœç´¢ä¿¡æ¯å¤±è´¥: {e}")
                        
                        continue
                    
                    # æ€è€ƒé˜¶æ®µèšåˆï¼ˆå»é™¤ <details><summary>... åŒ…è£¹å¤´ï¼‰
                    if phase == "thinking":
                        if delta_content:
                            if delta_content.startswith("<details"):
                                cleaned = (
                                    delta_content.split("</summary>\n>")[-1].strip()
                                    if "</summary>\n>" in delta_content
                                    else delta_content
                                )
                            else:
                                cleaned = delta_content
                            reasoning_content += cleaned
                    
                    # ç­”æ¡ˆé˜¶æ®µèšåˆ
                    elif phase == "answer":
                        # å½“ edit_content åŒæ—¶åŒ…å«æ€è€ƒç»“æŸæ ‡è®°ä¸ç­”æ¡ˆæ—¶ï¼Œæå–ç­”æ¡ˆéƒ¨åˆ†
                        if edit_content and "</details>\n" in edit_content:
                            content_after = edit_content.split("</details>\n")[-1]
                            if content_after:
                                final_content += content_after
                        elif delta_content:
                            final_content += delta_content
                
                # æ¸…ç†å¹¶è¿”å›
                final_content = (final_content or "").strip()
                reasoning_content = (reasoning_content or "").strip()
                
                # è‹¥æ²¡æœ‰èšåˆåˆ°ç­”æ¡ˆï¼Œä½†æœ‰æ€è€ƒå†…å®¹ï¼Œåˆ™ä¿åº•è¿”å›æ€è€ƒå†…å®¹
                if not final_content and reasoning_content:
                    final_content = reasoning_content
                
                # å·¥å…·è°ƒç”¨æ£€æµ‹ï¼ˆéæµå¼æ¨¡å¼ï¼‰
                if enable_toolify and final_content:
                    debug_log("[TOOLIFY] æ£€æŸ¥éæµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨")
                    tool_response = parse_toolify_response(final_content, request.model)
                    if tool_response:
                        debug_log("[TOOLIFY] éæµå¼å“åº”ä¸­æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                        # è¿”å›åŒ…å«tool_callsçš„å“åº”
                        return {
                            "id": transformed["body"]["chat_id"],
                            "object": "chat.completion",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "message": tool_response,
                                    "finish_reason": "tool_calls"
                                }
                            ],
                            "usage": usage_info
                        }
                
                debug_log(
                    "éæµå¼å“åº”èšåˆå®Œæˆ",
                    content_length=len(final_content),
                    reasoning_length=len(reasoning_content),
                    usage=usage_info
                )
                
                # è¿”å›æ ‡å‡†OpenAIæ ¼å¼å“åº”
                return create_openai_response(
                    transformed["body"]["chat_id"],
                    request.model,
                    final_content,
                    reasoning_content,
                    usage_info
                )
        
        except HTTPException:
            raise
        except Exception as e:
            debug_log("éæµå¼å¤„ç†é”™è¯¯", error=str(e))
            retry_count += 1
            last_error = str(e)
            
            if retry_count > settings.MAX_RETRIES:
                raise HTTPException(
                    status_code=500,
                    detail=f"Non-stream processing failed after {settings.MAX_RETRIES} retries: {last_error}"
                )
    
    # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
    raise HTTPException(status_code=500, detail="Unexpected error in non-stream processing")
