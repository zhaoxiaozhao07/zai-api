"""
OpenAI API endpoints - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé›†æˆ Toolify å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼‰
"""

import time
import json
import random
import asyncio
from fastapi import APIRouter, Header, HTTPException
from fastapi.responses import StreamingResponse
import httpx
from typing import Optional, Dict

from .config import settings
from .schemas import OpenAIRequest, ModelsResponse, Model
from .helpers import debug_log, get_logger, perf_timer
from .zai_transformer import ZAITransformer
from .toolify_handler import should_enable_toolify, prepare_toolify_request, parse_toolify_response
from .toolify.detector import StreamingFunctionCallDetector
from .toolify_config import get_toolify

# å°è¯•å¯¼å…¥orjsonï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼Œå¦‚æœä¸å¯ç”¨åˆ™fallbackåˆ°æ ‡å‡†json
try:
    import orjson
    
    # åˆ›å»ºorjsonå…¼å®¹å±‚
    class JSONEncoder:
        @staticmethod
        def dumps(obj, **kwargs):
            # orjson.dumpsè¿”å›bytesï¼Œéœ€è¦decode
            return orjson.dumps(obj).decode('utf-8')
        
        @staticmethod
        def loads(s, **kwargs):
            # orjson.loadså¯ä»¥æ¥å—stræˆ–bytes
            return orjson.loads(s)
    
    json_lib = JSONEncoder()
    debug_log("âœ… ä½¿ç”¨ orjson è¿›è¡Œ JSON åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰")
except ImportError:
    json_lib = json
    debug_log("âš ï¸ orjson æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡† json åº“")

router = APIRouter()

# è·å–ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨
logger = get_logger("openai_api")

# å…¨å±€è½¬æ¢å™¨å®ä¾‹
transformer = ZAITransformer()

# ================== å®¢æˆ·ç«¯æ± ç®¡ç† ==================
# ä¸ºæ¯ä¸ªä»£ç†ç»´æŠ¤ä¸€ä¸ªé•¿æœŸå®¢æˆ·ç«¯ï¼ˆå¤ç”¨è¿æ¥ï¼‰
_proxy_clients: Dict[str, httpx.AsyncClient] = {}
_default_client: Optional[httpx.AsyncClient] = None  # æ— ä»£ç†æ—¶çš„é»˜è®¤å®¢æˆ·ç«¯
_client_lock = asyncio.Lock()  # ä¿æŠ¤å®¢æˆ·ç«¯æ± çš„å¹¶å‘è®¿é—®

# ä¼˜åŒ–çš„è¿æ¥æ± é…ç½®
CONNECTION_POOL_CONFIG = {
    "limits": httpx.Limits(
        max_keepalive_connections=20,  # ä¿æŒè¿æ¥æ•°
        max_connections=100,            # æœ€å¤§è¿æ¥æ•°
        keepalive_expiry=30,           # è¿æ¥ä¿æ´»æ—¶é—´
    ),
    "timeout": httpx.Timeout(
        connect=10.0,    # è¿æ¥è¶…æ—¶
        read=30.0,       # è¯»å–è¶…æ—¶
        write=30.0,      # å†™å…¥è¶…æ—¶
        pool=10.0,       # è¿æ¥æ± è·å–è¶…æ—¶
    ),
    "http2": True,  # å¯ç”¨HTTP/2
}

async def get_or_create_client(proxy_url: Optional[str] = None) -> httpx.AsyncClient:
    """
    è·å–æˆ–åˆ›å»ºHTTPå®¢æˆ·ç«¯ï¼ˆç®€å•çš„æ± åŒ–å¤ç”¨ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        proxy_url: ä»£ç†åœ°å€ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç›´è¿
    
    Returns:
        httpx.AsyncClient: å¤ç”¨çš„HTTPå®¢æˆ·ç«¯
    """
    global _proxy_clients, _default_client
    
    async with _client_lock:  # ä¿æŠ¤å¹¶å‘è®¿é—®
        # ç›´è¿æƒ…å†µ
        if proxy_url is None:
            if _default_client is None:
                debug_log("[CLIENT] åˆ›å»ºé»˜è®¤å®¢æˆ·ç«¯ï¼ˆæ— ä»£ç†ï¼‰")
                _default_client = httpx.AsyncClient(**CONNECTION_POOL_CONFIG)
            return _default_client
        
        # ä»£ç†æƒ…å†µ
        if proxy_url not in _proxy_clients:
            debug_log(f"[CLIENT] ä¸ºä»£ç†åˆ›å»ºæ–°å®¢æˆ·ç«¯: {proxy_url}")
            _proxy_clients[proxy_url] = httpx.AsyncClient(
                proxy=proxy_url,
                **CONNECTION_POOL_CONFIG
            )
        
        return _proxy_clients[proxy_url]

async def cleanup_clients():
    """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„HTTPå®¢æˆ·ç«¯"""
    global _proxy_clients, _default_client
    
    # å…³é—­æ‰€æœ‰ä»£ç†å®¢æˆ·ç«¯
    for proxy_url, client in _proxy_clients.items():
        debug_log(f"[CLIENT] å…³é—­ä»£ç†å®¢æˆ·ç«¯: {proxy_url}")
        await client.aclose()
    _proxy_clients.clear()
    
    # å…³é—­é»˜è®¤å®¢æˆ·ç«¯
    if _default_client:
        debug_log("[CLIENT] å…³é—­é»˜è®¤å®¢æˆ·ç«¯")
        await _default_client.aclose()
        _default_client = None
    
    debug_log("[CLIENT] æ‰€æœ‰å®¢æˆ·ç«¯å·²æ¸…ç†")

# ================== ä»£ç†æ± ç®¡ç† ==================
_proxy_list = []  # ä»£ç†åˆ—è¡¨
_proxy_index = 0  # å½“å‰ä»£ç†ç´¢å¼•
_proxy_lock = asyncio.Lock()  # ä»£ç†åˆ‡æ¢é”

def init_proxy_pool():
    """åˆå§‹åŒ–ä»£ç†æ± """
    global _proxy_list
    
    # ä¼˜å…ˆä½¿ç”¨ HTTPS_PROXY_LISTï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ HTTP_PROXY_LIST
    _proxy_list = settings.HTTPS_PROXY_LIST or settings.HTTP_PROXY_LIST
    
    if _proxy_list:
        debug_log(f"[PROXY] åˆå§‹åŒ–ä»£ç†æ± ï¼Œå…± {len(_proxy_list)} ä¸ªä»£ç†ï¼Œç­–ç•¥: {settings.PROXY_STRATEGY}")
        for i, proxy in enumerate(_proxy_list):
            debug_log(f"  ä»£ç† {i+1}: {proxy}")

def get_next_proxy() -> Optional[str]:
    """
    è·å–ä¸‹ä¸€ä¸ªä»£ç†ï¼ˆæ ¹æ®ç­–ç•¥ï¼‰
    
    Returns:
        Optional[str]: ä»£ç†åœ°å€ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨ä»£ç†åˆ™è¿”å› None
    """
    global _proxy_index
    
    if not _proxy_list:
        return None
    
    if settings.PROXY_STRATEGY == "round-robin":
        # è½®è¯¢ç­–ç•¥ï¼šæ¯æ¬¡è°ƒç”¨éƒ½åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªä»£ç†
        proxy = _proxy_list[_proxy_index]
        _proxy_index = (_proxy_index + 1) % len(_proxy_list)
        debug_log(f"[PROXY] Round-robiné€‰æ‹©ä»£ç† {_proxy_index}: {proxy}")
        return proxy
    else:
        # failover ç­–ç•¥ï¼šå§‹ç»ˆä½¿ç”¨å½“å‰ç´¢å¼•çš„ä»£ç†
        proxy = _proxy_list[_proxy_index]
        debug_log(f"[PROXY] Failoverä½¿ç”¨ä»£ç† {_proxy_index}: {proxy}")
        return proxy

async def switch_proxy_on_failure():
    """
    å¤±è´¥æ—¶åˆ‡æ¢ä»£ç†ï¼ˆä»… failover ç­–ç•¥éœ€è¦ï¼‰
    """
    global _proxy_index
    
    if not _proxy_list or settings.PROXY_STRATEGY != "failover":
        return
    
    async with _proxy_lock:
        old_index = _proxy_index
        _proxy_index = (_proxy_index + 1) % len(_proxy_list)
        debug_log(f"[PROXY] ä»£ç†å¤±è´¥ï¼Œä» {old_index} åˆ‡æ¢åˆ° {_proxy_index}: {_proxy_list[_proxy_index]}")

# åˆå§‹åŒ–ä»£ç†æ± 
init_proxy_pool()

async def get_request_client() -> httpx.AsyncClient:
    """
    ä¸ºå½“å‰è¯·æ±‚è·å–åˆé€‚çš„HTTPå®¢æˆ·ç«¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    ä½¿ç”¨å®¢æˆ·ç«¯æ± å¤ç”¨è¿æ¥ï¼Œæ¯ä¸ªä»£ç†ç»´æŠ¤ä¸€ä¸ªé•¿æœŸå®¢æˆ·ç«¯
    
    Returns:
        httpx.AsyncClient: å¤ç”¨çš„HTTPå®¢æˆ·ç«¯
    """
    # è·å–å½“å‰åº”è¯¥ä½¿ç”¨çš„ä»£ç†
    proxy = get_next_proxy()
    
    # è·å–æˆ–åˆ›å»ºå¯¹åº”çš„å®¢æˆ·ç«¯
    return await get_or_create_client(proxy)

def calculate_backoff_delay(retry_count: int, status_code: int = None, base_delay: float = 1.5, max_delay: float = 8.0) -> float:
    """
    è®¡ç®—é€€é¿å»¶è¿Ÿæ—¶é—´ï¼ˆå¸¦éšæœºæŠ–åŠ¨å’Œé’ˆå¯¹ä¸åŒé”™è¯¯çš„ç­–ç•¥ï¼‰
    
    Args:
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°ï¼ˆä»1å¼€å§‹ï¼‰
        status_code: HTTPçŠ¶æ€ç ï¼Œç”¨äºåŒºåˆ†ä¸åŒé”™è¯¯ç±»å‹
        base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        è®¡ç®—åçš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    # ä½¿ç”¨çº¿æ€§å¢é•¿è€ŒéæŒ‡æ•°å¢é•¿ï¼šbase_delay * retry_count
    # è¿™æ ·ï¼šç¬¬1æ¬¡=1.5s, ç¬¬2æ¬¡=3s, ç¬¬3æ¬¡=4.5s, ç¬¬4æ¬¡=6s
    linear_delay = base_delay * retry_count
    
    # é’ˆå¯¹ä¸åŒé”™è¯¯ç±»å‹è°ƒæ•´å»¶è¿Ÿ
    if status_code == 429:  # Rate limit - æ›´é•¿çš„é€€é¿æ—¶é—´
        linear_delay *= 1.5  # 1.5å€å»¶è¿Ÿ
        debug_log("[BACKOFF] æ£€æµ‹åˆ°429é™æµé”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿é€€é¿æ—¶é—´")
    elif status_code in [502, 503, 504]:  # æœåŠ¡ç«¯é”™è¯¯ - é€‚ä¸­å»¶è¿Ÿ
        linear_delay *= 1.2  # 1.2å€å»¶è¿Ÿ
        debug_log("[BACKOFF] æ£€æµ‹åˆ°æœåŠ¡ç«¯é”™è¯¯ï¼Œä½¿ç”¨é€‚ä¸­é€€é¿æ—¶é—´")
    elif status_code in [400, 401, 405]:  # è®¤è¯/è¯·æ±‚é”™è¯¯ - æ ‡å‡†å»¶è¿Ÿ
        debug_log(f"[BACKOFF] æ£€æµ‹åˆ°{status_code}è®¤è¯/è¯·æ±‚é”™è¯¯ï¼Œä½¿ç”¨æ ‡å‡†é€€é¿æ—¶é—´")
    
    # é™åˆ¶æœ€å¤§å»¶è¿Ÿ
    linear_delay = min(linear_delay, max_delay)
    
    # æ·»åŠ éšæœºæŠ–åŠ¨å› å­ï¼ˆÂ±20%ï¼‰ï¼Œé¿å…å¤šä¸ªè¯·æ±‚åŒæ—¶é‡è¯•é€ æˆé›ªå´©
    jitter = linear_delay * 0.2  # 20%çš„æŠ–åŠ¨èŒƒå›´
    jittered_delay = linear_delay + random.uniform(-jitter, jitter)
    
    # ç¡®ä¿å»¶è¿Ÿä¸ä¼šå°äº0.5ç§’
    final_delay = max(jittered_delay, 0.5)
    
    debug_log(
        "[BACKOFF] è®¡ç®—é€€é¿å»¶è¿Ÿ",
        retry_count=retry_count,
        status_code=status_code,
        linear=f"{linear_delay:.2f}s",
        final=f"{final_delay:.2f}s"
    )
    
    return final_delay


@router.get("/v1/models")
async def list_models():
    """List available models"""
    current_time = int(time.time())
    response = ModelsResponse(
        data=[
            Model(id=settings.PRIMARY_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.THINKING_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.SEARCH_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.AIR_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_45V_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_THINKING_MODEL, created=current_time, owned_by="z.ai"),
            Model(id=settings.GLM_46_SEARCH_MODEL, created=current_time, owned_by="z.ai"),
        ]
    )
    return response


@router.post("/v1/chat/completions")
async def chat_completions(request: OpenAIRequest, authorization: str = Header(...)):
    """Handle chat completion requests with ZAI transformer - æ”¯æŒæµå¼å’Œéæµå¼ä»¥åŠå·¥å…·è°ƒç”¨"""
    role = request.messages[0].role if request.messages else "unknown"
    debug_log(
        "æ”¶åˆ°å®¢æˆ·ç«¯è¯·æ±‚",
        model=request.model,
        stream=request.stream,
        message_count=len(request.messages),
        tools_count=len(request.tools) if request.tools else 0
    )
    
    # è·å–å¤ç”¨çš„HTTPå®¢æˆ·ç«¯
    request_client = None
    
    try:
        # Validate API key
        if not settings.SKIP_AUTH_TOKEN:
            if not authorization.startswith("Bearer "):
                raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")
            
            api_key = authorization[7:]
            if api_key != settings.AUTH_TOKEN:
                raise HTTPException(status_code=401, detail="Invalid API key")
        
        # è·å–å¤ç”¨çš„HTTPå®¢æˆ·ç«¯
        request_client = await get_request_client()
        debug_log("[REQUEST] è·å–å¤ç”¨HTTPå®¢æˆ·ç«¯")
            
        # è½¬æ¢è¯·æ±‚
        request_dict = request.model_dump()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨å·¥å…·è°ƒç”¨
        enable_toolify = should_enable_toolify(request_dict)
        
        # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
        messages = [msg.model_dump() if hasattr(msg, 'model_dump') else msg for msg in request.messages]
        
        # å¦‚æœå¯ç”¨å·¥å…·è°ƒç”¨ï¼Œé¢„å¤„ç†æ¶ˆæ¯å¹¶æ³¨å…¥æç¤ºè¯
        if enable_toolify:
            debug_log("[TOOLIFY] å·¥å…·è°ƒç”¨åŠŸèƒ½å·²å¯ç”¨")
            messages, _ = prepare_toolify_request(request_dict, messages)
            # ä»è¯·æ±‚ä¸­ç§»é™¤ tools å’Œ tool_choiceï¼ˆå·²è½¬æ¢ä¸ºæç¤ºè¯ï¼‰
            request_dict_for_transform = request_dict.copy()
            request_dict_for_transform.pop("tools", None)
            request_dict_for_transform.pop("tool_choice", None)
            request_dict_for_transform["messages"] = messages
        else:
            request_dict_for_transform = request_dict
        
        debug_log("å¼€å§‹è½¬æ¢è¯·æ±‚æ ¼å¼: OpenAI -> Z.AI")
        
        # åˆå§‹è½¬æ¢ï¼ˆä¼ å…¥request_clientç”¨äºå›¾åƒä¸Šä¼ å’ŒTokenè·å–ï¼‰
        transformed = await transformer.transform_request_in(request_dict_for_transform, client=request_client)
        
        # æ ¹æ®streamå‚æ•°å†³å®šè¿”å›æµå¼æˆ–éæµå¼å“åº”
        if not request.stream:
            debug_log("ä½¿ç”¨éæµå¼æ¨¡å¼")
            result = await handle_non_stream_request(request, transformed, enable_toolify, request_client)
            return result

        # è°ƒç”¨ä¸Šæ¸¸APIï¼ˆæµå¼æ¨¡å¼ï¼‰
        async def stream_response():
            """æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆåŒ…å«é‡è¯•æœºåˆ¶ã€æ™ºèƒ½é€€é¿ç­–ç•¥å’Œå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰"""
            nonlocal transformed  # å£°æ˜ä½¿ç”¨å¤–éƒ¨ä½œç”¨åŸŸçš„transformedå˜é‡
            nonlocal request_client  # å£°æ˜ä½¿ç”¨å¤–éƒ¨ä½œç”¨åŸŸçš„request_clientå˜é‡
            retry_count = 0
            last_error = None
            last_status_code = None  # è®°å½•ä¸Šæ¬¡å¤±è´¥çš„çŠ¶æ€ç 
            
            # å¦‚æœå¯ç”¨å·¥å…·è°ƒç”¨ï¼Œåˆ›å»ºæ£€æµ‹å™¨
            toolify_detector = None
            if enable_toolify:
                toolify_instance = get_toolify()
                if toolify_instance:
                    toolify_detector = StreamingFunctionCallDetector(toolify_instance.trigger_signal)
                    debug_log("[TOOLIFY] æµå¼å·¥å…·è°ƒç”¨æ£€æµ‹å™¨å·²åˆå§‹åŒ–")

            while retry_count <= settings.MAX_RETRIES:
                try:
                    # æ™ºèƒ½é€€é¿é‡è¯•ç­–ç•¥ï¼ˆå¸¦éšæœºæŠ–åŠ¨å’Œé”™è¯¯ç±»å‹åŒºåˆ†ï¼‰
                    if retry_count > 0:
                        # ä½¿ç”¨æ™ºèƒ½é€€é¿ç­–ç•¥è®¡ç®—å»¶è¿Ÿï¼ˆçº¿æ€§å¢é•¿ï¼š1.5s, 3s, 4.5s, 6s...ï¼‰
                        delay = calculate_backoff_delay(
                            retry_count=retry_count,
                            status_code=last_status_code
                        )
                        debug_log(
                            f"[RETRY] é‡è¯•è¯·æ±‚ ({retry_count}/{settings.MAX_RETRIES})",
                            retry_count=retry_count,
                            max_retries=settings.MAX_RETRIES,
                            delay=f"{delay:.2f}s",
                            last_status=last_status_code
                        )
                        await asyncio.sleep(delay)

                    # ä½¿ç”¨è¯·æ±‚ä¸“ç”¨çš„HTTPå®¢æˆ·ç«¯
                    client = request_client
                    
                    # å‘èµ·æµå¼è¯·æ±‚ï¼ˆå¸¦æ€§èƒ½è¿½è¸ªï¼‰
                    # ä½¿ç”¨è½¬æ¢åçš„headersï¼ˆåŒ…å«Accept-Encodingï¼‰
                    headers = transformed["config"]["headers"].copy()
                    
                    request_start_time = time.perf_counter()
                    async with client.stream(
                        "POST",
                        transformed["config"]["url"],
                        json=transformed["body"],
                        headers=headers,
                    ) as response:
                        # è®°å½•é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
                        ttfb = (time.perf_counter() - request_start_time) * 1000
                        debug_log(f"â±ï¸ ä¸Šæ¸¸TTFB (é¦–å­—èŠ‚æ—¶é—´)", ttfb_ms=f"{ttfb:.2f}ms")
                        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                        if response.status_code != 200:
                            error_text = await response.aread()
                            error_msg = error_text.decode('utf-8', errors='ignore')
                            debug_log(
                                "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                                status_code=response.status_code,
                                error_detail=error_msg[:200]
                            )
                            
                            # å¯é‡è¯•çš„é”™è¯¯
                            retryable_codes = [400, 401, 405, 429, 502, 503, 504]
                            if response.status_code in retryable_codes and retry_count < settings.MAX_RETRIES:
                                # è®°å½•çŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯
                                last_status_code = response.status_code
                                last_error = f"{response.status_code}: {error_msg}"
                                retry_count += 1
                                
                                # æ£€æŸ¥å½“å‰æ˜¯å¦ä½¿ç”¨åŒ¿åToken
                                from .token_pool import get_token_pool
                                token_pool = get_token_pool()
                                current_token = transformed.get("token", "")
                                is_anonymous = token_pool.is_anonymous_token(current_token)
                                
                                if is_anonymous:
                                    # åŒ¿åTokenï¼šä»»ä½•é”™è¯¯éƒ½æ¸…ç†ç¼“å­˜å¹¶é‡æ–°è·å–
                                    debug_log(f"[ANONYMOUS] æ£€æµ‹åˆ°åŒ¿åTokené”™è¯¯ {response.status_code}ï¼Œæ¸…ç†ç¼“å­˜å¹¶é‡æ–°è·å–")
                                    await transformer.clear_anonymous_token_cache()
                                    
                                    # æ¸…ç†å®¢æˆ·ç«¯ç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°åˆ›å»ºï¼ˆåŒ¿åTokenå¯èƒ½å¯¼è‡´è¿æ¥çŠ¶æ€å¼‚å¸¸ï¼‰
                                    debug_log("[CLIENT] æ¸…ç†HTTPå®¢æˆ·ç«¯ç¼“å­˜ï¼Œä¸‹æ¬¡è¯·æ±‚å°†åˆ›å»ºæ–°å®¢æˆ·ç«¯")
                                    await cleanup_clients()
                                    
                                    # é‡æ–°è·å–å®¢æˆ·ç«¯
                                    request_client = await get_request_client()
                                    
                                    # ä½¿ç”¨æ–°çš„åŒ¿åTokené‡æ–°ç”Ÿæˆè¯·æ±‚
                                    transformed = await transformer.transform_request_in(request_dict_for_transform, client=request_client)
                                    debug_log("[OK] å·²è·å–æ–°çš„åŒ¿åTokenå¹¶é‡æ–°ç”Ÿæˆè¯·æ±‚")
                                else:
                                    # é…ç½®Tokenï¼šæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
                                    if response.status_code in [400, 401, 405]:
                                        debug_log(f"[CONFIG] é…ç½®Tokené”™è¯¯ {response.status_code}ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªToken")
                                        new_token = await transformer.switch_token()
                                        transformer.refresh_header_template()
                                        transformed = await transformer.transform_request_in(request_dict_for_transform, client=request_client)
                                        debug_log(f"[OK] å·²åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé…ç½®Token")
                                    
                                    # ç½‘ç»œé”™è¯¯æ—¶å°è¯•åˆ‡æ¢ä»£ç†ï¼ˆä»…é™ failover ç­–ç•¥ï¼‰
                                    if response.status_code in [502, 503, 504] and _proxy_list:
                                        await switch_proxy_on_failure()
                                
                                continue
                            
                            error_response = {
                                "error": {
                                    "message": f"Upstream error: {response.status_code}",
                                    "type": "upstream_error",
                                    "code": response.status_code,
                                    "details": error_msg[:500]
                                }
                            }
                            yield f"data: {json_lib.dumps(error_response)}\n\n"
                            yield "data: [DONE]\n\n"
                            return

                        # 200 æˆåŠŸï¼Œå¤„ç†å“åº”
                        debug_log("Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç† SSE æµ", status="success")

                        # å¤„ç†çŠ¶æ€
                        has_thinking = False
                        first_thinking_chunk = True
                        accumulated_content = ""  # ç´¯ç§¯å†…å®¹ç”¨äºå·¥å…·è°ƒç”¨æ£€æµ‹
                        yielded_chunks_count = 0  # ç»Ÿè®¡yieldçš„chunkæ•°é‡
                        usage_info = None  # æš‚å­˜usageä¿¡æ¯

                        # å¤„ç†SSEæµ - ä½¿ç”¨ aiter_lines() è‡ªåŠ¨å¤„ç†è¡Œåˆ†å‰²
                        buffer = ""
                        line_count = 0
                        debug_log("å¼€å§‹è¿­ä»£SSEæµ...")

                        async for line in response.aiter_lines():
                            line_count += 1
                            if not line:
                                continue
                            
                            debug_log(f"[RAW-LINE-{line_count}] é•¿åº¦: {len(line)}, å¼€å¤´: {repr(line[:50]) if len(line) > 0 else 'EMPTY'}")

                            # ç´¯ç§¯åˆ°bufferå¤„ç†å®Œæ•´çš„æ•°æ®è¡Œ
                            buffer += line + "\n"

                            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„dataè¡Œ
                            while "\n" in buffer:
                                current_line, buffer = buffer.split("\n", 1)
                                if not current_line.strip():
                                    continue

                                if current_line.startswith("data:"):
                                    chunk_str = current_line[5:].strip()
                                    debug_log(f"[SSE-RAW] æ”¶åˆ°æ•°æ®è¡Œï¼Œé•¿åº¦: {len(chunk_str)}, é¢„è§ˆ: {chunk_str[:100] if chunk_str else 'empty'}")
                                    if not chunk_str or chunk_str == "[DONE]":
                                            if chunk_str == "[DONE]":
                                                debug_log("[SSE-RAW] æ”¶åˆ° [DONE] ä¿¡å·")
                                                # æµç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                                                if toolify_detector:
                                                    debug_log(f"[TOOLIFY] æµç»“æŸï¼Œæ£€æµ‹å™¨çŠ¶æ€: {toolify_detector.state}, ç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                    debug_log(f"[TOOLIFY] ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:500])}")
                                                    parsed_tools, remaining_content = toolify_detector.finalize()
                                                    debug_log(f"[TOOLIFY] finalize()ç»“æœ: tools={parsed_tools}, remaining={repr(remaining_content[:100]) if remaining_content else 'empty'}")
                                                    
                                                    # å…ˆè¾“å‡ºå‰©ä½™çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                                                    if remaining_content:
                                                        debug_log(f"[TOOLIFY] è¾“å‡ºç¼“å†²åŒºå‰©ä½™å†…å®¹: {len(remaining_content)}å­—ç¬¦")
                                                        if not has_thinking:
                                                            has_thinking = True
                                                            role_chunk = {
                                                                "choices": [{
                                                                    "delta": {"role": "assistant"},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                        
                                                        content_chunk = {
                                                            "choices": [{
                                                                "delta": {"content": remaining_content},
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                    
                                                    # ç„¶åå¤„ç†å·¥å…·è°ƒç”¨
                                                    if parsed_tools:
                                                        debug_log("[TOOLIFY] æµç»“æŸæ—¶æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                                                        from .toolify_handler import format_toolify_response_for_stream
                                                        tool_chunks = format_toolify_response_for_stream(
                                                            parsed_tools, 
                                                            request.model, 
                                                            transformed["body"]["chat_id"]
                                                        )
                                                        for chunk in tool_chunks:
                                                            yield chunk
                                                        # æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨åæå‰ç»“æŸ
                                                        debug_log("[REQUEST] æµå¼å“åº”ï¼ˆæ—©æœŸå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰å®Œæˆ")
                                                        return
                                                    else:
                                                        debug_log("[TOOLIFY] finalize()æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                                                yield "data: [DONE]\n\n"
                                            continue

                                    try:
                                        chunk_data = json_lib.loads(chunk_str)
                                        debug_log(f"[SSE-JSON] è§£ææˆåŠŸï¼Œtype: {chunk_data.get('type')}")

                                        if chunk_data.get("type") == "chat:completion":
                                            data = chunk_data.get("data", {})
                                            phase = data.get("phase")
                                            debug_log(f"[SSE] å¤„ç†å—: type={chunk_data.get('type')}, phase={phase}, has_delta={bool(data.get('delta_content'))}, has_usage={bool(data.get('usage'))}")

                                            # å¤„ç†tool_callé˜¶æ®µï¼ˆæå–æœç´¢ä¿¡æ¯ï¼‰
                                            if phase == "tool_call":
                                                edit_content = data.get("edit_content", "")
                                                
                                                # æå–æœç´¢æŸ¥è¯¢ä¿¡æ¯
                                                if edit_content and "<glm_block" in edit_content and "search" in edit_content:
                                                    # å°è¯•ä»edit_contentä¸­æå–æœç´¢æŸ¥è¯¢
                                                    try:
                                                        import re
                                                        # å…ˆå°è¯•ç›´æ¥è§£ç Unicode
                                                        decoded = edit_content
                                                        try:
                                                            # è§£ç \uXXXXæ ¼å¼çš„Unicodeå­—ç¬¦
                                                            decoded = edit_content.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')
                                                        except:
                                                            # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                                                            try:
                                                                import codecs
                                                                decoded = codecs.decode(edit_content, 'unicode_escape')
                                                            except:
                                                                pass
                                                        
                                                        # æå–queriesæ•°ç»„
                                                        queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                                                        if queries_match:
                                                            queries_str = queries_match.group(1)
                                                            # æå–æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
                                                            queries = re.findall(r'"([^"]+)"', queries_str)
                                                            if queries:
                                                                search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ªæŸ¥è¯¢
                                                                
                                                                if not has_thinking:
                                                                    has_thinking = True
                                                                    role_chunk = {
                                                                        "choices": [{
                                                                            "delta": {"role": "assistant"},
                                                                            "finish_reason": None,
                                                                            "index": 0,
                                                                            "logprobs": None,
                                                                        }],
                                                                        "created": int(time.time()),
                                                                        "id": transformed["body"]["chat_id"],
                                                                        "model": request.model,
                                                                        "object": "chat.completion.chunk",
                                                                        "system_fingerprint": "fp_zai_001",
                                                                    }
                                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                                
                                                                search_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"content": f"\n\n{search_info}\n\n"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yielded_chunks_count += 1
                                                                debug_log(f"[YIELD] search info chunk #{yielded_chunks_count}, queries: {queries}")
                                                                yield f"data: {json_lib.dumps(search_chunk)}\n\n"
                                                    except Exception as e:
                                                        debug_log(f"[SSE-TOOL_CALL] æå–æœç´¢ä¿¡æ¯å¤±è´¥: {e}")
                                                
                                                # å¦‚æœä¸æ˜¯usageä¿¡æ¯ï¼Œè·³è¿‡å…¶ä»–tool_callå†…å®¹
                                                if not data.get("usage"):
                                                    debug_log(f"[SSE-TOOL_CALL] è·³è¿‡tool_callé˜¶æ®µçš„å…¶ä»–å†…å®¹")
                                                    continue
                                            
                                            # å¤„ç†æ€è€ƒå†…å®¹
                                            if phase == "thinking":
                                                delta_content = data.get("delta_content", "")
                                                debug_log(f"[SSE-THINKING] delta_contenté•¿åº¦: {len(delta_content) if delta_content else 0}, å†…å®¹: {repr(delta_content[:50]) if delta_content else 'EMPTY'}")
                                                
                                                # å·¥å…·è°ƒç”¨æ£€æµ‹
                                                if toolify_detector and delta_content:
                                                    debug_log(f"[SSE-THINKING] è°ƒç”¨å·¥å…·æ£€æµ‹å™¨")
                                                    is_tool_detected, content_to_yield = toolify_detector.process_chunk(delta_content)
                                                    
                                                    if is_tool_detected:
                                                        debug_log(f"[TOOLIFY] åœ¨thinkingé˜¶æ®µæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è§¦å‘ä¿¡å·ï¼Œç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹æ—¶ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:200])}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹å™¨å½“å‰çŠ¶æ€: {toolify_detector.state}")
                                                        # å¦‚æœä¹‹å‰æœ‰è¾“å‡ºå†…å®¹ï¼Œå…ˆå‘é€
                                                        if content_to_yield:
                                                            debug_log(f"[TOOLIFY] è¾“å‡ºè§¦å‘ä¿¡å·å‰çš„å†…å®¹: {repr(content_to_yield[:100])}")
                                                            if not has_thinking:
                                                                has_thinking = True
                                                                role_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"role": "assistant"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                            
                                                            content_chunk = {
                                                                "choices": [{
                                                                    "delta": {"content": content_to_yield},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                        debug_log(f"[TOOLIFY] è·³è¿‡æœ¬æ¬¡deltaå¤„ç†ï¼Œç­‰å¾…æ›´å¤šå†…å®¹")
                                                        continue
                                                    
                                                    # ä½¿ç”¨æ£€æµ‹å™¨å¤„ç†åçš„å†…å®¹
                                                    delta_content = content_to_yield
                                                
                                                if not has_thinking:
                                                    has_thinking = True
                                                    role_chunk = {
                                                        "choices": [{
                                                            "delta": {"role": "assistant"},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                if delta_content:
                                                    if delta_content.startswith("<details"):
                                                        content = (
                                                            delta_content.split("</summary>\n>")[-1].strip()
                                                            if "</summary>\n>" in delta_content
                                                            else delta_content
                                                        )
                                                    else:
                                                        content = delta_content
                                                    
                                                    if first_thinking_chunk:
                                                        formatted_content = f"<think>{content}"
                                                        first_thinking_chunk = False
                                                    else:
                                                        formatted_content = content

                                                    thinking_chunk = {
                                                        "choices": [{
                                                            "delta": {
                                                                "role": "assistant",
                                                                "content": formatted_content,
                                                            },
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yielded_chunks_count += 1
                                                    debug_log(f"[YIELD] thinking chunk #{yielded_chunks_count}, contenté•¿åº¦: {len(formatted_content)}")
                                                    yield f"data: {json_lib.dumps(thinking_chunk)}\n\n"

                                            # å¤„ç†ç­”æ¡ˆå†…å®¹
                                            elif phase == "answer":
                                                edit_content = data.get("edit_content", "")
                                                delta_content = data.get("delta_content", "")
                                                debug_log(f"[SSE-ANSWER] delta_contenté•¿åº¦: {len(delta_content) if delta_content else 0}, edit_contenté•¿åº¦: {len(edit_content) if edit_content else 0}")
                                                
                                                # å·¥å…·è°ƒç”¨æ£€æµ‹
                                                if toolify_detector and delta_content:
                                                    debug_log(f"[SSE-ANSWER] è°ƒç”¨å·¥å…·æ£€æµ‹å™¨")
                                                    is_tool_detected, content_to_yield = toolify_detector.process_chunk(delta_content)
                                                    
                                                    if is_tool_detected:
                                                        debug_log(f"[TOOLIFY] åœ¨answeré˜¶æ®µæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è§¦å‘ä¿¡å·ï¼Œç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹æ—¶ç¼“å†²åŒºå†…å®¹: {repr(toolify_detector.content_buffer[:200])}")
                                                        debug_log(f"[TOOLIFY] æ£€æµ‹å™¨å½“å‰çŠ¶æ€: {toolify_detector.state}")
                                                        # å¦‚æœä¹‹å‰æœ‰è¾“å‡ºå†…å®¹ï¼Œå…ˆå‘é€
                                                        if content_to_yield:
                                                            debug_log(f"[TOOLIFY] è¾“å‡ºè§¦å‘ä¿¡å·å‰çš„å†…å®¹: {repr(content_to_yield[:100])}")
                                                            if not has_thinking:
                                                                has_thinking = True
                                                                role_chunk = {
                                                                    "choices": [{
                                                                        "delta": {"role": "assistant"},
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "logprobs": None,
                                                                    }],
                                                                    "created": int(time.time()),
                                                                    "id": transformed["body"]["chat_id"],
                                                                    "model": request.model,
                                                                    "object": "chat.completion.chunk",
                                                                    "system_fingerprint": "fp_zai_001",
                                                                }
                                                                yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                                            
                                                            content_chunk = {
                                                                "choices": [{
                                                                    "delta": {"content": content_to_yield},
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "logprobs": None,
                                                                }],
                                                                "created": int(time.time()),
                                                                "id": transformed["body"]["chat_id"],
                                                                "model": request.model,
                                                                "object": "chat.completion.chunk",
                                                                "system_fingerprint": "fp_zai_001",
                                                            }
                                                            yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                                                        debug_log(f"[TOOLIFY] è·³è¿‡æœ¬æ¬¡deltaå¤„ç†ï¼Œç­‰å¾…æ›´å¤šå†…å®¹")
                                                        continue
                                                    
                                                    # ä½¿ç”¨æ£€æµ‹å™¨å¤„ç†åçš„å†…å®¹
                                                    delta_content = content_to_yield

                                                if not has_thinking:
                                                    has_thinking = True
                                                    role_chunk = {
                                                        "choices": [{
                                                            "delta": {"role": "assistant"},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                # å¤„ç†æ€è€ƒç»“æŸå’Œç­”æ¡ˆå¼€å§‹
                                                if edit_content and "</details>\n" in edit_content:
                                                    if has_thinking and not first_thinking_chunk:
                                                        sig_chunk = {
                                                            "choices": [{
                                                                "delta": {
                                                                    "role": "assistant",
                                                                    "content": "</think>",
                                                                },
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(sig_chunk)}\n\n"

                                                    content_after = edit_content.split("</details>\n")[-1]
                                                    if content_after:
                                                        content_chunk = {
                                                            "choices": [{
                                                                "delta": {
                                                                    "role": "assistant",
                                                                    "content": content_after,
                                                                },
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(content_chunk)}\n\n"

                                                elif delta_content:
                                                    if not has_thinking:
                                                        has_thinking = True
                                                        role_chunk = {
                                                            "choices": [{
                                                                "delta": {"role": "assistant"},
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "logprobs": None,
                                                            }],
                                                            "created": int(time.time()),
                                                            "id": transformed["body"]["chat_id"],
                                                            "model": request.model,
                                                            "object": "chat.completion.chunk",
                                                            "system_fingerprint": "fp_zai_001",
                                                        }
                                                        yield f"data: {json_lib.dumps(role_chunk)}\n\n"

                                                    content_chunk = {
                                                        "choices": [{
                                                            "delta": {"content": delta_content},
                                                            "finish_reason": None,
                                                            "index": 0,
                                                            "logprobs": None,
                                                        }],
                                                        "created": int(time.time()),
                                                        "id": transformed["body"]["chat_id"],
                                                        "model": request.model,
                                                        "object": "chat.completion.chunk",
                                                        "system_fingerprint": "fp_zai_001",
                                                    }
                                                    yielded_chunks_count += 1
                                                    debug_log(f"[YIELD] answer chunk #{yielded_chunks_count}, contenté•¿åº¦: {len(delta_content)}")
                                                    yield f"data: {json_lib.dumps(content_chunk)}\n\n"

                                            # æš‚å­˜usageä¿¡æ¯ï¼Œä½†ä¸ç«‹å³ç»“æŸï¼ˆå¯èƒ½åé¢è¿˜æœ‰answerå†…å®¹ï¼‰
                                            if data.get("usage"):
                                                debug_log("[SSE] æ”¶åˆ°usageä¿¡æ¯ï¼Œæš‚å­˜ä½†ç»§ç»­å¤„ç†")
                                                # æš‚å­˜usageä¿¡æ¯
                                                usage_info = data["usage"]
                                                # ä¸è¦åœ¨è¿™é‡Œreturnï¼Œç»§ç»­å¤„ç†åç»­chunks

                                    except json.JSONDecodeError as e:
                                        debug_log(
                                            "JSONè§£æé”™è¯¯",
                                            error=str(e),
                                            json_length=len(chunk_str),
                                            json_preview=chunk_str[:100] if len(chunk_str) > 100 else chunk_str
                                        )
                                    except Exception as e:
                                        debug_log("å¤„ç†chunké”™è¯¯", error=str(e))

                        debug_log("SSE æµå¤„ç†å®Œæˆ", line_count=line_count, yielded_chunks=yielded_chunks_count)
                        
                        # æµè‡ªç„¶ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                        if toolify_detector:
                            debug_log(f"[TOOLIFY] æµè‡ªç„¶ç»“æŸ - æ£€æµ‹å™¨çŠ¶æ€: {toolify_detector.state}, ç¼“å†²åŒºé•¿åº¦: {len(toolify_detector.content_buffer)}")
                            parsed_tools, remaining_content = toolify_detector.finalize()
                            debug_log(f"[TOOLIFY] æµè‡ªç„¶ç»“æŸ - finalize()ç»“æœ: tools={parsed_tools}, remaining={repr(remaining_content[:100]) if remaining_content else 'empty'}")
                            
                            # å…ˆè¾“å‡ºå‰©ä½™çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                            if remaining_content:
                                debug_log(f"[TOOLIFY] è¾“å‡ºç¼“å†²åŒºå‰©ä½™å†…å®¹: {len(remaining_content)}å­—ç¬¦")
                                if not has_thinking:
                                    has_thinking = True
                                    role_chunk = {
                                        "choices": [{
                                            "delta": {"role": "assistant"},
                                            "finish_reason": None,
                                            "index": 0,
                                            "logprobs": None,
                                        }],
                                        "created": int(time.time()),
                                        "id": transformed["body"]["chat_id"],
                                        "model": request.model,
                                        "object": "chat.completion.chunk",
                                        "system_fingerprint": "fp_zai_001",
                                    }
                                    yield f"data: {json_lib.dumps(role_chunk)}\n\n"
                                
                                content_chunk = {
                                    "choices": [{
                                        "delta": {"content": remaining_content},
                                        "finish_reason": None,
                                        "index": 0,
                                        "logprobs": None,
                                    }],
                                    "created": int(time.time()),
                                    "id": transformed["body"]["chat_id"],
                                    "model": request.model,
                                    "object": "chat.completion.chunk",
                                    "system_fingerprint": "fp_zai_001",
                                }
                                yield f"data: {json_lib.dumps(content_chunk)}\n\n"
                            
                            # ç„¶åå¤„ç†å·¥å…·è°ƒç”¨
                            if parsed_tools:
                                debug_log("[TOOLIFY] æµè‡ªç„¶ç»“æŸ - æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºå·¥å…·è°ƒç”¨ç»“æœ")
                                from .toolify_handler import format_toolify_response_for_stream
                                tool_chunks = format_toolify_response_for_stream(
                                    parsed_tools, 
                                    request.model, 
                                    transformed["body"]["chat_id"]
                                )
                                for chunk in tool_chunks:
                                    yield chunk
                                # æµå¼å“åº”å®Œæˆ
                                debug_log("[REQUEST] æµå¼å“åº”ï¼ˆå·¥å…·è°ƒç”¨ï¼‰å®Œæˆ")
                                return
                        
                        debug_log("[SSE] æµè‡ªç„¶ç»“æŸ - æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºfinish chunk")
                        # è¾“å‡ºæœ€åçš„finish chunkï¼ˆåŒ…å«usageä¿¡æ¯ï¼‰
                        finish_chunk = {
                            "choices": [{
                                "delta": {},
                                "finish_reason": "stop",
                                "index": 0,
                                "logprobs": None,
                            }],
                            "usage": usage_info if 'usage_info' in locals() else {
                                "prompt_tokens": 0,
                                "completion_tokens": 0,
                                "total_tokens": 0
                            },
                            "created": int(time.time()),
                            "id": transformed["body"]["chat_id"],
                            "model": request.model,
                            "object": "chat.completion.chunk",
                            "system_fingerprint": "fp_zai_001",
                        }
                        yield f"data: {json_lib.dumps(finish_chunk)}\n\n"
                        yield "data: [DONE]\n\n"
                        # æµå¼å“åº”æ­£å¸¸å®Œæˆ
                        debug_log("[REQUEST] æµå¼å“åº”å®Œæˆ")
                        return

                except Exception as e:
                    debug_log("æµå¤„ç†é”™è¯¯", error=str(e))
                    retry_count += 1
                    last_error = str(e)
                    
                    # ç½‘ç»œè¿æ¥é”™è¯¯æ—¶å°è¯•åˆ‡æ¢ä»£ç†ï¼ˆä»…é™ failover ç­–ç•¥ï¼‰
                    if _proxy_list and "connect" in str(e).lower():
                        await switch_proxy_on_failure()

                    if retry_count > settings.MAX_RETRIES:
                        error_response = {
                            "error": {
                                "message": f"Stream processing failed: {last_error}",
                                "type": "stream_error"
                            }
                        }
                        yield f"data: {json_lib.dumps(error_response)}\n\n"
                        yield "data: [DONE]\n\n"
                        # æµå¼å“åº”é”™è¯¯å®Œæˆ
                        debug_log("[REQUEST] æµå¼å“åº”é”™è¯¯")
                        return

        return StreamingResponse(
            stream_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )

    except HTTPException:
        debug_log("[REQUEST] å¤„ç†HTTPException")
        raise
    except Exception as e:
        debug_log("å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯", error=str(e))
        debug_log("[REQUEST] å¤„ç†å¼‚å¸¸")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


def create_openai_response(chat_id: str, model: str, content: str, reasoning_content: str = "", usage: dict = None) -> dict:
    """åˆ›å»ºOpenAIæ ¼å¼çš„å“åº”å¯¹è±¡"""
    response = {
        "id": chat_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": "stop"
            }
        ],
        "usage": usage or {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }
    
    # å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œæ·»åŠ åˆ°messageä¸­
    if reasoning_content:
        response["choices"][0]["message"]["reasoning_content"] = reasoning_content
    
    return response


async def handle_non_stream_request(request: OpenAIRequest, transformed: dict, enable_toolify: bool = False, request_client: httpx.AsyncClient = None) -> dict:
    """
    å¤„ç†éæµå¼è¯·æ±‚
    
    Args:
        request: OpenAIè¯·æ±‚å¯¹è±¡
        transformed: è½¬æ¢åçš„è¯·æ±‚
        enable_toolify: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
        request_client: è¯·æ±‚ä¸“ç”¨çš„HTTPå®¢æˆ·ç«¯
    
    è¯´æ˜ï¼šä¸Šæ¸¸å§‹ç»ˆä»¥ SSE å½¢å¼è¿”å›ï¼ˆtransform_request_in å›ºå®š stream=Trueï¼‰ï¼Œ
    å› æ­¤è¿™é‡Œéœ€è¦èšåˆ aiter_lines() çš„ data: å—ï¼Œæå– usageã€æ€è€ƒå†…å®¹ä¸ç­”æ¡ˆå†…å®¹ï¼Œ
    å¹¶æœ€ç»ˆäº§å‡ºä¸€æ¬¡æ€§ OpenAI æ ¼å¼å“åº”ã€‚
    """
    final_content = ""
    reasoning_content = ""
    usage_info = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
    }
    
    retry_count = 0
    last_error = None
    last_status_code = None
    
    while retry_count <= settings.MAX_RETRIES:
        try:
            # æ™ºèƒ½é€€é¿é‡è¯•ç­–ç•¥
            if retry_count > 0:
                delay = calculate_backoff_delay(
                    retry_count=retry_count,
                    status_code=last_status_code
                )
                debug_log(
                    f"[RETRY] éæµå¼è¯·æ±‚é‡è¯• ({retry_count}/{settings.MAX_RETRIES})",
                    retry_count=retry_count,
                    delay=f"{delay:.2f}s"
                )
                await asyncio.sleep(delay)
            
            # ä½¿ç”¨æä¾›çš„å®¢æˆ·ç«¯ï¼Œæˆ–è·å–å¤ç”¨çš„å®¢æˆ·ç«¯
            if request_client:
                client = request_client
            else:
                client = await get_request_client()
            
            # å‘èµ·æµå¼è¯·æ±‚ï¼ˆä¸Šæ¸¸å§‹ç»ˆè¿”å›SSEæµï¼Œå¸¦æ€§èƒ½è¿½è¸ªï¼‰
            # ä½¿ç”¨è½¬æ¢åçš„headersï¼ˆåŒ…å«Accept-Encodingï¼‰
            headers = transformed["config"]["headers"].copy()
            
            request_start_time = time.perf_counter()
            async with client.stream(
                "POST",
                transformed["config"]["url"],
                json=transformed["body"],
                headers=headers,
            ) as response:
                # è®°å½•éæµå¼è¯·æ±‚çš„TTFB
                ttfb = (time.perf_counter() - request_start_time) * 1000
                debug_log(f"â±ï¸ éæµå¼ä¸Šæ¸¸TTFB", ttfb_ms=f"{ttfb:.2f}ms")
                # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_msg = error_text.decode('utf-8', errors='ignore')
                    debug_log(
                        "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                        status_code=response.status_code,
                        error_detail=error_msg[:200]
                    )
                    
                    # å¯é‡è¯•çš„é”™è¯¯
                    retryable_codes = [400, 401, 405, 429, 502, 503, 504]
                    if response.status_code in retryable_codes and retry_count < settings.MAX_RETRIES:
                        last_status_code = response.status_code
                        last_error = f"{response.status_code}: {error_msg}"
                        retry_count += 1
                        
                        # æ£€æŸ¥å½“å‰æ˜¯å¦ä½¿ç”¨åŒ¿åToken
                        from .token_pool import get_token_pool
                        token_pool = get_token_pool()
                        current_token = transformed.get("token", "")
                        is_anonymous = token_pool.is_anonymous_token(current_token)
                        
                        if is_anonymous:
                            # åŒ¿åTokenï¼šä»»ä½•é”™è¯¯éƒ½æ¸…ç†ç¼“å­˜å¹¶é‡æ–°è·å–
                            debug_log(f"[ANONYMOUS-NONSTREAM] æ£€æµ‹åˆ°åŒ¿åTokené”™è¯¯ {response.status_code}ï¼Œæ¸…ç†ç¼“å­˜å¹¶é‡æ–°è·å–")
                            await transformer.clear_anonymous_token_cache()
                            
                            # æ¸…ç†å®¢æˆ·ç«¯ç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°åˆ›å»º
                            debug_log("[CLIENT-NONSTREAM] æ¸…ç†HTTPå®¢æˆ·ç«¯ç¼“å­˜ï¼Œä¸‹æ¬¡è¯·æ±‚å°†åˆ›å»ºæ–°å®¢æˆ·ç«¯")
                            await cleanup_clients()
                            
                            # é‡æ–°è·å–å®¢æˆ·ç«¯
                            if request_client:
                                request_client = await get_request_client()
                                client = request_client
                            else:
                                client = await get_request_client()
                            
                            # ä½¿ç”¨æ–°çš„åŒ¿åTokené‡æ–°ç”Ÿæˆè¯·æ±‚
                            request_dict = request.model_dump()
                            transformed = await transformer.transform_request_in(request_dict, client=client)
                            debug_log("[OK-NONSTREAM] å·²è·å–æ–°çš„åŒ¿åTokenå¹¶é‡æ–°ç”Ÿæˆè¯·æ±‚")
                        else:
                            # é…ç½®Tokenï¼šæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
                            if response.status_code in [400, 401, 405]:
                                debug_log(f"[CONFIG-NONSTREAM] é…ç½®Tokené”™è¯¯ {response.status_code}ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªToken")
                                new_token = await transformer.switch_token()
                                transformer.refresh_header_template()
                                request_dict = request.model_dump()
                                transformed = await transformer.transform_request_in(request_dict, client=client)
                                debug_log(f"[OK-NONSTREAM] å·²åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé…ç½®Token")
                            
                            # ç½‘ç»œé”™è¯¯æ—¶å°è¯•åˆ‡æ¢ä»£ç†ï¼ˆä»…é™ failover ç­–ç•¥ï¼‰
                            if response.status_code in [502, 503, 504] and _proxy_list:
                                await switch_proxy_on_failure()
                        
                        continue
                    
                    # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f"Upstream error: {error_msg[:500]}"
                    )
                
                # 200 æˆåŠŸï¼ŒèšåˆSSEæµæ•°æ®
                debug_log("Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹èšåˆéæµå¼æ•°æ®", status="success")
                
                # é‡ç½®èšåˆå˜é‡
                final_content = ""
                reasoning_content = ""
                
                async for line in response.aiter_lines():
                    if not line:
                        continue
                    
                    line = line.strip()
                    
                    # ä»…å¤„ç†ä»¥ data: å¼€å¤´çš„ SSE è¡Œ
                    if not line.startswith("data:"):
                        # å°è¯•è§£æä¸ºé”™è¯¯ JSON
                        try:
                            maybe_err = json_lib.loads(line)
                            if isinstance(maybe_err, dict) and (
                                "error" in maybe_err or "code" in maybe_err or "message" in maybe_err
                            ):
                                msg = (
                                    (maybe_err.get("error") or {}).get("message")
                                    if isinstance(maybe_err.get("error"), dict)
                                    else maybe_err.get("message")
                                ) or "ä¸Šæ¸¸è¿”å›é”™è¯¯"
                                raise HTTPException(status_code=500, detail=msg)
                        except (json.JSONDecodeError, HTTPException):
                            pass
                        continue
                    
                    data_str = line[5:].strip()
                    if not data_str or data_str in ("[DONE]", "DONE", "done"):
                        continue
                    
                    # è§£æ SSE æ•°æ®å—
                    try:
                        chunk = json_lib.loads(data_str)
                    except json.JSONDecodeError:
                        continue
                    
                    if chunk.get("type") != "chat:completion":
                        continue
                    
                    data = chunk.get("data", {})
                    phase = data.get("phase")
                    delta_content = data.get("delta_content", "")
                    edit_content = data.get("edit_content", "")
                    
                    # è®°å½•ç”¨é‡ï¼ˆé€šå¸¸åœ¨æœ€åå—ä¸­å‡ºç°ï¼‰
                    if data.get("usage"):
                        try:
                            usage_info = data["usage"]
                        except Exception:
                            pass
                    
                    # å¤„ç†tool_callé˜¶æ®µï¼ˆæå–æœç´¢ä¿¡æ¯ï¼‰
                    if phase == "tool_call":
                        edit_content = data.get("edit_content", "")
                        
                        # æå–æœç´¢æŸ¥è¯¢ä¿¡æ¯å¹¶æ·»åŠ åˆ°æœ€ç»ˆå†…å®¹
                        if edit_content and "<glm_block" in edit_content and "search" in edit_content:
                            try:
                                import re
                                # å…ˆå°è¯•ç›´æ¥è§£ç Unicode
                                decoded = edit_content
                                try:
                                    decoded = edit_content.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')
                                except:
                                    try:
                                        import codecs
                                        decoded = codecs.decode(edit_content, 'unicode_escape')
                                    except:
                                        pass
                                
                                # æå–queriesæ•°ç»„
                                queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                                if queries_match:
                                    queries_str = queries_match.group(1)
                                    queries = re.findall(r'"([^"]+)"', queries_str)
                                    if queries:
                                        search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])
                                        final_content += f"\n\n{search_info}\n\n"
                                        debug_log(f"[éæµå¼] æå–åˆ°æœç´¢ä¿¡æ¯: {queries}")
                            except Exception as e:
                                debug_log(f"[éæµå¼] æå–æœç´¢ä¿¡æ¯å¤±è´¥: {e}")
                        
                        continue
                    
                    # æ€è€ƒé˜¶æ®µèšåˆï¼ˆå»é™¤ <details><summary>... åŒ…è£¹å¤´ï¼‰
                    if phase == "thinking":
                        if delta_content:
                            if delta_content.startswith("<details"):
                                cleaned = (
                                    delta_content.split("</summary>\n>")[-1].strip()
                                    if "</summary>\n>" in delta_content
                                    else delta_content
                                )
                            else:
                                cleaned = delta_content
                            reasoning_content += cleaned
                    
                    # ç­”æ¡ˆé˜¶æ®µèšåˆ
                    elif phase == "answer":
                        # å½“ edit_content åŒæ—¶åŒ…å«æ€è€ƒç»“æŸæ ‡è®°ä¸ç­”æ¡ˆæ—¶ï¼Œæå–ç­”æ¡ˆéƒ¨åˆ†
                        if edit_content and "</details>\n" in edit_content:
                            content_after = edit_content.split("</details>\n")[-1]
                            if content_after:
                                final_content += content_after
                        elif delta_content:
                            final_content += delta_content
                
                # æ¸…ç†å¹¶è¿”å›
                final_content = (final_content or "").strip()
                reasoning_content = (reasoning_content or "").strip()
                
                # è‹¥æ²¡æœ‰èšåˆåˆ°ç­”æ¡ˆï¼Œä½†æœ‰æ€è€ƒå†…å®¹ï¼Œåˆ™ä¿åº•è¿”å›æ€è€ƒå†…å®¹
                if not final_content and reasoning_content:
                    final_content = reasoning_content
                
                # å·¥å…·è°ƒç”¨æ£€æµ‹ï¼ˆéæµå¼æ¨¡å¼ï¼‰
                if enable_toolify and final_content:
                    debug_log("[TOOLIFY] æ£€æŸ¥éæµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨")
                    tool_response = parse_toolify_response(final_content, request.model)
                    if tool_response:
                        debug_log("[TOOLIFY] éæµå¼å“åº”ä¸­æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                        # è¿”å›åŒ…å«tool_callsçš„å“åº”
                        return {
                            "id": transformed["body"]["chat_id"],
                            "object": "chat.completion",
                            "created": int(time.time()),
                            "model": request.model,
                            "choices": [
                                {
                                    "index": 0,
                                    "message": tool_response,
                                    "finish_reason": "tool_calls"
                                }
                            ],
                            "usage": usage_info
                        }
                
                debug_log(
                    "éæµå¼å“åº”èšåˆå®Œæˆ",
                    content_length=len(final_content),
                    reasoning_length=len(reasoning_content),
                    usage=usage_info
                )
                
                # è¿”å›æ ‡å‡†OpenAIæ ¼å¼å“åº”
                return create_openai_response(
                    transformed["body"]["chat_id"],
                    request.model,
                    final_content,
                    reasoning_content,
                    usage_info
                )
        
        except HTTPException:
            raise
        except Exception as e:
            debug_log("éæµå¼å¤„ç†é”™è¯¯", error=str(e))
            retry_count += 1
            last_error = str(e)
            
            # ç½‘ç»œè¿æ¥é”™è¯¯æ—¶å°è¯•åˆ‡æ¢ä»£ç†ï¼ˆä»…é™ failover ç­–ç•¥ï¼‰
            if _proxy_list and "connect" in str(e).lower():
                await switch_proxy_on_failure()
            
            if retry_count > settings.MAX_RETRIES:
                raise HTTPException(
                    status_code=500,
                    detail=f"Non-stream processing failed after {settings.MAX_RETRIES} retries: {last_error}"
                )
    
    # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
    raise HTTPException(status_code=500, detail="Unexpected error in non-stream processing")
