#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å“åº”è§£æå™¨æ¨¡å— - å°è£…æ‰€æœ‰å“åº”å†…å®¹è§£æé€»è¾‘

ä» openai_service.py æ‹†åˆ†å‡ºæ¥ï¼Œæä¾›ç»Ÿä¸€çš„å“åº”å†…å®¹å¤„ç†æ¥å£

æ€§èƒ½ä¼˜åŒ–ï¼šæ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼åœ¨ç±»åˆå§‹åŒ–æ—¶é¢„ç¼–è¯‘
"""

import re
from typing import Tuple, List

from ..helpers import debug_log


class ResponseParser:
    """å“åº”è§£æå™¨ç±»ï¼Œå°è£…æ‰€æœ‰å“åº”å†…å®¹è§£æé€»è¾‘"""

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨ï¼Œé¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼"""
        # ===== clean_thinking ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ =====
        # å±æ€§æ®‹ç‰‡æ£€æµ‹
        self._re_attr_residue = re.compile(r'(duration=|last_tool_call_name|view=)')
        self._re_attr_end = re.compile(r'[">]$')
        # æ ‡ç­¾æ¸…ç†
        self._re_glm_block = re.compile(r'<glm_block[^>]*>.*?</glm_block>', re.DOTALL)
        self._re_url_tag = re.compile(r'<url>[^<]*</url>')
        self._re_details_open = re.compile(r'<details[^>]*>')
        self._re_details_close = re.compile(r'</details>')
        self._re_summary = re.compile(r'<summary[^>]*>.*?</summary>', re.DOTALL)
        # å¼•ç”¨æ ‡è®°æ¸…ç†
        self._re_quote_line_start = re.compile(r'^>\s*', re.MULTILINE)
        self._re_quote_newline = re.compile(r'\n>\s*')
        # å¤šä½™ç©ºè¡Œæ¸…ç†
        self._re_multi_newlines = re.compile(r'\n{3,}')
        
        # ===== extract_image_urls ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ =====
        # æ ¼å¼1: image_url ç±»å‹ï¼ˆè½¬ä¹‰æ ¼å¼ï¼‰
        self._re_image_url_escaped = re.compile(r'\\\"url\\\":\s*\\\"(https?://[^\\\"\\\\]+(?:\\\\.[^\\\"\\\\]+)*[^\\\"\\\\]*)\\\"')
        self._re_image_url_plain = re.compile(r'"url":\s*"(https?://[^"]+)"')
        # æ ¼å¼2: img_url ç±»å‹ï¼ˆè½¬ä¹‰æ ¼å¼ï¼‰
        self._re_img_url_escaped = re.compile(r'\\\"img_url\\\":\s*\\\"(https?://[^\\\"]+)\\\"')
        self._re_img_url_plain = re.compile(r'"img_url":\s*"(https?://[^"]+)"')
        
        # ===== extract_search_info ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ =====
        self._re_queries = re.compile(r'"queries":\s*\[(.*?)\]')
        self._re_query_items = re.compile(r'"([^"]+)"')

    def clean_thinking(self, delta_content: str) -> str:
        """æ¸…ç† thinking å†…å®¹ï¼Œæå–çº¯æ–‡æœ¬
        
        å¤„ç†æ ¼å¼ï¼š
        - ç§»é™¤ <details> å’Œ <summary> æ ‡ç­¾
        - ç§»é™¤ markdown å¼•ç”¨ç¬¦å· "> "
        - ä¿ç•™çº¯æ–‡æœ¬å†…å®¹
        """
        if not delta_content:
            return ""
        
        # 0. å…ˆä¸¢å¼ƒå¯èƒ½å‡ºç°åœ¨ <details> ä¹‹å‰çš„å±æ€§æ®‹ç‰‡
        first_newline = delta_content.find("\n")
        if first_newline != -1:
            first_line = delta_content[:first_newline].strip()
            if self._re_attr_residue.search(first_line) and self._re_attr_end.search(first_line):
                delta_content = delta_content[first_newline + 1:]

        # 1. ç§»é™¤ <glm_block>...</glm_block> å·¥å…·è°ƒç”¨å—
        delta_content = self._re_glm_block.sub('', delta_content)
        
        # 2. ç§»é™¤ <url>...</url> æ ‡ç­¾
        delta_content = self._re_url_tag.sub('', delta_content)

        # 3. ç§»é™¤ <details> å¼€å§‹æ ‡ç­¾
        delta_content = self._re_details_open.sub('', delta_content)
        
        # 4. ç§»é™¤ </details> ç»“æŸæ ‡ç­¾
        delta_content = self._re_details_close.sub('', delta_content)

        # 5. ç§»é™¤ <summary> æ ‡ç­¾åŠå…¶å†…å®¹
        delta_content = self._re_summary.sub('', delta_content)
        
        # 6. ç§»é™¤è¡Œé¦–çš„å¼•ç”¨æ ‡è®° "> "
        delta_content = self._re_quote_line_start.sub('', delta_content)
        delta_content = self._re_quote_newline.sub('\n', delta_content)
        
        # 7. ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        delta_content = self._re_multi_newlines.sub('\n\n', delta_content)
        
        # 8. å»é™¤é¦–å°¾ç©ºç™½
        return delta_content.strip()

    def split_edit_content(self, edit_content: str) -> Tuple[str, str]:
        """æ‹†åˆ† edit_contentï¼Œè¿”å› (thinking_part, answer_part)
        
        å¤„ç†æ ¼å¼ï¼š
        <details type="reasoning" done="false/true" ...>
        <summary>Thinking...</summary>
        > æ€è€ƒå†…å®¹
        </details>
        å›ç­”å†…å®¹
        """
        if not edit_content:
            return "", ""

        thinking_part = ""
        answer_part = ""

        if "</details>" in edit_content:
            parts = edit_content.split("</details>", 1)
            thinking_part = parts[0] + "</details>"
            answer_part = parts[1] if len(parts) > 1 else ""
        else:
            answer_part = edit_content

        # æ¸…ç† thinking å†…å®¹
        if thinking_part:
            thinking_part = self.clean_thinking(thinking_part)
        
        # æ¸…ç† answer å†…å®¹
        answer_part = answer_part.strip()
        if answer_part:
            answer_part = answer_part.lstrip('\n')
            answer_part = answer_part.replace("<think>", "").replace("</think>", "")
        
        return thinking_part, answer_part

    def diff_new_content(self, existing: str, incoming: str) -> str:
        """è®¡ç®— incoming ç›¸æ¯” existing çš„æ–°å¢éƒ¨åˆ†ï¼ˆç”¨äºæµå¼å¢é‡è¾“å‡ºï¼‰"""
        incoming = incoming or ""
        if not incoming:
            return ""

        existing = existing or ""
        if not existing:
            return incoming

        if incoming == existing:
            return ""

        # å¦‚æœ incoming æ˜¯ existing çš„æ‰©å±•ï¼Œè¿”å›æ–°å¢éƒ¨åˆ†
        if incoming.startswith(existing):
            return incoming[len(existing):]

        # å¯»æ‰¾æœ€é•¿å…¬å…±å‰ç¼€ä»¥è®¡ç®—å¢é‡
        max_overlap = min(len(existing), len(incoming))
        for overlap in range(max_overlap, 0, -1):
            if existing[-overlap:] == incoming[:overlap]:
                return incoming[overlap:]

        # å¦‚æœ existing å®Œå…¨åŒ…å«åœ¨ incoming ä¸­
        if existing in incoming:
            return incoming.replace(existing, "", 1)

        # æ— æ³•ç¡®å®šå¢é‡ï¼Œè¿”å›å®Œæ•´å†…å®¹
        return incoming

    def extract_image_urls(self, content: str) -> List[str]:
        """ä»ä¸Šæ¸¸å“åº”å†…å®¹ä¸­æå–å›¾ç‰‡URL
        
        å¤„ç†æ ¼å¼ç¤ºä¾‹ï¼š
        1. {"image_url":{"url":"https://qc4n.bigmodel.cn/xxx.png?..."}}
        2. {"img_url": "https://bigmodel-us3-prod-agent.cn-wlcb.ufileos.com/xxx.jpg", ...}
        
        Returns:
            list: æå–åˆ°çš„å›¾ç‰‡URLåˆ—è¡¨
        """
        if not content:
            return []
        
        image_urls = []
        
        # === æ ¼å¼1: image_url ç±»å‹ï¼ˆbigmodel.cn åŸŸåï¼‰===
        if '\\"type\\":\\"image_url\\"' in content or '"type":"image_url"' in content:
            matches = self._re_image_url_escaped.findall(content)
            for url in matches:
                clean_url = url.replace('\\/', '/').replace('\\"', '"')
                if clean_url and 'bigmodel.cn' in clean_url:
                    image_urls.append(clean_url)
            
            if not image_urls:
                matches = self._re_image_url_plain.findall(content)
                for url in matches:
                    if url and 'bigmodel.cn' in url:
                        image_urls.append(url)
        
        # === æ ¼å¼2: img_url ç±»å‹ï¼ˆufileos.com åŸŸåï¼‰===
        if 'img_url' in content or 'image_reference' in content:
            matches = self._re_img_url_escaped.findall(content)
            for url in matches:
                clean_url = url.replace('\\/', '/')
                if clean_url and ('ufileos.com' in clean_url or 'bigmodel' in clean_url):
                    image_urls.append(clean_url)
            
            if not image_urls:
                matches = self._re_img_url_plain.findall(content)
                for url in matches:
                    if url and ('ufileos.com' in url or 'bigmodel' in url):
                        image_urls.append(url)
        
        return image_urls

    def format_images_as_markdown(self, image_urls: List[str]) -> str:
        """å°†å›¾ç‰‡URLåˆ—è¡¨æ ¼å¼åŒ–ä¸ºmarkdownå›¾ç‰‡æ ¼å¼
        
        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            
        Returns:
            str: markdownæ ¼å¼çš„å›¾ç‰‡å­—ç¬¦ä¸²
        """
        if not image_urls:
            return ""
        
        markdown_images = []
        for i, url in enumerate(image_urls, 1):
            markdown_images.append(f"![å›¾ç‰‡{i}]({url})")
        
        return "\n\n".join(markdown_images)

    def extract_search_info(self, reasoning_content: str, edit_content: str) -> str:
        """ä» edit_content ä¸­æå–æœç´¢ä¿¡æ¯"""
        if edit_content and "<glm_block" in edit_content and "search" in edit_content:
            try:
                decoded = edit_content
                try:
                    decoded = edit_content.encode("utf-8").decode("unicode_escape").encode("latin1").decode("utf-8")
                except Exception:
                    try:
                        import codecs
                        decoded = codecs.decode(edit_content, "unicode_escape")
                    except Exception:
                        pass

                queries_match = self._re_queries.search(decoded)
                if queries_match:
                    queries_str = queries_match.group(1)
                    queries = self._re_query_items.findall(queries_str)
                    if queries:
                        search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])
                        reasoning_content += f"\n\n{search_info}\n\n"
                        debug_log("[æœç´¢ä¿¡æ¯] æå–åˆ°æœç´¢æŸ¥è¯¢", queries=queries)
            except Exception as exc:
                debug_log("[æœç´¢ä¿¡æ¯] æå–å¤±è´¥", error=str(exc))
        return reasoning_content


# å…¨å±€å•ä¾‹å®ä¾‹
response_parser = ResponseParser()
