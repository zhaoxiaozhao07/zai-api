"""Service layer orchestrating OpenAI-compatible chat completions."""

from __future__ import annotations

import asyncio
import json
import time
from typing import AsyncIterator, Dict, Optional, Tuple

import httpx
from fastapi import HTTPException

from ..helpers import (
    info_log,
    debug_log,
    error_log,
    bind_request_context,
    reset_request_context,
    request_stage_log,
)
from ..schemas import OpenAIRequest
from ..config import settings
from ..zai_transformer import ZAITransformer
from ..token_pool import get_token_pool
from .network_manager import network_manager


class ChatCompletionService:
    """Encapsulate chat completion workflow independent of FastAPI layer."""

    def __init__(self) -> None:
        self.transformer = ZAITransformer()

    async def prepare_request(self, request: OpenAIRequest) -> Tuple[dict, dict]:
        request_dict = request.model_dump()
        return self._prepare_messages(request, request_dict)

    def _prepare_messages(self, request: OpenAIRequest, request_dict: dict) -> Tuple[dict, dict]:
        messages = [
            msg.model_dump() if hasattr(msg, "model_dump") else msg
            for msg in request.messages
        ]
        
        # ç§»é™¤ tools å’Œ tool_choice å‚æ•°ï¼ˆä¸Šæ¸¸ä¸æ”¯æŒï¼‰
        transformed_dict = request_dict.copy()
        if "tools" in transformed_dict:
            del transformed_dict["tools"]
            info_log("[è¯·æ±‚å¤„ç†] å·²ç§»é™¤ tools å‚æ•°ï¼ˆä¸Šæ¸¸ä¸æ”¯æŒï¼‰")
        if "tool_choice" in transformed_dict:
            del transformed_dict["tool_choice"]
            info_log("[è¯·æ±‚å¤„ç†] å·²ç§»é™¤ tool_choice å‚æ•°ï¼ˆä¸Šæ¸¸ä¸æ”¯æŒï¼‰")
        
        transformed_dict["messages"] = messages
        return request_dict, transformed_dict

    async def build_transformed(self, request_dict: dict, client: httpx.AsyncClient, upstream: str) -> dict:
        request_stage_log("transform_in", "å¼€å§‹è½¬æ¢è¯·æ±‚æ ¼å¼: OpenAI -> Z.AI", upstream=upstream)
        return await self.transformer.transform_request_in(
            request_dict,
            client=client,
            upstream_url=upstream,
        )

    async def get_request_context(self) -> Tuple[httpx.AsyncClient, Optional[str], str]:
        client, proxy = await network_manager.get_request_client()
        upstream = await network_manager.get_next_upstream()
        bind_request_context(proxy=proxy, upstream=upstream)
        debug_log("[REQUEST] è·å–è¯·æ±‚ä¸Šä¸‹æ–‡", proxy=proxy or "ç›´è¿", upstream=upstream)
        return client, proxy, upstream

    async def ensure_authorization(self, authorization: str) -> None:
        if settings.SKIP_AUTH_TOKEN:
            return

        if not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

        api_key = authorization[7:]
        if api_key != settings.AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Invalid API key")

    async def handle_non_stream_request(
        self,
        request: OpenAIRequest,
        transformed: dict,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
        request_dict_for_transform: dict,
        json_lib,
    ) -> dict:
        bind_request_context(mode="non_stream")
        request_stage_log("non_stream_pipeline", "è¿›å…¥éæµå¼å¤„ç†æµç¨‹")
        final_content = ""
        reasoning_content = ""
        latest_full_thinking = ""
        usage_info = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }

        retry_count = 0
        last_error = None
        last_status_code = None

        while retry_count <= settings.MAX_RETRIES:
            try:
                if retry_count > 0:
                    delay = self.calculate_backoff_delay(retry_count, last_status_code)
                    info_log(
                        "[RETRY] éæµå¼è¯·æ±‚é‡è¯•",
                        retry_count=retry_count,
                        delay=f"{delay:.2f}s",
                    )
                    await asyncio.sleep(delay)

                client = request_client
                headers = transformed["config"]["headers"].copy()

                attempt = retry_count + 1
                request_stage_log(
                    "upstream_request",
                    "å‘ä¸Šæ¸¸å‘èµ·éæµå¼è¯·æ±‚",
                    attempt=attempt,
                    upstream=current_upstream,
                    proxy=current_proxy or "direct",
                )
                request_start_time = time.perf_counter()
                async with client.stream(
                    "POST",
                    transformed["config"]["url"],
                    json=transformed["body"],
                    headers=headers,
                ) as response:
                    ttfb = (time.perf_counter() - request_start_time) * 1000
                    debug_log("â±ï¸ éæµå¼ä¸Šæ¸¸TTFB", ttfb_ms=f"{ttfb:.2f}ms")

                    if response.status_code != 200:
                        last_status_code = response.status_code
                        error_text = await response.aread()
                        error_msg = error_text.decode("utf-8", errors="ignore")
                        error_log(
                            "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                            status_code=response.status_code,
                            error_detail=error_msg[:200],
                        )

                        should_retry, transformed, request_client, current_proxy, current_upstream = await self._handle_retryable_error(
                            response.status_code,
                            retry_count,
                            transformed,
                            request_dict_for_transform,
                            request_client,
                            current_proxy,
                            current_upstream,
                        )
                        if should_retry:
                            retry_count += 1
                            continue

                        raise HTTPException(
                            status_code=response.status_code,
                            detail=f"Upstream error: {error_msg[:500]}",
                        )

                    request_stage_log(
                        "upstream_response",
                        "Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹èšåˆéæµå¼æ•°æ®",
                        status="success",
                        attempt=attempt,
                    )

                    # å†…å®¹ç´¯ç§¯å˜é‡
                    # ä¸Šæ¸¸ä¼šå…ˆä»¥å¤šæ¡ `phase=thinking` çš„ data æ¨ç†ï¼Œå†åœ¨ `phase=answer` / `phase=other` ä¸­ç»™å‡º
                    # å®Œæ•´çš„ edit_contentï¼ˆå« <details> æ¨ç† + æœ€ç»ˆå›ç­”ï¼‰ä»¥åŠå¢é‡çš„ delta_content
                    # ä¸ºäº†æ„é€ éæµå¼ä¸€æ¬¡æ€§è¿”å›ï¼Œæˆ‘ä»¬ï¼š
                    # 1ï¼‰æŒç»­ç´¯ç§¯ thinking/answer çš„ delta_contentï¼Œç”¨äºå…œåº•
                    # 2ï¼‰ä¼˜å…ˆä»å¸¦ </details> çš„ edit_content ä¸­è§£æå‡ºå®Œæ•´ reasoning_content + answer
                    thinking_content = ""  # ç´¯ç§¯ thinking é˜¶æ®µçš„ delta_contentï¼ˆå…œåº•ï¼‰
                    answer_content = ""    # ç´¯ç§¯ answer é˜¶æ®µçš„ delta_contentï¼ˆå…œåº•ï¼‰
                    latest_full_edit = ""  # è®°å½•æœ€åä¸€ä¸ªåŒ…å« </details> çš„ edit_contentï¼ˆå®Œæ•´æ€è€ƒ+æ­£æ–‡ï¼‰
                    latest_usage = None

                    async for line in response.aiter_lines():
                        if not line:
                            continue

                        line = line.strip()
                        if not line.startswith("data:"):
                            try:
                                maybe_err = json_lib.loads(line)
                                if isinstance(maybe_err, dict) and (
                                    "error" in maybe_err or "code" in maybe_err or "message" in maybe_err
                                ):
                                    msg = (
                                        (maybe_err.get("error") or {}).get("message")
                                        if isinstance(maybe_err.get("error"), dict)
                                        else maybe_err.get("message")
                                    ) or "ä¸Šæ¸¸è¿”å›é”™è¯¯"
                                    raise HTTPException(status_code=500, detail=msg)
                            except (json.JSONDecodeError, HTTPException):
                                pass
                            continue

                        data_str = line[5:].strip()
                        if not data_str or data_str.lower() == "[done]":
                            continue

                        try:
                            chunk = json_lib.loads(data_str)
                        except json.JSONDecodeError:
                            continue

                        if chunk.get("type") != "chat:completion":
                            continue

                        data = chunk.get("data", {})
                        phase = data.get("phase")
                        delta_content = data.get("delta_content", "")
                        edit_content = data.get("edit_content", "")

                        # æ”¶é›† usage ä¿¡æ¯
                        if data.get("usage"):
                            latest_usage = data["usage"]

                        # tool_call é˜¶æ®µï¼šå¦‚æœå¸¦æœ‰ edit_indexï¼Œä¼˜å…ˆæ£€æµ‹å›¾ç‰‡ï¼Œå¦åˆ™æ”¾å…¥ thinking
                        if phase == "tool_call":
                            if data.get("edit_index") is not None and edit_content:
                                # å…ˆæ£€æµ‹æ˜¯å¦æœ‰å›¾ç‰‡
                                image_urls = self._extract_image_urls(edit_content)
                                if image_urls:
                                    # æœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼æ”¾å…¥ answer_content
                                    markdown_images = self._format_images_as_markdown(image_urls)
                                    if markdown_images:
                                        answer_content += "\n\n" + markdown_images + "\n\n"
                                else:
                                    # æ— å›¾ç‰‡ï¼Œæ”¾å…¥ thinking_content
                                    thinking_content += edit_content
                            continue

                        # thinking é˜¶æ®µï¼šç´¯ç§¯ delta_contentï¼ˆåªç”¨äºå…œåº•ï¼Œä¼˜å…ˆç”¨ answer/other é˜¶æ®µçš„å®Œæ•´ edit_contentï¼‰
                        if phase == "thinking":
                            if delta_content:
                                thinking_content += delta_content
                            continue

                        # answer é˜¶æ®µï¼šå¤„ç† edit_contentï¼ˆåŒ…å«å®Œæ•´ thinking+æ­£æ–‡ï¼‰å’Œ delta_content
                        if phase == "answer":
                            # å¦‚æœæœ‰ edit_content ä¸”åŒ…å«å®Œæ•´çš„ </details>ï¼Œä¼˜å…ˆç”¨å®ƒæ¥è§£æå®Œæ•´æ¨ç†+å›ç­”
                            if edit_content and "</details>" in edit_content:
                                latest_full_edit = edit_content
                            
                            # ç´¯ç§¯ answer çš„ delta_content
                            if delta_content:
                                answer_content += delta_content
                            continue

                        # other é˜¶æ®µï¼šå¯èƒ½æœ‰ usage ä¿¡æ¯ï¼Œä¹Ÿå¯èƒ½æœ‰æœ€åçš„ edit_content ç‰‡æ®µ
                        # å¦‚æœå¸¦æœ‰ edit_indexï¼Œè¯´æ˜æ˜¯å·¥å…·è°ƒç”¨ç›¸å…³å†…å®¹ï¼Œåº”æ”¾å…¥ thinking
                        if phase == "other":
                            # æ”¶é›† usage
                            if data.get("usage"):
                                latest_usage = data["usage"]

                            # åˆ¤æ–­æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨ç›¸å…³å†…å®¹ï¼ˆå¸¦ edit_indexï¼‰
                            has_edit_index = data.get("edit_index") is not None

                            # è·å–å†…å®¹
                            tail_text = None
                            if edit_content:
                                tail_text = edit_content
                            elif delta_content:
                                tail_text = delta_content

                            if tail_text:
                                if has_edit_index:
                                    # å¸¦ edit_index çš„å†…å®¹ï¼šå…ˆæ£€æµ‹æ˜¯å¦æœ‰å›¾ç‰‡
                                    image_urls = self._extract_image_urls(tail_text)
                                    if image_urls:
                                        # æœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼æ”¾å…¥ answer_content
                                        markdown_images = self._format_images_as_markdown(image_urls)
                                        if markdown_images:
                                            answer_content += "\n\n" + markdown_images + "\n\n"
                                    else:
                                        # æ— å›¾ç‰‡ï¼Œæ”¾å…¥ thinking_content
                                        thinking_content += tail_text
                                else:
                                    # æ™®é€šå†…å®¹æ”¾å…¥ answer_content
                                    answer_content += tail_text
                            continue

                    # å¦‚æœä¸Šæ¸¸åœ¨ answer/other é˜¶æ®µç»™å‡ºäº†å®Œæ•´çš„ edit_contentï¼ˆå« </details>ï¼‰ï¼Œ
                    # åˆ™ä¼˜å…ˆç”¨å®ƒæ¥æ‹†åˆ†å‡º reasoning_content å’Œ æœ€ç»ˆå›ç­”ï¼Œé¿å…ä»…ä¾èµ–å¢é‡å¯¼è‡´å†…å®¹ä¸å®Œæ•´æˆ–æˆªæ–­
                    # æ³¨æ„ï¼šlatest_full_edit ä¸­å¯èƒ½ä¸åŒ…å« <details> å¼€å¤´ï¼ˆä¾‹å¦‚åªå‰©å±æ€§æ®‹ç‰‡ï¼‰ï¼Œ
                    # è¿™é‡Œç»Ÿä¸€äº¤ç»™ _split_edit_content + _clean_thinking åšæ¸…æ´—ï¼Œç§»é™¤ true" duration="1" ç­‰æ®‹ç•™ã€‚
                    if latest_full_edit:
                        thinking_part, answer_part = self._split_edit_content(latest_full_edit)
                        if thinking_part:
                            thinking_content = thinking_part
                        if answer_part:
                            answer_content = answer_part

                    # æ¸…ç†å†…å®¹
                    thinking_content = thinking_content.strip()
                    answer_content = answer_content.strip()

                    # ä½¿ç”¨ usage ä¿¡æ¯
                    usage_info = latest_usage or {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                    }

                    request_stage_log(
                        "non_stream_completed",
                        "éæµå¼å“åº”å®Œæˆ",
                        has_thinking=bool(thinking_content),
                        completion_tokens=usage_info.get("completion_tokens"),
                        prompt_tokens=usage_info.get("prompt_tokens"),
                    )

                    # æ„å»ºæ¶ˆæ¯å¯¹è±¡
                    message = {
                        "role": "assistant",
                        "content": answer_content,
                    }
                    # å¦‚æœæœ‰ thinking å†…å®¹ï¼Œæ·»åŠ  reasoning_content å­—æ®µ
                    if thinking_content:
                        message["reasoning_content"] = thinking_content

                    return {
                        "id": transformed["body"]["chat_id"],
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": request.model,
                        "choices": [
                            {
                                "index": 0,
                                "message": message,
                                "finish_reason": "stop",
                            }
                        ],
                        "usage": usage_info,
                    }

            except HTTPException:
                raise
            except Exception as exc:
                error_log("éæµå¼å¤„ç†å‘ç”Ÿå¼‚å¸¸", error=str(exc))
                last_error = str(exc)
                retry_count += 1

        reset_request_context("mode")
        raise HTTPException(status_code=500, detail=f"éæµå¼è¯·æ±‚å¤±è´¥: {last_error}")

    def calculate_backoff_delay(
        self,
        retry_count: int,
        status_code: Optional[int] = None,
        base_delay: float = 1.5,
        max_delay: float = 8.0,
    ) -> float:
        linear_delay = base_delay * retry_count

        if status_code == 429:
            linear_delay *= 1.5
        elif status_code in [502, 503, 504]:
            linear_delay *= 1.2

        linear_delay = min(linear_delay, max_delay)
        jitter = linear_delay * 0.2
        return max(linear_delay + (2 * jitter * (time.time() % 1) - jitter), 0.5)

    async def stream_response(
        self,
        request: OpenAIRequest,
        transformed: dict,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
        request_dict_for_transform: dict,
        json_lib,
    ) -> AsyncIterator[str]:
        bind_request_context(mode="stream")
        request_stage_log("stream_pipeline", "è¿›å…¥æµå¼å¤„ç†æµç¨‹")
        retry_count = 0
        last_error = None
        last_status_code = None

        while retry_count <= settings.MAX_RETRIES:
            try:
                if retry_count > 0:
                    delay = self.calculate_backoff_delay(retry_count, last_status_code)
                    info_log(
                        "[RETRY] æµå¼è¯·æ±‚é‡è¯•",
                        retry_count=retry_count,
                        delay=f"{delay:.2f}s",
                        last_status=last_status_code,
                    )
                    await asyncio.sleep(delay)

                client = request_client
                headers = transformed["config"]["headers"].copy()

                attempt = retry_count + 1
                request_stage_log(
                    "upstream_request",
                    "å‘ä¸Šæ¸¸å‘èµ·æµå¼è¯·æ±‚",
                    attempt=attempt,
                    upstream=current_upstream,
                    proxy=current_proxy or "direct",
                )
                request_start_time = time.perf_counter()
                async with client.stream(
                    "POST",
                    transformed["config"]["url"],
                    json=transformed["body"],
                    headers=headers,
                ) as response:
                    ttfb = (time.perf_counter() - request_start_time) * 1000
                    debug_log("â±ï¸ ä¸Šæ¸¸TTFB (é¦–å­—èŠ‚æ—¶é—´)", ttfb_ms=f"{ttfb:.2f}ms")

                    if response.status_code != 200:
                        error_text = await response.aread()
                        error_msg = error_text.decode("utf-8", errors="ignore")
                        error_log(
                            "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                            status_code=response.status_code,
                            error_detail=error_msg[:200],
                        )

                        should_retry, transformed, request_client, current_proxy, current_upstream = await self._handle_retryable_error(
                            response.status_code,
                            retry_count,
                            transformed,
                            request_dict_for_transform,
                            request_client,
                            current_proxy,
                            current_upstream,
                        )
                        if should_retry:
                            retry_count += 1
                            last_status_code = response.status_code
                            last_error = f"{response.status_code}: {error_msg}"
                            continue

                        error_response = {
                            "error": {
                                "message": f"Upstream error: {response.status_code}",
                                "type": "upstream_error",
                                "code": response.status_code,
                                "details": error_msg[:500],
                            }
                        }
                        yield f"data: {json_lib.dumps(error_response)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

                    request_stage_log(
                        "upstream_stream_ready",
                        "Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç† SSE æµ",
                        status="success",
                        attempt=attempt,
                    )

                    has_thinking = False
                    # ç´¯ç§¯å·²è¾“å‡ºçš„ reasoning/contentï¼Œç”¨äºåšå¢é‡ diffï¼Œé¿å…è¦†ç›–å¼ delta å¯¼è‡´æˆªæ–­
                    thinking_accumulator = ""
                    answer_accumulator = ""

                    async for line in response.aiter_lines():
                        if not line or not line.strip():
                            continue

                        if not line.startswith("data:"):
                            continue

                        chunk_str = line[5:].strip()
                        if not chunk_str or chunk_str == "[DONE]":
                            if chunk_str == "[DONE]":
                                yield "data: [DONE]\n\n"
                            continue

                        try:
                            chunk_data = json_lib.loads(chunk_str)
                        except json.JSONDecodeError:
                            continue

                        if chunk_data.get("type") != "chat:completion":
                            yield f"data: {chunk_str}\n\n"
                            continue

                        data = chunk_data.get("data", {})
                        delta_content = data.get("delta_content")
                        edit_content = data.get("edit_content")
                        phase = data.get("phase")
                        is_done = phase == "done" or data.get("done")
                        error_info = data.get("error")

                        # æ£€æµ‹ä¸Šæ¸¸è¿”å›çš„é”™è¯¯ï¼ˆå¦‚å†…å®¹å®‰å…¨è­¦å‘Šï¼‰
                        if error_info:
                            error_detail = error_info.get("detail") or error_info.get("content") or "Unknown error"
                            error_log(f"[UPSTREAM_ERROR] ä¸Šæ¸¸è¿”å›é”™è¯¯: {error_detail}")
                            
                            if not has_thinking:
                                has_thinking = True
                                yield self._build_role_chunk(json_lib, transformed, request)
                            
                            error_message = f"\n\n[ç³»ç»Ÿæç¤º: {error_detail}]"
                            yield self._build_content_chunk(json_lib, transformed, request, error_message)
                            
                            if is_done:
                                finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                                yield finish_chunk
                                yield "data: [DONE]\n\n"
                                await self._mark_token_success(transformed)
                                request_stage_log("stream_completed", "æµå¼å“åº”å®Œæˆï¼ˆå¸¦é”™è¯¯ï¼‰", has_error=True)
                                return
                            continue

                        # tool_call é˜¶æ®µï¼šå¦‚æœå¸¦æœ‰ edit_indexï¼Œä¼˜å…ˆæ£€æµ‹å›¾ç‰‡ï¼Œå¦åˆ™æ”¾å…¥æ€è€ƒ
                        if phase == "tool_call":
                            if data.get("edit_index") is not None and edit_content:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                # å…ˆæ£€æµ‹æ˜¯å¦æœ‰å›¾ç‰‡ï¼ˆåŒ…æ‹¬ image_reference å·¥å…·è¿”å›çš„å›¾ç‰‡ï¼‰
                                image_urls = self._extract_image_urls(edit_content)
                                if image_urls:
                                    # æœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼æ”¾å…¥æ­£æ–‡
                                    markdown_images = self._format_images_as_markdown(image_urls)
                                    if markdown_images:
                                        yield self._build_content_chunk(json_lib, transformed, request, "\n\n" + markdown_images + "\n\n")
                                        answer_accumulator += markdown_images
                                else:
                                    # æ— å›¾ç‰‡ï¼Œè®¡ç®—æ–°å¢çš„æ€è€ƒå†…å®¹
                                    new_thinking = self._diff_new_content(thinking_accumulator, edit_content)
                                    if new_thinking:
                                        cleaned = self._clean_thinking(new_thinking)
                                        if cleaned:
                                            yield self._build_reasoning_chunk(json_lib, transformed, request, cleaned)
                                            thinking_accumulator += new_thinking
                            continue

                        # thinking é˜¶æ®µï¼šæµå¼è¾“å‡º reasoning_contentï¼ˆä½¿ç”¨å¢é‡ diffï¼Œé˜²æ­¢è¦†ç›–å¼ delta æˆªæ–­ï¼‰
                        if phase == "thinking":
                            if delta_content:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                # å…ˆåšâ€œåŸæ ·å¢é‡â€ï¼šç›´æ¥æŠŠæœ¬æ¬¡ delta é‡Œçš„æ–°å¢éƒ¨åˆ†å…ˆè¾“å‡º
                                raw_new = self._diff_new_content(thinking_accumulator, delta_content)
                                if raw_new:
                                    cleaned_raw = self._clean_thinking(raw_new)
                                    if cleaned_raw:
                                        yield self._build_reasoning_chunk(
                                            json_lib,
                                            transformed,
                                            request,
                                            cleaned_raw,
                                        )
                                        thinking_accumulator += raw_new

                                # å†åšä¸€æ¬¡åŸºäºæ¸…æ´—åçš„å…œåº•å¢é‡ï¼Œé˜²æ­¢ä¸Šæ¸¸è¦†ç›–å¼ delta å¯¼è‡´é—æ¼
                                cleaned_full = self._clean_thinking(delta_content)
                                if cleaned_full:
                                    new_reasoning = self._diff_new_content(
                                        self._clean_thinking(thinking_accumulator),
                                        cleaned_full,
                                    )
                                    if new_reasoning:
                                        yield self._build_reasoning_chunk(
                                            json_lib,
                                            transformed,
                                            request,
                                            new_reasoning,
                                        )
                                        # è¿™é‡Œä¸å†ä¿®æ”¹ thinking_accumulatorï¼Œé¿å…ä¸åŸå§‹å¢é‡çŠ¶æ€ä¸ä¸€è‡´
                            continue

                        # answer é˜¶æ®µï¼šå¤„ç† edit_contentï¼ˆåŒ…å«å®Œæ•´thinkingï¼‰å’Œ delta_content
                        if phase == "answer":
                            # å¦‚æœæœ‰ edit_content ä¸”åŒ…å«å®Œæ•´ thinkingï¼Œå¿½ç•¥ï¼ˆå› ä¸ºå·²åœ¨ thinking é˜¶æ®µè¾“å‡ºï¼‰
                            if edit_content and "</details>" in edit_content:
                                # edit_content åŒ…å«å®Œæ•´çš„ thinkingï¼Œä½†æˆ‘ä»¬å·²ç»é€šè¿‡ delta è¾“å‡ºäº†
                                pass
                            
                            # æµå¼è¾“å‡º answer çš„ delta_content
                            if delta_content:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                yield self._build_content_chunk(json_lib, transformed, request, delta_content)
                                answer_accumulator += delta_content
                            continue

                        # other é˜¶æ®µï¼šå¯èƒ½æœ‰ usage ä¿¡æ¯ï¼Œä¹Ÿå¯èƒ½æºå¸¦æ­£æ–‡çš„æœ€åä¸€å°æ®µï¼ˆedit_content æˆ– delta_contentï¼‰
                        # å¦‚æœå¸¦æœ‰ edit_indexï¼Œè¯´æ˜æ˜¯å·¥å…·è°ƒç”¨ç›¸å…³å†…å®¹ï¼Œåº”æ”¾å…¥æ€è€ƒ
                        if phase == "other":
                            # 1) å…ˆå¤„ç† usage
                            if data.get("usage"):
                                yield self._build_usage_chunk(json_lib, transformed, request, data["usage"])

                            # 2) åˆ¤æ–­æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨ç›¸å…³å†…å®¹ï¼ˆå¸¦ edit_indexï¼‰
                            has_edit_index = data.get("edit_index") is not None
                            
                            # 3) å¤„ç†å†…å®¹
                            tail_text = None
                            if edit_content:
                                tail_text = edit_content
                            elif delta_content:
                                tail_text = delta_content

                            if tail_text:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                if has_edit_index:
                                    # å¸¦ edit_index çš„å†…å®¹ï¼šå…ˆæ£€æµ‹æ˜¯å¦æœ‰å›¾ç‰‡
                                    image_urls = self._extract_image_urls(tail_text)
                                    if image_urls:
                                        # æœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼æ”¾å…¥æ­£æ–‡
                                        markdown_images = self._format_images_as_markdown(image_urls)
                                        if markdown_images:
                                            yield self._build_content_chunk(json_lib, transformed, request, "\n\n" + markdown_images + "\n\n")
                                            answer_accumulator += markdown_images
                                    else:
                                        # æ— å›¾ç‰‡ï¼Œæ”¾å…¥ reasoning_contentï¼ˆæ€è€ƒï¼‰
                                        new_thinking = self._diff_new_content(thinking_accumulator, tail_text)
                                        if new_thinking:
                                            cleaned = self._clean_thinking(new_thinking)
                                            if cleaned:
                                                yield self._build_reasoning_chunk(json_lib, transformed, request, cleaned)
                                                thinking_accumulator += new_thinking
                                else:
                                    # æ™®é€šå†…å®¹æ”¾å…¥ contentï¼ˆæ­£æ–‡ï¼‰
                                    yield self._build_content_chunk(json_lib, transformed, request, tail_text)
                            continue

                        # è¾“å‡º usage ä¿¡æ¯
                        if data.get("usage"):
                            yield self._build_usage_chunk(json_lib, transformed, request, data["usage"])

                        # æ£€æŸ¥æ˜¯å¦ä¸º done çŠ¶æ€
                        if is_done:
                            finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                            yield finish_chunk
                            yield "data: [DONE]\n\n"
                            await self._mark_token_success(transformed)
                            request_stage_log("stream_completed", "æµå¼å“åº”å®Œæˆ", has_error=False)
                            return

                    finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                    yield finish_chunk
                    yield "data: [DONE]\n\n"

                    await self._mark_token_success(transformed)
                    request_stage_log(
                        "stream_completed",
                        "æµå¼å“åº”å®Œæˆ",
                        has_error=False,
                    )
                    return

            except Exception as exc:
                error_log("æµå¤„ç†é”™è¯¯", error=str(exc))
                retry_count += 1
                last_error = str(exc)
                last_status_code = None

                if network_manager.has_proxy_pool() and "connect" in str(exc).lower():
                    await network_manager.switch_proxy_on_failure()

                if retry_count > settings.MAX_RETRIES:
                    error_response = {
                        "error": {
                            "message": f"Stream processing failed: {last_error}",
                            "type": "stream_error",
                        }
                    }
                    yield f"data: {json_lib.dumps(error_response)}\n\n"
                    yield "data: [DONE]\n\n"
                    error_log("[REQUEST] æµå¼å“åº”é”™è¯¯")
                    return
            finally:
                reset_request_context("mode")

    async def _mark_token_success(self, transformed: dict) -> None:
        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        if current_token:
            token_pool.mark_token_success(current_token)

    async def _handle_retryable_error(
        self,
        status_code: int,
        retry_count: int,
        transformed: dict,
        request_dict_for_transform: dict,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
    ) -> Tuple[bool, dict, httpx.AsyncClient, Optional[str], str]:
        retryable_codes = [400, 401, 405, 429, 502, 503, 504]
        if status_code not in retryable_codes or retry_count >= settings.MAX_RETRIES:
            return False, transformed, request_client, current_proxy, current_upstream

        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        
        # Token é”™è¯¯å¤„ç†ï¼šæ ‡è®°å¤±è´¥å¹¶åˆ‡æ¢
        if current_token:
            token_pool.mark_token_failure(current_token)
        info_log(f"[TOKEN] Tokené”™è¯¯ {status_code}ï¼Œåˆ‡æ¢Token")

        await self.transformer.switch_token()
        await self.transformer.refresh_header_template()
        current_upstream = await network_manager.get_next_upstream()
        transformed = await self.transformer.transform_request_in(
            request_dict_for_transform,
            client=request_client,
            upstream_url=current_upstream,
        )
        info_log("[OK] å·²åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé…ç½®Token")

        if network_manager.has_upstream_pool() and network_manager.upstream_strategy == "failover":
            await network_manager.switch_upstream_on_failure()
            current_upstream = await network_manager.get_next_upstream()
            transformed = await self.transformer.transform_request_in(
                request_dict_for_transform,
                client=request_client,
                upstream_url=current_upstream,
            )
            info_log("[FAILOVER] Tokené”™è¯¯ï¼Œå·²åˆ‡æ¢ä¸Šæ¸¸")

        if status_code in [502, 503, 504]:
            if network_manager.has_proxy_pool() and network_manager.proxy_strategy == "failover":
                await network_manager.switch_proxy_on_failure()
            if network_manager.has_upstream_pool() and network_manager.upstream_strategy == "failover":
                await network_manager.switch_upstream_on_failure()
                current_upstream = await network_manager.get_next_upstream()
                transformed = await self.transformer.transform_request_in(
                    request_dict_for_transform,
                    client=request_client,
                    upstream_url=current_upstream,
                )
                info_log("[FAILOVER] ç½‘ç»œé”™è¯¯ï¼Œå·²åˆ‡æ¢ä¸Šæ¸¸")

        return True, transformed, request_client, current_proxy, current_upstream

    async def mark_token_success_if_configured(self, transformed: dict) -> None:
        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        if current_token:
            token_pool.mark_token_success(current_token)

    def _build_role_chunk(self, json_lib, transformed: dict, request: OpenAIRequest) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'role': 'assistant'},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_content_chunk(
        self,
        json_lib,
        transformed: dict,
        request: OpenAIRequest,
        content: str,
    ) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'content': content},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_reasoning_chunk(
        self,
        json_lib,
        transformed: dict,
        request: OpenAIRequest,
        reasoning_content: str,
    ) -> str:
        """æ„å»ºåŒ…å« reasoning_content çš„æµå¼å“åº”å—"""
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'reasoning_content': reasoning_content},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_usage_chunk(self, json_lib, transformed: dict, request: OpenAIRequest, usage) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
            'usage': usage,
        })}\n\n"

    def _build_finish_chunk(self, json_lib, transformed: dict, request: OpenAIRequest) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {},
                'finish_reason': 'stop',
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _extract_search_info(self, reasoning_content: str, edit_content: str) -> str:
        """ä» edit_content ä¸­æå–æœç´¢ä¿¡æ¯"""
        if edit_content and "<glm_block" in edit_content and "search" in edit_content:
            try:
                import re
                decoded = edit_content
                try:
                    decoded = edit_content.encode("utf-8").decode("unicode_escape").encode("latin1").decode("utf-8")
                except Exception:
                    try:
                        import codecs
                        decoded = codecs.decode(edit_content, "unicode_escape")
                    except Exception:
                        pass

                queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                if queries_match:
                    queries_str = queries_match.group(1)
                    queries = re.findall(r'"([^"]+)"', queries_str)
                    if queries:
                        search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])
                        reasoning_content += f"\n\n{search_info}\n\n"
                        debug_log("[æœç´¢ä¿¡æ¯] æå–åˆ°æœç´¢æŸ¥è¯¢", queries=queries)
            except Exception as exc:
                debug_log("[æœç´¢ä¿¡æ¯] æå–å¤±è´¥", error=str(exc))
        return reasoning_content

    def _extract_image_urls(self, content: str) -> list:
        """ä»ä¸Šæ¸¸å“åº”å†…å®¹ä¸­æå–å›¾ç‰‡URL
        
        å¤„ç†æ ¼å¼ç¤ºä¾‹ï¼š
        1. {"image_url":{"url":"https://qc4n.bigmodel.cn/xxx.png?..."}}
        2. {"img_url": "https://bigmodel-us3-prod-agent.cn-wlcb.ufileos.com/xxx.jpg", ...}
        
        Returns:
            list: æå–åˆ°çš„å›¾ç‰‡URLåˆ—è¡¨
        """
        import re
        
        if not content:
            return []
        
        image_urls = []
        
        # === æ ¼å¼1: image_url ç±»å‹ï¼ˆbigmodel.cn åŸŸåï¼‰===
        # åŒ¹é… "type":"image_url" åé¢çš„ "url":"xxx" æ¨¡å¼
        if '"type\\":\\"image_url\\"' in content or '"type":"image_url"' in content:
            # æ¨¡å¼1ï¼šè½¬ä¹‰JSONæ ¼å¼ \"url\":\"xxx\"
            pattern1 = r'\"url\":\s*\"(https?://[^\"\\]+(?:\\.[^\"\\]+)*[^\"\\]*)\"'
            # æ¨¡å¼2ï¼šæ™®é€šJSONæ ¼å¼ "url":"xxx"
            pattern2 = r'"url":\s*"(https?://[^"]+)"'
            
            matches = re.findall(pattern1, content)
            for url in matches:
                clean_url = url.replace('\\/', '/').replace('\\"', '"')
                if clean_url and 'bigmodel.cn' in clean_url:
                    image_urls.append(clean_url)
            
            if not image_urls:
                matches = re.findall(pattern2, content)
                for url in matches:
                    if url and 'bigmodel.cn' in url:
                        image_urls.append(url)
        
        # === æ ¼å¼2: img_url ç±»å‹ï¼ˆufileos.com åŸŸåï¼Œimage_reference å·¥å…·è°ƒç”¨ï¼‰===
        # åŒ¹é… "img_url": "https://xxx.ufileos.com/xxx.jpg" æ¨¡å¼
        if 'img_url' in content or 'image_reference' in content:
            # è½¬ä¹‰JSONæ ¼å¼: \"img_url\": \"xxx\"
            pattern_img = r'\\\"img_url\\\":\s*\\\"(https?://[^\\\"]+)\\\"'
            matches = re.findall(pattern_img, content)
            for url in matches:
                clean_url = url.replace('\\/', '/')
                if clean_url and ('ufileos.com' in clean_url or 'bigmodel' in clean_url):
                    image_urls.append(clean_url)
            
            # æ™®é€šJSONæ ¼å¼: "img_url": "xxx"
            if not image_urls:
                pattern_img2 = r'"img_url":\s*"(https?://[^"]+)"'
                matches = re.findall(pattern_img2, content)
                for url in matches:
                    if url and ('ufileos.com' in url or 'bigmodel' in url):
                        image_urls.append(url)
        
        return image_urls

    def _format_images_as_markdown(self, image_urls: list) -> str:
        """å°†å›¾ç‰‡URLåˆ—è¡¨æ ¼å¼åŒ–ä¸ºmarkdownå›¾ç‰‡æ ¼å¼
        
        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            
        Returns:
            str: markdownæ ¼å¼çš„å›¾ç‰‡å­—ç¬¦ä¸²
        """
        if not image_urls:
            return ""
        
        markdown_images = []
        for i, url in enumerate(image_urls, 1):
            # ç”Ÿæˆmarkdownå›¾ç‰‡æ ¼å¼
            markdown_images.append(f"![å›¾ç‰‡{i}]({url})")
        
        return "\n\n".join(markdown_images)

    def _clean_thinking(self, delta_content: str) -> str:
        """æ¸…ç† thinking å†…å®¹ï¼Œæå–çº¯æ–‡æœ¬
        
        å¤„ç†æ ¼å¼ï¼š
        - ç§»é™¤ <details> å’Œ <summary> æ ‡ç­¾
        - ç§»é™¤ markdown å¼•ç”¨ç¬¦å· "> "
        - ä¿ç•™çº¯æ–‡æœ¬å†…å®¹
        """
        import re
        
        if not delta_content:
            return ""
        
        # 0. å…ˆä¸¢å¼ƒå¯èƒ½å‡ºç°åœ¨ <details> ä¹‹å‰çš„å±æ€§æ®‹ç‰‡ï¼Œå¦‚ï¼štrue" duration="2" view="" last_tool_call_name="">
        #   è¿™ç±»å†…å®¹é€šå¸¸å‡ºç°åœ¨ edit_content å¼€å¤´ï¼Œä½†å¹¶ä¸æ˜¯æ€è€ƒæ­£æ–‡çš„ä¸€éƒ¨åˆ†
        #   ç­–ç•¥ï¼šå¦‚æœé¦–è¡ŒåŒ…å« duration= æˆ– last_tool_call_name ç­‰å­—æ®µï¼Œä¸”ä»¥ "> æˆ– "> ç»“å°¾ï¼Œåˆ™è§†ä¸ºå±æ€§ä¸²ï¼Œä¸¢å¼ƒæ•´è¡Œ
        first_newline = delta_content.find("\n")
        if first_newline != -1:
            first_line = delta_content[:first_newline].strip()
            # å¦‚æœè¿™ä¸€è¡ŒåŒ…å«å…¸å‹çš„ <details> å±æ€§å­—æ®µï¼Œä¸”ä»¥ > æˆ– "> ç»“å°¾ï¼Œåˆ¤å®šä¸ºå±æ€§æ®‹ç‰‡
            if re.search(r'(duration=|last_tool_call_name|view=)', first_line) and re.search(r'[">]$', first_line):
                delta_content = delta_content[first_newline + 1 :]

        # 1. ç§»é™¤ <details> å¼€å§‹æ ‡ç­¾ï¼ˆåŒ…æ‹¬æ‰€æœ‰å±æ€§ï¼‰
        delta_content = re.sub(r'<details[^>]*>', '', delta_content)
        
        # 2. ç§»é™¤ </details> ç»“æŸæ ‡ç­¾
        delta_content = re.sub(r'</details>', '', delta_content)
        
        # 3. ç§»é™¤ <summary> æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆå¦‚ "Thinking..." æˆ– "Thought for X seconds"ï¼‰
        delta_content = re.sub(r'<summary[^>]*>.*?</summary>', '', delta_content, flags=re.DOTALL)
        
        # 4. ç§»é™¤è¡Œé¦–çš„å¼•ç”¨æ ‡è®° "> "ï¼ˆmarkdown æ ¼å¼ï¼‰
        delta_content = re.sub(r'^>\s*', '', delta_content, flags=re.MULTILINE)
        delta_content = re.sub(r'\n>\s*', '\n', delta_content)
        
        # 5. ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆ3ä¸ªåŠä»¥ä¸Šè¿ç»­æ¢è¡Œç¬¦ï¼‰
        delta_content = re.sub(r'\n{3,}', '\n\n', delta_content)
        
        # 6. å»é™¤é¦–å°¾ç©ºç™½
        return delta_content.strip()

    def _split_edit_content(self, edit_content: str) -> Tuple[str, str]:
        """æ‹†åˆ† edit_contentï¼Œè¿”å› (thinking_part, answer_part)
        
        å¤„ç†æ ¼å¼ï¼š
        <details type="reasoning" done="false/true" ...>
        <summary>Thinking...</summary>
        > æ€è€ƒå†…å®¹
        </details>
        å›ç­”å†…å®¹
        """
        if not edit_content:
            return "", ""

        thinking_part = ""
        answer_part = ""

        # æŸ¥æ‰¾ </details> æ ‡ç­¾ä½ç½®
        if "</details>" in edit_content:
            # åˆ†å‰² thinking å’Œ answer éƒ¨åˆ†
            parts = edit_content.split("</details>", 1)
            thinking_part = parts[0] + "</details>"  # ä¿ç•™å®Œæ•´çš„ details æ ‡ç­¾ç”¨äºæ¸…ç†
            answer_part = parts[1] if len(parts) > 1 else ""
        else:
            # æ²¡æœ‰ details æ ‡ç­¾ï¼Œæ•´ä¸ªå†…å®¹å½“ä½œç­”æ¡ˆ
            answer_part = edit_content

        # æ¸…ç† thinking å†…å®¹ï¼ˆç§»é™¤æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬ï¼‰
        if thinking_part:
            thinking_part = self._clean_thinking(thinking_part)
        
        # æ¸…ç† answer å†…å®¹ï¼ˆç§»é™¤å¯èƒ½çš„æ ‡ç­¾ï¼‰
        answer_part = answer_part.strip()
        if answer_part:
            # ç§»é™¤å¼€å¤´çš„æ¢è¡Œç¬¦
            answer_part = answer_part.lstrip('\n')
            # ç§»é™¤å¯èƒ½åŒ…å«çš„ think æ ‡ç­¾
            answer_part = answer_part.replace("<think>", "").replace("</think>", "")
        
        return thinking_part, answer_part

    def _diff_new_content(self, existing: str, incoming: str) -> str:
        """è®¡ç®— incoming ç›¸æ¯” existing çš„æ–°å¢éƒ¨åˆ†ï¼ˆç”¨äºæµå¼å¢é‡è¾“å‡ºï¼‰"""
        incoming = incoming or ""
        if not incoming:
            return ""

        existing = existing or ""
        if not existing:
            return incoming

        if incoming == existing:
            return ""

        # å¦‚æœ incoming æ˜¯ existing çš„æ‰©å±•ï¼Œè¿”å›æ–°å¢éƒ¨åˆ†
        if incoming.startswith(existing):
            return incoming[len(existing):]

        # å¯»æ‰¾æœ€é•¿å…¬å…±å‰ç¼€ä»¥è®¡ç®—å¢é‡
        max_overlap = min(len(existing), len(incoming))
        for overlap in range(max_overlap, 0, -1):
            if existing[-overlap:] == incoming[:overlap]:
                return incoming[overlap:]

        # å¦‚æœ existing å®Œå…¨åŒ…å«åœ¨ incoming ä¸­
        if existing in incoming:
            return incoming.replace(existing, "", 1)

        # æ— æ³•ç¡®å®šå¢é‡ï¼Œè¿”å›å®Œæ•´å†…å®¹
        return incoming


chat_completion_service = ChatCompletionService()

