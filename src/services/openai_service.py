"""Service layer orchestrating OpenAI-compatible chat completions."""

from __future__ import annotations

import asyncio
import json
import time
from typing import AsyncIterator, Dict, Optional, Tuple

import httpx
from fastapi import HTTPException

from ..helpers import (
    info_log,
    debug_log,
    error_log,
    bind_request_context,
    reset_request_context,
    request_stage_log,
)
from ..schemas import OpenAIRequest
from ..config import settings
from ..toolify.detector import StreamingFunctionCallDetector
from ..toolify_handler import (
    should_enable_toolify,
    prepare_toolify_request,
    parse_toolify_response,
    format_toolify_response_for_stream,
)
from ..toolify_config import get_toolify
from ..zai_transformer import ZAITransformer
from ..token_pool import get_token_pool
from .network_manager import network_manager


class ChatCompletionService:
    """Encapsulate chat completion workflow independent of FastAPI layer."""

    def __init__(self) -> None:
        self.transformer = ZAITransformer()

    async def prepare_request(self, request: OpenAIRequest) -> Tuple[dict, dict, bool]:
        request_dict = request.model_dump()
        return self._prepare_messages(request, request_dict)

    def _prepare_messages(self, request: OpenAIRequest, request_dict: dict) -> Tuple[dict, dict, bool]:
        enable_toolify = should_enable_toolify(request_dict)
        messages = [
            msg.model_dump() if hasattr(msg, "model_dump") else msg
            for msg in request.messages
        ]

        if enable_toolify:
            info_log("[TOOLIFY] å·¥å…·è°ƒç”¨åŠŸèƒ½å·²å¯ç”¨")
            messages, _ = prepare_toolify_request(request_dict, messages)
            transformed_dict = request_dict.copy()
            transformed_dict.pop("tools", None)
            transformed_dict.pop("tool_choice", None)
            transformed_dict["messages"] = messages
        else:
            transformed_dict = request_dict

        return request_dict, transformed_dict, enable_toolify

    async def build_transformed(self, request_dict: dict, client: httpx.AsyncClient, upstream: str) -> dict:
        request_stage_log("transform_in", "å¼€å§‹è½¬æ¢è¯·æ±‚æ ¼å¼: OpenAI -> Z.AI", upstream=upstream)
        return await self.transformer.transform_request_in(
            request_dict,
            client=client,
            upstream_url=upstream,
        )

    async def get_request_context(self) -> Tuple[httpx.AsyncClient, Optional[str], str]:
        client, proxy = await network_manager.get_request_client()
        upstream = await network_manager.get_next_upstream()
        bind_request_context(proxy=proxy, upstream=upstream)
        debug_log("[REQUEST] è·å–è¯·æ±‚ä¸Šä¸‹æ–‡", proxy=proxy or "ç›´è¿", upstream=upstream)
        return client, proxy, upstream

    async def ensure_authorization(self, authorization: str) -> None:
        if settings.SKIP_AUTH_TOKEN:
            return

        if not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

        api_key = authorization[7:]
        if api_key != settings.AUTH_TOKEN:
            raise HTTPException(status_code=401, detail="Invalid API key")

    async def handle_non_stream_request(
        self,
        request: OpenAIRequest,
        transformed: dict,
        enable_toolify: bool,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
        request_dict_for_transform: dict,
        json_lib,
    ) -> dict:
        bind_request_context(mode="non_stream")
        request_stage_log("non_stream_pipeline", "è¿›å…¥éæµå¼å¤„ç†æµç¨‹")
        final_content = ""
        reasoning_content = ""
        usage_info = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }

        retry_count = 0
        last_error = None
        last_status_code = None

        while retry_count <= settings.MAX_RETRIES:
            try:
                if retry_count > 0:
                    delay = self.calculate_backoff_delay(retry_count, last_status_code)
                    info_log(
                        "[RETRY] éæµå¼è¯·æ±‚é‡è¯•",
                        retry_count=retry_count,
                        delay=f"{delay:.2f}s",
                    )
                    await asyncio.sleep(delay)

                client = request_client
                headers = transformed["config"]["headers"].copy()

                attempt = retry_count + 1
                request_stage_log(
                    "upstream_request",
                    "å‘ä¸Šæ¸¸å‘èµ·éæµå¼è¯·æ±‚",
                    attempt=attempt,
                    upstream=current_upstream,
                    proxy=current_proxy or "direct",
                )
                request_start_time = time.perf_counter()
                async with client.stream(
                    "POST",
                    transformed["config"]["url"],
                    json=transformed["body"],
                    headers=headers,
                ) as response:
                    ttfb = (time.perf_counter() - request_start_time) * 1000
                    debug_log("â±ï¸ éæµå¼ä¸Šæ¸¸TTFB", ttfb_ms=f"{ttfb:.2f}ms")

                    if response.status_code != 200:
                        last_status_code = response.status_code
                        error_text = await response.aread()
                        error_msg = error_text.decode("utf-8", errors="ignore")
                        error_log(
                            "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                            status_code=response.status_code,
                            error_detail=error_msg[:200],
                        )

                        should_retry, transformed, request_client, current_proxy, current_upstream = await self._handle_retryable_error(
                            response.status_code,
                            retry_count,
                            transformed,
                            request_dict_for_transform,
                            request_client,
                            current_proxy,
                            current_upstream,
                        )
                        if should_retry:
                            retry_count += 1
                            continue

                        raise HTTPException(
                            status_code=response.status_code,
                            detail=f"Upstream error: {error_msg[:500]}",
                        )

                    request_stage_log(
                        "upstream_response",
                        "Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹èšåˆéæµå¼æ•°æ®",
                        status="success",
                        attempt=attempt,
                    )

                    final_content = ""
                    reasoning_content = ""

                    async for line in response.aiter_lines():
                        if not line:
                            continue

                        line = line.strip()
                        if not line.startswith("data:"):
                            try:
                                maybe_err = json_lib.loads(line)
                                if isinstance(maybe_err, dict) and (
                                    "error" in maybe_err or "code" in maybe_err or "message" in maybe_err
                                ):
                                    msg = (
                                        (maybe_err.get("error") or {}).get("message")
                                        if isinstance(maybe_err.get("error"), dict)
                                        else maybe_err.get("message")
                                    ) or "ä¸Šæ¸¸è¿”å›é”™è¯¯"
                                    raise HTTPException(status_code=500, detail=msg)
                            except (json.JSONDecodeError, HTTPException):
                                pass
                            continue

                        data_str = line[5:].strip()
                        if not data_str or data_str.lower() == "[done]":
                            continue

                        try:
                            chunk = json_lib.loads(data_str)
                        except json.JSONDecodeError:
                            continue

                        if chunk.get("type") != "chat:completion":
                            continue

                        data = chunk.get("data", {})
                        phase = data.get("phase")
                        delta_content = data.get("delta_content", "")
                        edit_content = data.get("edit_content", "")

                        if data.get("usage"):
                            try:
                                usage_info = data["usage"]
                            except Exception:  # pragma: no cover
                                pass

                        if phase == "tool_call":
                            reasoning_content = self._extract_search_info(reasoning_content, edit_content)
                            continue

                        if phase == "thinking" and delta_content:
                            reasoning_content += self._clean_thinking(delta_content)
                        elif phase == "answer":
                            final_content += self._extract_answer(delta_content, edit_content)

                    final_content = (final_content or "").strip()
                    reasoning_content = (reasoning_content or "").strip()

                    # æ¸…ç†ä¸Šæ¸¸å¯èƒ½è‡ªå¸¦çš„thinkæ ‡ç­¾ï¼ˆé¿å…é‡å¤ï¼‰
                    reasoning_content = reasoning_content.replace("<think>", "").replace("</think>", "")
                    final_content = final_content.replace("<think>", "").replace("</think>", "")

                    if enable_toolify and final_content:
                        debug_log("[TOOLIFY] æ£€æŸ¥éæµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨")
                        tool_response = parse_toolify_response(final_content, request.model)
                        if tool_response:
                            info_log("[TOOLIFY] éæµå¼å“åº”ä¸­æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                            request_stage_log(
                                "non_stream_toolify",
                                "éæµå¼å“åº”ä¸­æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨",
                                finish_reason="tool_calls",
                            )
                            return {
                                "id": transformed["body"]["chat_id"],
                                "object": "chat.completion",
                                "created": int(time.time()),
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "message": tool_response,
                                        "finish_reason": "tool_calls",
                                    }
                                ],
                                "usage": usage_info,
                            }

                    request_stage_log(
                        "non_stream_completed",
                        "éæµå¼å“åº”å®Œæˆ",
                        completion_tokens=usage_info.get("completion_tokens"),
                        prompt_tokens=usage_info.get("prompt_tokens"),
                    )

                    # æ„å»ºæ¶ˆæ¯å¯¹è±¡ï¼Œå°† reasoning_content ä½œä¸ºå•ç‹¬çš„å­—æ®µ
                    message = {
                        "role": "assistant",
                        "content": final_content,
                    }
                    # åªæœ‰å½“å­˜åœ¨ reasoning_content æ—¶æ‰æ·»åŠ è¯¥å­—æ®µ
                    if reasoning_content:
                        message["reasoning_content"] = reasoning_content

                    return {
                        "id": transformed["body"]["chat_id"],
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": request.model,
                        "choices": [
                            {
                                "index": 0,
                                "message": message,
                                "finish_reason": "stop",
                            }
                        ],
                        "usage": usage_info,
                    }

            except HTTPException:
                raise
            except Exception as exc:
                error_log("éæµå¼å¤„ç†å‘ç”Ÿå¼‚å¸¸", error=str(exc))
                last_error = str(exc)
                retry_count += 1

        reset_request_context("mode")
        raise HTTPException(status_code=500, detail=f"éæµå¼è¯·æ±‚å¤±è´¥: {last_error}")

    def calculate_backoff_delay(
        self,
        retry_count: int,
        status_code: Optional[int] = None,
        base_delay: float = 1.5,
        max_delay: float = 8.0,
    ) -> float:
        linear_delay = base_delay * retry_count

        if status_code == 429:
            linear_delay *= 1.5
        elif status_code in [502, 503, 504]:
            linear_delay *= 1.2

        linear_delay = min(linear_delay, max_delay)
        jitter = linear_delay * 0.2
        return max(linear_delay + (2 * jitter * (time.time() % 1) - jitter), 0.5)

    async def stream_response(
        self,
        request: OpenAIRequest,
        transformed: dict,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
        request_dict_for_transform: dict,
        json_lib,
        enable_toolify: bool,
    ) -> AsyncIterator[str]:
        bind_request_context(mode="stream")
        request_stage_log("stream_pipeline", "è¿›å…¥æµå¼å¤„ç†æµç¨‹")
        retry_count = 0
        last_error = None
        last_status_code = None

        toolify_detector = None
        if enable_toolify:
            toolify_instance = get_toolify()
            if toolify_instance:
                toolify_detector = StreamingFunctionCallDetector(toolify_instance.trigger_signal)
                debug_log("[TOOLIFY] æµå¼å·¥å…·è°ƒç”¨æ£€æµ‹å™¨å·²åˆå§‹åŒ–")

        while retry_count <= settings.MAX_RETRIES:
            try:
                if retry_count > 0:
                    delay = self.calculate_backoff_delay(retry_count, last_status_code)
                    info_log(
                        "[RETRY] æµå¼è¯·æ±‚é‡è¯•",
                        retry_count=retry_count,
                        delay=f"{delay:.2f}s",
                        last_status=last_status_code,
                    )
                    await asyncio.sleep(delay)

                client = request_client
                headers = transformed["config"]["headers"].copy()

                attempt = retry_count + 1
                request_stage_log(
                    "upstream_request",
                    "å‘ä¸Šæ¸¸å‘èµ·æµå¼è¯·æ±‚",
                    attempt=attempt,
                    upstream=current_upstream,
                    proxy=current_proxy or "direct",
                )
                request_start_time = time.perf_counter()
                async with client.stream(
                    "POST",
                    transformed["config"]["url"],
                    json=transformed["body"],
                    headers=headers,
                ) as response:
                    ttfb = (time.perf_counter() - request_start_time) * 1000
                    debug_log("â±ï¸ ä¸Šæ¸¸TTFB (é¦–å­—èŠ‚æ—¶é—´)", ttfb_ms=f"{ttfb:.2f}ms")

                    if response.status_code != 200:
                        error_text = await response.aread()
                        error_msg = error_text.decode("utf-8", errors="ignore")
                        error_log(
                            "ä¸Šæ¸¸è¿”å›é”™è¯¯",
                            status_code=response.status_code,
                            error_detail=error_msg[:200],
                        )

                        should_retry, transformed, request_client, current_proxy, current_upstream = await self._handle_retryable_error(
                            response.status_code,
                            retry_count,
                            transformed,
                            request_dict_for_transform,
                            request_client,
                            current_proxy,
                            current_upstream,
                        )
                        if should_retry:
                            retry_count += 1
                            last_status_code = response.status_code
                            last_error = f"{response.status_code}: {error_msg}"
                            continue

                        error_response = {
                            "error": {
                                "message": f"Upstream error: {response.status_code}",
                                "type": "upstream_error",
                                "code": response.status_code,
                                "details": error_msg[:500],
                            }
                        }
                        yield f"data: {json_lib.dumps(error_response)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

                    request_stage_log(
                        "upstream_stream_ready",
                        "Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç† SSE æµ",
                        status="success",
                        attempt=attempt,
                    )

                    has_thinking = False

                    async for line in response.aiter_lines():
                        if not line or not line.strip():
                            continue

                        if not line.startswith("data:"):
                            continue

                        chunk_str = line[5:].strip()
                        if not chunk_str or chunk_str == "[DONE]":
                            if chunk_str == "[DONE]" and toolify_detector:
                                parsed_tools, remaining_content = toolify_detector.finalize()
                                if remaining_content:
                                    # æ¸…ç†å¯èƒ½åŒ…å«çš„thinkæ ‡ç­¾
                                    remaining_content = remaining_content.replace("<think>", "").replace("</think>", "")
                                    
                                    if remaining_content:
                                        if not has_thinking:
                                            has_thinking = True
                                            yield self._build_role_chunk(json_lib, transformed, request)
                                        yield self._build_content_chunk(json_lib, transformed, request, remaining_content)

                                if parsed_tools:
                                    for chunk in format_toolify_response_for_stream(
                                        parsed_tools,
                                        request.model,
                                        transformed["body"]["chat_id"],
                                    ):
                                        yield chunk
                                    request_stage_log(
                                        "stream_toolify_completed",
                                        "æµå¼å“åº”ï¼ˆæ—©æœŸå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰å®Œæˆ",
                                    )
                                    return

                            if chunk_str == "[DONE]":
                                yield "data: [DONE]\n\n"
                            continue

                        try:
                            chunk_data = json_lib.loads(chunk_str)
                        except json.JSONDecodeError:
                            continue

                        if chunk_data.get("type") != "chat:completion":
                            yield f"data: {chunk_str}\n\n"
                            continue

                        data = chunk_data.get("data", {})
                        delta_content = data.get("delta_content")
                        edit_content = data.get("edit_content")
                        edit_index = data.get("edit_index")
                        phase = data.get("phase")
                        is_done = phase == "done" or data.get("done")
                        error_info = data.get("error")

                        # è¯¦ç»†è°ƒè¯•ï¼šè®°å½•æ¯ä¸ªchunkçš„åŸå§‹æ•°æ®
                        debug_log(f"[RAW_CHUNK] phase={phase}, delta={bool(delta_content)}, edit={bool(edit_content)}, edit_index={edit_index}, usage={bool(data.get('usage'))}, done={data.get('done')}, error={bool(error_info)}")
                        if delta_content:
                            debug_log(f"[RAW_DELTA] len={len(delta_content)}, content={delta_content[:100]}")
                        if edit_content:
                            edit_len = len(edit_content)
                            edit_preview = edit_content[:200] if edit_len <= 500 else f"{edit_content[:100]}...{edit_content[-100:]}"
                            debug_log(f"[RAW_EDIT] len={edit_len}, content={edit_preview}")
                            # å¯¹äºåŒ…å« edit_content çš„å…³é”® chunkï¼Œè®°å½•å®Œæ•´ JSON
                            if edit_len > 10 or is_done:
                                debug_log(f"[RAW_JSON] {chunk_str[:1000]}")

                        # æ£€æµ‹ä¸Šæ¸¸è¿”å›çš„é”™è¯¯ï¼ˆå¦‚å†…å®¹å®‰å…¨è­¦å‘Šï¼‰
                        if error_info:
                            error_detail = error_info.get("detail") or error_info.get("content") or "Unknown error"
                            error_log(f"[UPSTREAM_ERROR] ä¸Šæ¸¸è¿”å›é”™è¯¯: {error_detail}")
                            
                            # å¦‚æœè¿˜æ²¡æœ‰å‘é€ä»»ä½•å†…å®¹ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                            if not has_thinking:
                                has_thinking = True
                                yield self._build_role_chunk(json_lib, transformed, request)
                            
                            # å‘é€é”™è¯¯æç¤ºç»™å®¢æˆ·ç«¯
                            error_message = f"\n\n[ç³»ç»Ÿæç¤º: {error_detail}]"
                            yield self._build_content_chunk(json_lib, transformed, request, error_message)
                            
                            # å¦‚æœåŒæ—¶æ ‡è®°ä¸º doneï¼Œç»“æŸæµ
                            if is_done:
                                finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                                yield finish_chunk
                                yield "data: [DONE]\n\n"
                                await self._mark_token_success(transformed)
                                request_stage_log(
                                    "stream_completed",
                                    "æµå¼å“åº”å®Œæˆï¼ˆå¸¦é”™è¯¯ï¼‰",
                                    has_error=True,
                                )
                                return
                            continue

                        # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•phaseå’Œæ˜¯å¦æœ‰å†…å®¹
                        if delta_content or edit_content:
                            debug_log(f"[PHASE] phase={phase}, has_delta={bool(delta_content)}, has_edit={bool(edit_content)}")

                        # å¤„ç†thinkingé˜¶æ®µï¼šé€šè¿‡ reasoning_content å­—æ®µæµå¼è¾“å‡º
                        if phase == "thinking":
                            if delta_content:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                # æ¸…ç†thinkingå†…å®¹ä¸­çš„HTMLæ ‡è®°ã€å¼•ç”¨ç¬¦å·å’Œthinkæ ‡ç­¾
                                cleaned_content = self._clean_thinking(delta_content)
                                
                                # é€šè¿‡ reasoning_content å­—æ®µè¾“å‡º
                                if cleaned_content:
                                    yield self._build_reasoning_chunk(json_lib, transformed, request, cleaned_content)
                            
                            # æ£€æŸ¥ edit_content æ˜¯å¦åŒ…å«å®Œæ•´çš„ thinking + answer
                            if edit_content and "</details>" in edit_content:
                                debug_log("[THINKING_EDIT] æ£€æµ‹åˆ° edit_content åŒ…å« </details>ï¼Œå¯èƒ½åŒ…å« answer")
                                # æå– </details> åçš„å†…å®¹ä½œä¸º answer
                                answer_content = edit_content.split("</details>")[-1].strip()
                                if answer_content:
                                    # æ¸…ç†å¯èƒ½çš„ think æ ‡ç­¾
                                    answer_content = answer_content.replace("<think>", "").replace("</think>", "")
                                    if answer_content:
                                        # è¾“å‡º answer å†…å®¹
                                        if not has_thinking:
                                            has_thinking = True
                                            yield self._build_role_chunk(json_lib, transformed, request)
                                        
                                        yield self._build_content_chunk(json_lib, transformed, request, answer_content)
                                        debug_log(f"[THINKING_EDIT] è¾“å‡º answer å†…å®¹: {answer_content[:50]}...")
                            
                            continue

                        # æ¸…ç†éthinkingé˜¶æ®µå†…å®¹ä¸­å¯èƒ½è‡ªå¸¦çš„thinkæ ‡ç­¾ï¼ˆé¿å…é‡å¤ï¼‰
                        if delta_content:
                            # ç§»é™¤ä¸Šæ¸¸è¿”å›çš„<think>å’Œ</think>æ ‡ç­¾
                            delta_content = delta_content.replace("<think>", "").replace("</think>", "")
                            if not delta_content:  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè·³è¿‡
                                continue

                        # è·³è¿‡tool_callé˜¶æ®µçš„å†…å®¹
                        if phase == "tool_call":
                            continue

                        # åœ¨answeré˜¶æ®µï¼Œå¤„ç†edit_contentï¼ˆå¯èƒ½åŒ…å«å®Œæ•´thinking + answerå¼€å¤´ï¼‰
                        if phase == "answer" and edit_content and not delta_content:
                            # è¿™ä¸ªchunkåªæœ‰edit_contentï¼Œå¯èƒ½åŒ…å«å®Œæ•´thinking + answerå¼€å¤´
                            # æå–</details>åçš„å†…å®¹ä½œä¸ºanswerå¼€å¤´
                            if "</details>" in edit_content:
                                answer_start = edit_content.split("</details>")[-1].strip()
                                if answer_start:
                                    # æ¸…ç†å¯èƒ½çš„thinkæ ‡ç­¾
                                    answer_start = answer_start.replace("<think>", "").replace("</think>", "")
                                    if answer_start:
                                        delta_content = answer_start
                                        debug_log(f"[EDIT_CONTENT] ä»edit_contentæå–answerå¼€å¤´: {answer_start[:50]}...")
                            # å¦‚æœæ²¡æœ‰</details>æˆ–æå–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªchunk
                            if not delta_content:
                                continue

                        if enable_toolify and toolify_detector:
                            yielded, should_continue, processed, has_thinking = self._process_toolify_detection(
                                toolify_detector,
                                delta_content,
                                has_thinking,
                                transformed,
                                request,
                                json_lib,
                            )
                            for chunk in yielded:
                                yield chunk
                            if should_continue:
                                continue
                            # æ¸…ç†å¤„ç†åçš„å†…å®¹ä¸­å¯èƒ½åŒ…å«çš„thinkæ ‡ç­¾
                            if processed:
                                processed = processed.replace("<think>", "").replace("</think>", "")
                            delta_content = processed

                        # è¾“å‡ºansweré˜¶æ®µçš„å†…å®¹
                        if delta_content and phase == "answer":
                            if not has_thinking:
                                has_thinking = True
                                yield self._build_role_chunk(json_lib, transformed, request)

                            yield self._build_content_chunk(json_lib, transformed, request, delta_content)

                        # å¤„ç† phase=other æ—¶çš„ edit_contentï¼ˆå¯èƒ½åŒ…å«æœ€åä¸€æ®µç­”æ¡ˆï¼‰
                        if phase == "other" and edit_content:
                            # æ¸…ç†å¯èƒ½çš„thinkæ ‡ç­¾
                            cleaned_edit = edit_content.replace("<think>", "").replace("</think>", "")
                            if cleaned_edit:
                                if not has_thinking:
                                    has_thinking = True
                                    yield self._build_role_chunk(json_lib, transformed, request)
                                
                                yield self._build_content_chunk(json_lib, transformed, request, cleaned_edit)
                                debug_log(f"[OTHER] è¾“å‡º phase=other çš„ edit_content: {cleaned_edit[:50]}...")

                        if data.get("usage"):
                            yield self._build_usage_chunk(json_lib, transformed, request, data["usage"])

                        # å¤„ç†å®Œå½“å‰ chunk çš„æ‰€æœ‰å†…å®¹åï¼Œæ£€æŸ¥æ˜¯å¦ä¸º done çŠ¶æ€
                        if is_done:
                            debug_log("[DONE] æ£€æµ‹åˆ° done æ ‡å¿—ï¼Œæµç»“æŸ")
                            
                            finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                            yield finish_chunk
                            yield "data: [DONE]\n\n"
                            
                            await self._mark_token_success(transformed)
                            request_stage_log(
                                "stream_completed",
                                "æµå¼å“åº”å®Œæˆ",
                                has_error=False,
                            )
                            return

                    finish_chunk = self._build_finish_chunk(json_lib, transformed, request)
                    yield finish_chunk
                    yield "data: [DONE]\n\n"

                    await self._mark_token_success(transformed)
                    request_stage_log(
                        "stream_completed",
                        "æµå¼å“åº”å®Œæˆ",
                        has_error=False,
                    )
                    return

            except Exception as exc:
                error_log("æµå¤„ç†é”™è¯¯", error=str(exc))
                retry_count += 1
                last_error = str(exc)
                last_status_code = None

                if network_manager.has_proxy_pool() and "connect" in str(exc).lower():
                    await network_manager.switch_proxy_on_failure()

                if retry_count > settings.MAX_RETRIES:
                    error_response = {
                        "error": {
                            "message": f"Stream processing failed: {last_error}",
                            "type": "stream_error",
                        }
                    }
                    yield f"data: {json_lib.dumps(error_response)}\n\n"
                    yield "data: [DONE]\n\n"
                    error_log("[REQUEST] æµå¼å“åº”é”™è¯¯")
                    return
            finally:
                reset_request_context("mode")

    async def _mark_token_success(self, transformed: dict) -> None:
        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        if current_token and not token_pool.is_anonymous_token(current_token):
            token_pool.mark_token_success(current_token)

    async def _handle_retryable_error(
        self,
        status_code: int,
        retry_count: int,
        transformed: dict,
        request_dict_for_transform: dict,
        request_client: httpx.AsyncClient,
        current_proxy: Optional[str],
        current_upstream: str,
    ) -> Tuple[bool, dict, httpx.AsyncClient, Optional[str], str]:
        retryable_codes = [400, 401, 405, 429, 502, 503, 504]
        if status_code not in retryable_codes or retry_count >= settings.MAX_RETRIES:
            return False, transformed, request_client, current_proxy, current_upstream

        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        is_anonymous = token_pool.is_anonymous_token(current_token)

        if is_anonymous:
            info_log(f"[ANONYMOUS] æ£€æµ‹åˆ°åŒ¿åTokené”™è¯¯ {status_code}ï¼Œæ¸…ç†ç¼“å­˜å¹¶é‡æ–°è·å–")
            await self.transformer.clear_anonymous_token_cache()
            await self.transformer.refresh_header_template()
            await network_manager.cleanup_current_client(current_proxy)
            new_client, new_proxy = await network_manager.get_request_client()
            request_client = new_client
            current_proxy = new_proxy

            if network_manager.has_upstream_pool() and network_manager.upstream_strategy == "failover":
                await network_manager.switch_upstream_on_failure()
                info_log("[FAILOVER] åŒ¿åTokené”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä¸Šæ¸¸åœ°å€")

            current_upstream = await network_manager.get_next_upstream()
            transformed = await self.transformer.transform_request_in(
                request_dict_for_transform,
                client=request_client,
                upstream_url=current_upstream,
            )
            info_log("[OK] å·²è·å–æ–°çš„åŒ¿åTokenå¹¶é‡æ–°ç”Ÿæˆè¯·æ±‚")
        else:
            token_pool.mark_token_failure(current_token)
            info_log(f"[CONFIG] é…ç½®Tokené”™è¯¯ {status_code}ï¼Œåˆ‡æ¢Token")

            await self.transformer.switch_token()
            await self.transformer.refresh_header_template()
            current_upstream = await network_manager.get_next_upstream()
            transformed = await self.transformer.transform_request_in(
                request_dict_for_transform,
                client=request_client,
                upstream_url=current_upstream,
            )
            info_log("[OK] å·²åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé…ç½®Token")

            if network_manager.has_upstream_pool() and network_manager.upstream_strategy == "failover":
                await network_manager.switch_upstream_on_failure()
                current_upstream = await network_manager.get_next_upstream()
                transformed = await self.transformer.transform_request_in(
                    request_dict_for_transform,
                    client=request_client,
                    upstream_url=current_upstream,
                )
                info_log("[FAILOVER] Tokené”™è¯¯ï¼Œå·²åˆ‡æ¢ä¸Šæ¸¸")

        if status_code in [502, 503, 504]:
            if network_manager.has_proxy_pool() and network_manager.proxy_strategy == "failover":
                await network_manager.switch_proxy_on_failure()
            if network_manager.has_upstream_pool() and network_manager.upstream_strategy == "failover":
                await network_manager.switch_upstream_on_failure()
                current_upstream = await network_manager.get_next_upstream()
                transformed = await self.transformer.transform_request_in(
                    request_dict_for_transform,
                    client=request_client,
                    upstream_url=current_upstream,
                )
                info_log("[FAILOVER] ç½‘ç»œé”™è¯¯ï¼Œå·²åˆ‡æ¢ä¸Šæ¸¸")

        return True, transformed, request_client, current_proxy, current_upstream

    def _process_toolify_detection(
        self,
        toolify_detector,
        delta_content: str,
        has_thinking: bool,
        transformed: dict,
        request: OpenAIRequest,
        json_lib,
    ) -> Tuple[list, bool, str, bool]:
        chunks_to_yield = []

        if not toolify_detector or not delta_content:
            return chunks_to_yield, False, delta_content, has_thinking

        debug_log("[TOOLIFY] è°ƒç”¨å·¥å…·æ£€æµ‹å™¨")
        is_tool_detected, content_to_yield = toolify_detector.process_chunk(delta_content)

        if is_tool_detected:
            if content_to_yield:
                if not has_thinking:
                    has_thinking = True
                    chunks_to_yield.append(self._build_role_chunk(json_lib, transformed, request))
                chunks_to_yield.append(
                    self._build_content_chunk(json_lib, transformed, request, content_to_yield)
                )

            return chunks_to_yield, True, "", has_thinking

        return chunks_to_yield, False, content_to_yield, has_thinking

    async def mark_token_success_if_configured(self, transformed: dict) -> None:
        token_pool = await get_token_pool()
        current_token = transformed.get("token", "")
        if current_token and not token_pool.is_anonymous_token(current_token):
            token_pool.mark_token_success(current_token)

    def _build_role_chunk(self, json_lib, transformed: dict, request: OpenAIRequest) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'role': 'assistant'},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_content_chunk(
        self,
        json_lib,
        transformed: dict,
        request: OpenAIRequest,
        content: str,
    ) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'content': content},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_reasoning_chunk(
        self,
        json_lib,
        transformed: dict,
        request: OpenAIRequest,
        reasoning_content: str,
    ) -> str:
        """æ„å»ºåŒ…å« reasoning_content çš„æµå¼å“åº”å—"""
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {'reasoning_content': reasoning_content},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _build_usage_chunk(self, json_lib, transformed: dict, request: OpenAIRequest, usage) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {},
                'finish_reason': None,
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
            'usage': usage,
        })}\n\n"

    def _build_finish_chunk(self, json_lib, transformed: dict, request: OpenAIRequest) -> str:
        return f"data: {json_lib.dumps({
            'choices': [{
                'delta': {},
                'finish_reason': 'stop',
                'index': 0,
                'logprobs': None,
            }],
            'created': int(time.time()),
            'id': transformed['body']['chat_id'],
            'model': request.model,
            'object': 'chat.completion.chunk',
            'system_fingerprint': 'fp_zai_001',
        })}\n\n"

    def _extract_search_info(self, reasoning_content: str, edit_content: str) -> str:
        if edit_content and "<glm_block" in edit_content and "search" in edit_content:
            try:
                import re
                decoded = edit_content
                try:
                    decoded = edit_content.encode("utf-8").decode("unicode_escape").encode("latin1").decode("utf-8")
                except Exception:
                    try:
                        import codecs
                        decoded = codecs.decode(edit_content, "unicode_escape")
                    except Exception:
                        pass

                queries_match = re.search(r'"queries":\s*\[(.*?)\]', decoded)
                if queries_match:
                    queries_str = queries_match.group(1)
                    queries = re.findall(r'"([^"]+)"', queries_str)
                    if queries:
                        search_info = "ğŸ” **æœç´¢ï¼š** " + "ã€€".join(queries[:5])
                        reasoning_content += f"\n\n{search_info}\n\n"
                        debug_log("[éæµå¼] æå–åˆ°æœç´¢ä¿¡æ¯", queries=queries)
            except Exception as exc:
                debug_log("[éæµå¼] æå–æœç´¢ä¿¡æ¯å¤±è´¥", error=str(exc))
        return reasoning_content

    def _clean_thinking(self, delta_content: str) -> str:
        import re
        
        # æ¸…ç†detailsæ ‡ç­¾çš„å¼€å¤´
        if delta_content.startswith("<details"):
            if "</summary>" in delta_content:
                # æå–</summary>åçš„å†…å®¹
                delta_content = delta_content.split("</summary>")[-1].strip()
        
        # ç§»é™¤å¯èƒ½å‡ºç°çš„summaryæ ‡ç­¾
        delta_content = re.sub(r'<summary[^>]*>.*?</summary>', '', delta_content, flags=re.DOTALL)
        
        # ç§»é™¤detailsæ ‡ç­¾
        delta_content = re.sub(r'</?details[^>]*>', '', delta_content)
        
        # ç§»é™¤å¼•ç”¨æ ‡è®° "> " (markdownå¼•ç”¨æ ¼å¼)
        delta_content = re.sub(r'^>\s*', '', delta_content, flags=re.MULTILINE)
        delta_content = re.sub(r'\n>\s*', '\n', delta_content)
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦
        delta_content = re.sub(r'\n{3,}', '\n\n', delta_content)
        
        return delta_content.strip()

    def _extract_answer(self, delta_content: str, edit_content: str) -> str:
        result = ""
        if edit_content and "</details>\n" in edit_content:
            content_after = edit_content.split("</details>\n")[-1]
            if content_after:
                result = content_after
        else:
            result = delta_content or ""
        
        return result


chat_completion_service = ChatCompletionService()

