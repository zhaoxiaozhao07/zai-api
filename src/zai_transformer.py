#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ZAIæ ¼å¼è½¬æ¢å™¨
"""

import asyncio
import time
import random
from typing import Dict, Any, Tuple, List, Optional
from functools import lru_cache
from fastuuid import uuid4
from furl import furl
from dateutil import tz
from datetime import datetime
from browserforge.headers import HeaderGenerator
from fastapi import HTTPException

from .config import settings, MODEL_MAPPING
from .helpers import error_log, info_log, debug_log, perf_timer, perf_track
from .signature import SignatureGenerator, decode_jwt_payload
from .token_pool import get_token_pool
from .image_handler import process_image_content


# å…¨å±€ HeaderGenerator å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_header_generator_instance = None

# ç¼“å­˜çš„æ—¶åŒºå¯¹è±¡ï¼ˆé¿å…é‡å¤æŸ¥æ‰¾ï¼‰
_cached_timezone = None


@lru_cache(maxsize=8)
def get_timezone(tz_name: str = "Asia/Shanghai"):
    """
    è·å–æ—¶åŒºå¯¹è±¡ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        tz_name: æ—¶åŒºåç§°ï¼Œé»˜è®¤ Asia/Shanghai
        
    Returns:
        æ—¶åŒºå¯¹è±¡
    """
    return tz.gettz(tz_name)


def generate_time_variables(timezone_name: str = "Asia/Shanghai") -> Dict[str, str]:
    """
    ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ—¶é—´ç›¸å…³å˜é‡ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    
    Args:
        timezone_name: æ—¶åŒºåç§°
        
    Returns:
        åŒ…å«æ‰€æœ‰æ—¶é—´å˜é‡çš„å­—å…¸
    """
    # è·å–ç¼“å­˜çš„æ—¶åŒºå¯¹è±¡
    timezone = get_timezone(timezone_name)
    
    # ä¸€æ¬¡è°ƒç”¨ datetime.now()ï¼Œé¿å…å¤šæ¬¡è°ƒç”¨
    now = datetime.now(tz=timezone)
    
    return {
        "{{CURRENT_DATETIME}}": now.strftime("%Y-%m-%d %H:%M:%S"),
        "{{CURRENT_DATE}}": now.strftime("%Y-%m-%d"),
        "{{CURRENT_TIME}}": now.strftime("%H:%M:%S"),
        "{{CURRENT_WEEKDAY}}": now.strftime("%A"),
        "{{CURRENT_TIMEZONE}}": timezone_name,
        "{{USER_LANGUAGE}}": "zh-CN",
    }


def get_header_generator_instance() -> HeaderGenerator:
    """è·å–æˆ–åˆ›å»º HeaderGenerator å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _header_generator_instance
    if _header_generator_instance is None:
        # é…ç½®HeaderGeneratorï¼šä¼˜å…ˆChromeå’ŒEdgeæµè§ˆå™¨ï¼ŒWindowså¹³å°ï¼Œæ¡Œé¢è®¾å¤‡
        _header_generator_instance = HeaderGenerator(
            browser=('chrome', 'edge'),
            os='windows',
            device='desktop',
            locale=('zh-CN', 'en-US'),
            http_version=2
        )
    return _header_generator_instance


def generate_uuid() -> str:
    """ç”ŸæˆUUID v4ï¼ˆä½¿ç”¨fastuuidæå‡æ€§èƒ½ï¼‰"""
    return str(uuid4())


# Headeræ¨¡æ¿ç¼“å­˜ï¼ˆå‡å°‘BrowserForgeè°ƒç”¨ï¼‰
_header_template_cache = None
_header_cache_lock = asyncio.Lock()


async def get_header_template() -> Dict[str, str]:
    """
    è·å–ç¼“å­˜çš„headeræ¨¡æ¿ï¼ˆä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
    
    Returns:
        headeræ¨¡æ¿å­—å…¸
    """
    global _header_template_cache
    
    # å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœå·²ç»ç¼“å­˜äº†ï¼Œç›´æ¥è¿”å›
    if _header_template_cache is not None:
        return _header_template_cache.copy()
    
    # ä½¿ç”¨å¼‚æ­¥é”ä¿æŠ¤ç¼“å­˜åˆå§‹åŒ–
    async with _header_cache_lock:
        # åŒé‡æ£€æŸ¥ï¼šå¯èƒ½å…¶ä»–åç¨‹å·²ç»åˆå§‹åŒ–äº†
        if _header_template_cache is not None:
            return _header_template_cache.copy()
        
        header_gen = get_header_generator_instance()
        
        # ä½¿ç”¨BrowserForgeç”ŸæˆåŸºç¡€headersï¼ˆä»…ä¸€æ¬¡ï¼‰
        base_headers = header_gen.generate()
        
        # è®¾ç½®ç‰¹å®šäºZ.AIçš„headers
        base_headers["Origin"] = "https://chat.z.ai"
        base_headers["Content-Type"] = "application/json"
        base_headers["X-Fe-Version"] = settings.ZAI_FE_VERSION
        
        # è®¾ç½®Fetchç›¸å…³headersï¼ˆç”¨äºCORSè¯·æ±‚ï¼‰
        base_headers["Sec-Fetch-Dest"] = "empty"
        base_headers["Sec-Fetch-Mode"] = "cors"
        base_headers["Sec-Fetch-Site"] = "same-origin"
        
        # ç¡®ä¿Accept-EncodingåŒ…å«zstdï¼ˆç°ä»£æµè§ˆå™¨æ”¯æŒï¼‰
        if "Accept-Encoding" in base_headers:
            if "zstd" not in base_headers["Accept-Encoding"]:
                base_headers["Accept-Encoding"] = base_headers["Accept-Encoding"] + ", zstd"
        else:
            base_headers["Accept-Encoding"] = "gzip, deflate, br, zstd"
        
        # ç¡®ä¿Acceptå¤´é€‚åˆAPIè¯·æ±‚
        base_headers["Accept"] = "*/*"
        
        # ä¿æŒè¿æ¥
        base_headers["Connection"] = "keep-alive"
        
        _header_template_cache = base_headers
        info_log("âœ… Headeræ¨¡æ¿å·²ç¼“å­˜", 
                  user_agent=base_headers.get("User-Agent", "")[:50],
                  has_sec_ch_ua=("sec-ch-ua" in base_headers or "Sec-Ch-Ua" in base_headers))
    
    return _header_template_cache.copy()


async def clear_header_template():
    """
    æ¸…é™¤ç¼“å­˜çš„headeræ¨¡æ¿ï¼Œå¼ºåˆ¶ä¸‹æ¬¡è°ƒç”¨æ—¶é‡æ–°ç”Ÿæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    """
    global _header_template_cache
    async with _header_cache_lock:
        _header_template_cache = None
        info_log("ğŸ”„ Headeræ¨¡æ¿ç¼“å­˜å·²æ¸…é™¤")


async def get_dynamic_headers(chat_id: str = "", user_agent: str = "") -> Dict[str, str]:
    """ä½¿ç”¨ç¼“å­˜çš„headeræ¨¡æ¿ç”Ÿæˆheadersï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        chat_id: å¯¹è¯IDï¼Œç”¨äºç”ŸæˆReferer
        user_agent: å¯é€‰çš„æŒ‡å®šUser-Agentï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼Œä½†ä¸æ¨èä½¿ç”¨ï¼‰
        
    Returns:
        å®Œæ•´çš„HTTP headerså­—å…¸
    """
    # ä½¿ç”¨ç¼“å­˜çš„æ¨¡æ¿ï¼ˆé¿å…æ¯æ¬¡è°ƒç”¨BrowserForgeï¼‰
    headers = await get_header_template()
    
    # ä»…æ›´æ–°éœ€è¦å˜åŒ–çš„å­—æ®µ
    if chat_id:
        headers["Referer"] = f"https://chat.z.ai/c/{chat_id}"
    else:
        headers["Referer"] = "https://chat.z.ai/"
    
    # å¦‚æœæŒ‡å®šäº†user_agentï¼Œè¦†ç›–æ¨¡æ¿ä¸­çš„User-Agent
    if user_agent:
        headers["User-Agent"] = user_agent
    
    return headers


def build_query_params(
    timestamp: int, 
    request_id: str, 
    token: str,
    user_agent: str,
    chat_id: str = "",
    user_id: str = ""
) -> Dict[str, str]:
    """æ„å»ºæŸ¥è¯¢å‚æ•°ï¼Œæ¨¡æ‹ŸçœŸå®çš„æµè§ˆå™¨è¯·æ±‚ï¼ˆä½¿ç”¨furlä¼˜åŒ–URLå¤„ç†ï¼‰"""
    if not user_id:
        try:
            payload = decode_jwt_payload(token)
            user_id = payload['id']
        except Exception:
            user_id = "guest-user-" + str(abs(hash(token)) % 1000000)
    
    # ä½¿ç”¨furlæ„å»ºURLï¼ˆæ›´ä¼˜é›…çš„URLå¤„ç†ï¼‰
    if chat_id:
        url = furl("https://chat.z.ai").add(path=["c", chat_id])
        pathname = f"/c/{chat_id}"
    else:
        url = furl("https://chat.z.ai")
        pathname = "/"
    
    # æ„å»ºå®Œæ•´çš„æŸ¥è¯¢å‚æ•°ï¼ŒåŒ…æ‹¬æµè§ˆå™¨æŒ‡çº¹ä¿¡æ¯
    query_params = {
        "timestamp": str(timestamp),
        "requestId": request_id,
        "user_id": user_id,
        "version": "0.0.1",
        "platform": "web",
        "token": token,
        "user_agent": user_agent,
        "language": "zh-CN",
        "languages": "zh-CN,zh",
        "timezone": "Asia/Shanghai",
        "cookie_enabled": "true",
        "screen_width": "2048",
        "screen_height": "1152",
        "screen_resolution": "2048x1152",
        "viewport_height": "654",
        "viewport_width": "1038",
        "viewport_size": "1038x654",
        "color_depth": "24",
        "pixel_ratio": "1.25",
        "current_url": str(url),
        "pathname": pathname,
        "search": "",
        "hash": "",
        "host": "chat.z.ai",
        "hostname": "chat.z.ai",
        "protocol": "https:",
        "referrer": "",
        "title": "Z.ai Chat - Free AI powered by GLM-4.6 & GLM-4.5",
        "timezone_offset": "-480",
        "local_time": datetime.now(tz=get_timezone("Asia/Shanghai")).strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z",
        "utc_time": datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S GMT"),
        "is_mobile": "false",
        "is_touch": "false",
        "max_touch_points": "10",
        "browser_name": "Chrome",
        "os_name": "Windows",
        "signature_timestamp": str(timestamp),
    }
    
    return query_params


class ZAITransformer:
    """ZAIè½¬æ¢å™¨ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.name = "zai"
        self.base_url = "https://chat.z.ai"
        # ä¸å†åœ¨åˆå§‹åŒ–æ—¶å›ºå®šapi_urlï¼Œè€Œæ˜¯åœ¨transform_request_inä¸­åŠ¨æ€è·å–
        # self.api_url = settings.API_ENDPOINT

        # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„æ¨¡å‹æ˜ å°„
        self.model_mapping = MODEL_MAPPING
        
        # åˆå§‹åŒ–ç­¾åç”Ÿæˆå™¨
        self.signature_generator = SignatureGenerator()

    async def get_token(self, http_client=None) -> str:
        """
        è·å–Z.AIè®¤è¯ä»¤ç‰Œï¼ˆä»tokenæ± è·å–ï¼‰

        Args:
            http_client: å¤–éƒ¨ä¼ å…¥çš„HTTPå®¢æˆ·ç«¯ï¼ˆç”¨äºåŒ¿åTokenè·å–ï¼‰

        Returns:
            str: å¯ç”¨çš„Token
        """
        token_pool = await get_token_pool()
        token = await token_pool.get_token(http_client=http_client)

        token_preview = f"{token[:20]}...{token[-20:]}" if len(token) > 40 else token
        debug_log(f"ä½¿ç”¨tokenæ± ä¸­çš„ä»¤ç‰Œ (æ± å¤§å°: {token_pool.get_pool_size()}): {token_preview}")
        return token
    
    async def switch_token(self, http_client=None) -> str:
        """
        åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªtokenï¼ˆè¯·æ±‚å¤±è´¥æ—¶è°ƒç”¨ï¼‰
        è‡ªåŠ¨å¤„ç†é™çº§åˆ°åŒ¿åTokençš„é€»è¾‘

        Args:
            http_client: å¤–éƒ¨ä¼ å…¥çš„HTTPå®¢æˆ·ç«¯ï¼ˆå¦‚æœåˆ‡æ¢åˆ°åŒ¿åTokenæ—¶ä½¿ç”¨ï¼‰

        Returns:
            str: ä¸‹ä¸€ä¸ªToken

        Raises:
            ValueError: é…ç½®Tokenå’ŒåŒ¿åTokenéƒ½ä¸å¯ç”¨æ—¶æŠ›å‡º
        """
        token_pool = await get_token_pool()
        token = await token_pool.switch_to_next()

        # å¦‚æœè¿”å› Noneï¼Œè¯´æ˜æ‰€æœ‰é…ç½® Token éƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°åŒ¿å Token
        if token is None:
            if settings.ENABLE_GUEST_TOKEN:
                info_log("[FALLBACK] é…ç½®Tokenå…¨éƒ¨å¤±è´¥ï¼Œé™çº§åˆ°åŒ¿åToken")
                await self.clear_anonymous_token_cache()  # æ¸…ç†ç¼“å­˜ï¼Œè·å–æ–°çš„åŒ¿å Token
                anonymous_token = await token_pool.get_anonymous_token(http_client)
                if anonymous_token:
                    return anonymous_token
                else:
                    raise ValueError("[ERROR] é…ç½®Tokenå’ŒåŒ¿åTokenéƒ½ä¸å¯ç”¨")
            else:
                raise ValueError("[ERROR] æ‰€æœ‰é…ç½®Tokenä¸å¯ç”¨ä¸”åŒ¿åTokenå·²ç¦ç”¨")

        return token
    
    async def clear_anonymous_token_cache(self):
        """
        æ¸…ç†åŒ¿åTokenç¼“å­˜ï¼ˆå½“Tokenå¤±æ•ˆæ—¶è°ƒç”¨ï¼‰
        çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬
        """
        token_pool = await get_token_pool()
        await token_pool.clear_anonymous_token_cache()  # è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬
        debug_log("[TRANSFORMER] åŒ¿åTokenç¼“å­˜å·²æ¸…ç†")
    
    async def refresh_header_template(self):
        """åˆ·æ–°headeræ¨¡æ¿ï¼ˆæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°ç”Ÿæˆï¼‰"""
        await clear_header_template()
        info_log("ğŸ”„ Headeræ¨¡æ¿å·²åˆ·æ–°ï¼Œä¸‹æ¬¡è¯·æ±‚å°†ä½¿ç”¨æ–°çš„header")
    
    def _has_image_content(self, messages: List[Dict]) -> bool:
        """
        æ£€æµ‹æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å›¾åƒ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            bool: å¦‚æœæ¶ˆæ¯ä¸­åŒ…å«å›¾åƒå†…å®¹åˆ™è¿”å›True
        """
        for msg in messages:
            if msg.get("role") == "user":
                content = msg.get("content")
                if isinstance(content, list):
                    for part in content:
                        if (part.get("type") == "image_url" and
                            part.get("image_url", {}).get("url")):
                            return True
        return False
    
    def _process_messages(self, messages: list, is_vision_model: bool = False) -> Tuple[list, list]:
        """
        å¤„ç†æ¶ˆæ¯åˆ—è¡¨ï¼Œè½¬æ¢systemè§’è‰²å’Œæå–å›¾ç‰‡å†…å®¹
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            is_vision_model: æ˜¯å¦æ˜¯è§†è§‰æ¨¡å‹ï¼ˆGLM-4.5V/GLM-4.6Vï¼‰ï¼Œè§†è§‰æ¨¡å‹ä¿ç•™å›¾ç‰‡åœ¨messagesä¸­
            
        Returns:
            (å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨, å›¾ç‰‡URLåˆ—è¡¨)
        """
        processed_messages = []
        image_urls = []
        
        for idx, orig_msg in enumerate(messages):
            msg = orig_msg.copy()

            # å¤„ç†systemè§’è‰²è½¬æ¢
            if msg.get("role") == "system":
                msg["role"] = "user"
                content = msg.get("content")

                if isinstance(content, list):
                    msg["content"] = [
                        {"type": "text", "text": "This is a system command, you must enforce compliance."}
                    ] + content
                elif isinstance(content, str):
                    msg["content"] = f"This is a system command, you must enforce compliance.{content}"

            # å¤„ç†userè§’è‰²çš„å›¾ç‰‡å†…å®¹
            elif msg.get("role") == "user":
                content = msg.get("content")
                if isinstance(content, list):
                    new_content = []
                    for part_idx, part in enumerate(content):
                        if (
                            part.get("type") == "image_url"
                            and part.get("image_url", {}).get("url")
                            and isinstance(part["image_url"]["url"], str)
                        ):
                            image_url = part["image_url"]["url"]
                            debug_log(f"    æ¶ˆæ¯[{idx}]å†…å®¹[{part_idx}]: æ£€æµ‹åˆ°å›¾ç‰‡URL")
                            image_urls.append(image_url)
                            
                            # è§†è§‰æ¨¡å‹ï¼šä¿ç•™å›¾ç‰‡åœ¨æ¶ˆæ¯ä¸­ï¼Œä½†ä¼šåœ¨ä¸Šä¼ åä¿®æ”¹URLæ ¼å¼
                            if is_vision_model:
                                new_content.append(part)
                            # éè§†è§‰æ¨¡å‹ï¼šç§»é™¤å›¾ç‰‡å†…å®¹ï¼Œåªä¿ç•™æ–‡æœ¬
                        elif part.get("type") == "text":
                            new_content.append(part)
                    
                    # å¦‚æœnew_contentåªæœ‰æ–‡æœ¬ï¼Œæå–ä¸ºå­—ç¬¦ä¸²
                    if len(new_content) == 1 and new_content[0].get("type") == "text":
                        msg["content"] = new_content[0].get("text", "")
                    elif new_content:
                        msg["content"] = new_content
                    else:
                        msg["content"] = ""

            processed_messages.append(msg)
        
        return processed_messages, image_urls
    
    def _extract_last_user_content(self, messages: list) -> str:
        """
        æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹
        """
        user_content = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                content = msg.get("content", "")
                if isinstance(content, str):
                    user_content = content
                elif isinstance(content, list) and len(content) > 0:
                    for part in content:
                        if part.get("type") == "text":
                            user_content = part.get("text", "")
                            break
                break
        return user_content

    @perf_track("transform_request_in", log_result=True, threshold_ms=10)
    async def transform_request_in(self, request: Dict[str, Any], client=None, upstream_url: str = None) -> Dict[str, Any]:
        """
        è½¬æ¢OpenAIè¯·æ±‚ä¸ºz.aiæ ¼å¼

        Args:
            request: OpenAIæ ¼å¼çš„è¯·æ±‚
            client: HTTPå®¢æˆ·ç«¯ï¼ˆç”¨äºå›¾åƒä¸Šä¼ å’ŒTokenè·å–ï¼‰
            upstream_url: ä¸Šæ¸¸APIåœ°å€ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

        Returns:
            è½¬æ¢åçš„è¯·æ±‚å­—å…¸
        """
        info_log(f"å¼€å§‹è½¬æ¢ OpenAI è¯·æ±‚åˆ° Z.AI æ ¼å¼: {request.get('model', settings.PRIMARY_MODEL)} -> Z.AI")

        # è·å–è®¤è¯ä»¤ç‰Œï¼ˆä¼ å…¥clientç”¨äºåŒ¿åTokenè·å–ï¼‰
        token = await self.get_token(http_client=client)

        # æ£€æŸ¥åŒ¿åTokenæ˜¯å¦å°è¯•ä½¿ç”¨è§†è§‰æ¨¡å‹
        token_pool = await get_token_pool()
        messages = request.get("messages", [])
        
        if token_pool.is_anonymous_token(token) and self._has_image_content(messages):
            error_log("[ERROR] åŒ¿åTokenå°è¯•ä½¿ç”¨è§†è§‰åŠŸèƒ½è¢«æ‹’ç»")
            raise HTTPException(
                status_code=400,
                detail="åŒ¿åTokenä¸æ”¯æŒå›¾åƒè¯†åˆ«åŠŸèƒ½ï¼Œè¯·é…ç½®ZAI_TOKENä½¿ç”¨è§†è§‰æ¨¡å‹ã€‚è®¾ç½®ç¯å¢ƒå˜é‡ZAI_TOKEN=your_tokenåé‡å¯æœåŠ¡ã€‚"
            )

        # ç¡®å®šè¯·æ±‚çš„æ¨¡å‹ç‰¹æ€§
        requested_model = request.get("model", settings.PRIMARY_MODEL)
        is_thinking = (requested_model == settings.THINKING_MODEL or
                      requested_model == settings.GLM_46_THINKING_MODEL or
                      requested_model == settings.GLM_47_THINKING_MODEL or  # åªæœ‰ GLM-4.7-Thinking å¯ç”¨ thinking
                      requested_model == settings.GLM_45V_MODEL or  # glm4.5v è§†è§‰æ¨¡å‹ä¹Ÿæ˜¯ thinking æ¨¡å‹
                      requested_model == settings.GLM_46V_MODEL or  # glm4.6v è§†è§‰æ¨¡å‹ä¹Ÿæ˜¯ thinking æ¨¡å‹
                      request.get("reasoning", False))
        is_search = (requested_model == settings.SEARCH_MODEL or
                    requested_model == settings.GLM_46_SEARCH_MODEL)
        is_advanced_search = (requested_model == settings.GLM_46_ADVANCED_SEARCH_MODEL)
        is_vision_model = (requested_model == settings.GLM_45V_MODEL or
                          requested_model == settings.GLM_46V_MODEL)
        is_glm47_model = (requested_model == settings.GLM_47_MODEL or
                          requested_model == settings.GLM_47_THINKING_MODEL)

        # è·å–ä¸Šæ¸¸æ¨¡å‹ID
        upstream_model_id = self.model_mapping.get(requested_model, "0727-360B-API")
        debug_log(f"  æ¨¡å‹æ˜ å°„: {requested_model} -> {upstream_model_id}")

        # å¤„ç†æ¶ˆæ¯åˆ—è¡¨å¹¶æå–å›¾åƒ
        debug_log(f"  å¼€å§‹å¤„ç† {len(request.get('messages', []))} æ¡æ¶ˆæ¯")
        with perf_timer("process_messages", threshold_ms=5):
            messages, image_urls = self._process_messages(request.get("messages", []), is_vision_model=is_vision_model)

        # æ„å»ºMCPæœåŠ¡å™¨åˆ—è¡¨
        mcp_servers = []
        # GLM-4.6V æ·»åŠ  VLM ä¸“æœ‰æœåŠ¡å™¨ï¼ˆæ”¯æŒå›¾ç‰‡æœç´¢ã€è¯†åˆ«ã€å¤„ç†ï¼‰
        if requested_model == settings.GLM_46V_MODEL:
            mcp_servers.extend(["vlm-image-search", "vlm-image-recognition", "vlm-image-processing"])
            debug_log(f"ğŸ” æ£€æµ‹åˆ° GLM-4.6V æ¨¡å‹ï¼Œæ·»åŠ  VLM MCP æœåŠ¡å™¨")
        if is_advanced_search:
            mcp_servers.append("advanced-search")
            debug_log(f"ğŸ” æ£€æµ‹åˆ°é«˜çº§æœç´¢æ¨¡å‹ï¼Œæ·»åŠ  advanced-search MCP æœåŠ¡å™¨")
        elif is_search:
            mcp_servers.append("deep-web-search")
            debug_log(f"ğŸ” æ£€æµ‹åˆ°æœç´¢æ¨¡å‹ï¼Œæ·»åŠ  deep-web-search MCP æœåŠ¡å™¨")
        
        # æ„å»ºéšè—çš„MCPæœåŠ¡å™¨ç‰¹æ€§åˆ—è¡¨
        hidden_mcp_features = [
            {"type": "mcp", "server": "vibe-coding", "status": "hidden"},
            {"type": "mcp", "server": "ppt-maker", "status": "hidden"},
            {"type": "mcp", "server": "image-search", "status": "hidden"},
            {"type": "mcp", "server": "deep-research", "status": "hidden"}
        ]

        # å¤„ç†å›¾åƒä¸Šä¼ 
        files_list = []
        uploaded_files_map = {}  # ç”¨äºè§†è§‰æ¨¡å‹(GLM-4.5V/GLM-4.6V)ï¼šåŸå§‹URL -> æ–‡ä»¶ä¿¡æ¯çš„æ˜ å°„
        
        if image_urls and client:
            info_log(f"æ£€æµ‹åˆ° {len(image_urls)} å¼ å›¾åƒï¼Œå¼€å§‹ä¸Šä¼ ")
            for idx, image_url in enumerate(image_urls):
                try:
                    file_obj = await process_image_content(image_url, token, client)
                    if file_obj:
                        # éè§†è§‰æ¨¡å‹ï¼šæ·»åŠ åˆ°filesåˆ—è¡¨
                        if not is_vision_model:
                            files_list.append(file_obj)
                        else:
                            # è§†è§‰æ¨¡å‹ï¼šä¿å­˜æ˜ å°„å…³ç³»ï¼Œç¨åä¿®æ”¹messagesä¸­çš„URL
                            uploaded_files_map[image_url] = file_obj
                        info_log(f"å›¾åƒ [{idx+1}/{len(image_urls)}] ä¸Šä¼ æˆåŠŸ", file_id=file_obj.get("id"))
                    else:
                        error_log(f"å›¾åƒ [{idx+1}/{len(image_urls)}] ä¸Šä¼ å¤±è´¥")
                except Exception as e:
                    error_log(f"å›¾åƒ [{idx+1}/{len(image_urls)}] å¤„ç†é”™è¯¯: {e}")
        elif image_urls:
            info_log(f"æ£€æµ‹åˆ° {len(image_urls)} å¼ å›¾åƒï¼Œä½†æœªæä¾›HTTPå®¢æˆ·ç«¯ï¼Œè·³è¿‡ä¸Šä¼ ")
        
        # GLM-4.5Vç‰¹æ®Šå¤„ç†ï¼šä¿®æ”¹messagesä¸­çš„å›¾ç‰‡URLæ ¼å¼
        # ç”Ÿæˆå½“å‰ç”¨æˆ·æ¶ˆæ¯IDï¼ˆç”¨äºå…³è”filesï¼‰
        current_user_message_id = generate_uuid() if is_vision_model else None
        
        if is_vision_model and uploaded_files_map:
            info_log(f"[Vision] å¼€å§‹ä¿®æ”¹æ¶ˆæ¯ä¸­çš„å›¾ç‰‡URLæ ¼å¼")
            for msg in messages:
                if msg.get("role") == "user" and isinstance(msg.get("content"), list):
                    for part in msg["content"]:
                        if part.get("type") == "image_url":
                            original_url = part.get("image_url", {}).get("url")
                            if original_url in uploaded_files_map:
                                file_info = uploaded_files_map[original_url]
                                # æå–fileä¿¡æ¯
                                file_data = file_info.get("file", {})
                                file_id = file_data.get("id", "")
                                # GLM-4.5Væ ¼å¼çš„URLåªéœ€è¦file_id
                                part["image_url"]["url"] = file_id
                                debug_log(f"[GLM-4.5V] å›¾ç‰‡URLå·²è½¬æ¢", 
                                         original=original_url[:50], 
                                         new=file_id)
                                
                                # æ·»åŠ ref_user_msg_idåˆ°æ–‡ä»¶ä¿¡æ¯ï¼ˆç”¨äºfilesæ•°ç»„ï¼‰
                                file_info["ref_user_msg_id"] = current_user_message_id
                                files_list.append(file_info)
            
        # æ„å»ºä¸Šæ¸¸è¯·æ±‚ä½“
        chat_id = generate_uuid()

        # GLM-4.5V å¯¹ features/background_tasks çš„è¦æ±‚ä¸å¸¸è§„æ¨¡å‹ç•¥æœ‰ä¸åŒï¼Œ
        # å°½é‡å¯¹é½å®é™…æŠ“åŒ…æ ¼å¼ï¼Œé¿å…ä¸Šæ¸¸è¿”å› "Oops" é”™è¯¯ã€‚
        if is_vision_model:
            features = {
                "image_generation": False,
                "web_search": False,
                "auto_web_search": False,
                "preview_mode": True,
                "flags": [],
                "enable_thinking": True,
            }
            background_tasks = {
                "title_generation": True,
                "tags_generation": True,
            }
        elif is_glm47_model:
            # GLM-4.7 ç³»åˆ—é‡‡ç”¨ç®€åŒ–æ ¼å¼ï¼Œenable_thinking æ ¹æ®å…·ä½“æ¨¡å‹å†³å®š
            # glm-4.7: enable_thinking=False, glm-4.7-thinking: enable_thinking=True
            glm47_enable_thinking = (requested_model == settings.GLM_47_THINKING_MODEL)
            features = {
                "image_generation": False,
                "web_search": False,
                "auto_web_search": False,
                "preview_mode": True,
                "flags": [],
                "enable_thinking": glm47_enable_thinking,
            }
            background_tasks = {
                "title_generation": True,
                "tags_generation": True,
            }
        else:
            features = {
                "image_generation": False,
                "web_search": is_search or is_advanced_search,
                "auto_web_search": is_search or is_advanced_search,
                "preview_mode": True,
                "flags": [],
                "features": hidden_mcp_features,
                "enable_thinking": is_thinking or is_search or is_advanced_search,
            }
            background_tasks = {
                "title_generation": False,
                "tags_generation": False,
            }

        body = {
            "stream": True,
            "model": upstream_model_id,
            "messages": messages,
            "params": {},
            "features": features,
            "background_tasks": background_tasks,
            "mcp_servers": mcp_servers,
            "variables": {
                "{{USER_NAME}}": "Guest",
                "{{USER_LOCATION}}": "Unknown",
                # ä½¿ç”¨ä¼˜åŒ–åçš„æ—¶é—´å˜é‡ç”Ÿæˆå‡½æ•°ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œé¿å…é‡å¤ï¼‰
                **generate_time_variables("Asia/Shanghai"),
            },
            "chat_id": chat_id,
            "id": generate_uuid(),
        }

        # ä¸æŠ“åŒ…ä¿æŒä¸€è‡´ï¼šGLM-4.5V/4.6V/4.7 éœ€è¦ current_user_message_id/parent_idï¼Œä¸”ä¸ä¸Šé€ model_item
        if is_vision_model:
            body["current_user_message_id"] = current_user_message_id
            body["current_user_message_parent_id"] = None
        elif is_glm47_model:
            # GLM-4.7 ä½¿ç”¨ç®€åŒ–æ ¼å¼ï¼šæ·»åŠ  current_user_message_id/parent_id ä½†ä¸æ·»åŠ  model_item
            body["current_user_message_id"] = generate_uuid()
            body["current_user_message_parent_id"] = None
        else:
            body["model_item"] = {
                "id": upstream_model_id,
                "name": requested_model,
                "owned_by": "openai"
            }
        
        # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ·»åŠ åˆ°bodyä¸­
        if files_list:
            body["files"] = files_list
            debug_log(f"æ·»åŠ  {len(files_list)} ä¸ªæ–‡ä»¶åˆ°è¯·æ±‚body")
        
        # GLM-4.5Véœ€è¦æ·»åŠ current_user_message_idå­—æ®µ
        if is_vision_model and current_user_message_id:
            body["current_user_message_id"] = current_user_message_id
            debug_log(f"[GLM-4.5V] æ·»åŠ current_user_message_id: {current_user_message_id}")

        # ç”Ÿæˆæ—¶é—´æˆ³å’Œè¯·æ±‚ID
        timestamp = int(time.time() * 1000)
        request_id = generate_uuid()
        
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å†…å®¹ï¼ˆç”¨äºç­¾åå’Œè¯·æ±‚ä½“ï¼‰
        with perf_timer("extract_user_content", threshold_ms=5):
            user_content = self._extract_last_user_content(messages)
        
        # å°†ç”¨æˆ·å†…å®¹æ·»åŠ åˆ°è¯·æ±‚ä½“ä¸­ï¼ˆæ–°è¦æ±‚ï¼‰
        body["signature_prompt"] = user_content
        
        # ä½¿ç”¨ç¼“å­˜çš„headeræ¨¡æ¿ç”Ÿæˆheadersï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        with perf_timer("generate_headers", threshold_ms=5):
            dynamic_headers = await get_dynamic_headers(chat_id)
        
        # ä»ç”Ÿæˆçš„headersä¸­æå–User-Agent
        user_agent = dynamic_headers.get("User-Agent", "")
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        user_id = ""
        try:
            payload = decode_jwt_payload(token)
            user_id = payload['id']
        except Exception as e:
            debug_log(f"è§£ç JWT tokenè·å–user_idå¤±è´¥: {e}")
            user_id = "guest-user-" + str(abs(hash(token)) % 1000000)
        
        query_params = build_query_params(timestamp, request_id, token, user_agent, chat_id, user_id)
        
        # ç”ŸæˆZ.AIç­¾å
        try:
            # ä½¿ç”¨SignatureGeneratorç”Ÿæˆç­¾å
            with perf_timer("generate_signature", threshold_ms=10):
                signature_result = self.signature_generator.generate(token, request_id, timestamp, user_content)
                signature = signature_result["signature"]

            # æ·»åŠ ç­¾ååˆ°headers
            dynamic_headers["X-Signature"] = signature
            query_params["signature_timestamp"] = str(timestamp)

            debug_log("  Z.AIç­¾åå·²ç”Ÿæˆå¹¶æ·»åŠ åˆ°è¯·æ±‚ä¸­")
        except Exception as e:
            error_log(f"ç”ŸæˆZ.AIç­¾åå¤±è´¥: {e}")

        # ä½¿ç”¨ä¼ å…¥çš„ä¸Šæ¸¸URLæˆ–é»˜è®¤é…ç½®
        api_url = upstream_url if upstream_url else settings.API_ENDPOINT
        debug_log(f"  ä½¿ç”¨ä¸Šæ¸¸åœ°å€: {api_url}")

        # æ„å»ºå®Œæ•´çš„URL
        url_with_params = f"{api_url}?" + "&".join([f"{k}={v}" for k, v in query_params.items()])

        headers = {
            **dynamic_headers,
            "Authorization": f"Bearer {token}",
            "Cache-Control": "no-cache",
            # "Pragma": "no-cache",
        }

        config = {
            "url": url_with_params,
            "headers": headers,
        }

        info_log("è¯·æ±‚è½¬æ¢å®Œæˆ")

        return {
            "body": body,
            "config": config,
            "token": token,
            "is_thinking": is_thinking,  # æ ‡è®°æ˜¯å¦ä¸ºthinkingæ¨¡å‹ï¼Œå“åº”å¤„ç†æ—¶ç”¨äºåˆ¤æ–­æ˜¯å¦è¾“å‡ºreasoning_content
            "is_vision_model": is_vision_model,  # æ ‡è®°æ˜¯å¦ä¸ºVç³»åˆ—è§†è§‰æ¨¡å‹ï¼ˆ4.5v/4.6vï¼‰ï¼Œç”¨äºåŒºåˆ†å¤šé˜¶æ®µæ€è€ƒæ ¼å¼
        }
